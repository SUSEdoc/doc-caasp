<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Cluster Disaster Recovery | Administration Guide | SUSE CaaS Platform 4.5.1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.1" /><meta name="book-title" content="Administration Guide" /><meta name="chapter-title" content="Chapter 12. Cluster Disaster Recovery" /><meta name="description" content="Etcd is a crucial component of Kubernetes - the etcd cluster stores the entire Kubernetes cluster state, which means critical configuration data, specifications, as well as the statuses of the running workloads. It also serves as the backend for service discovery. Chapter 13, Backup and Restore with…" /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="AdminGuide" /><link rel="home" href="index.html" title="Administration Guide" /><link rel="up" href="index.html" title="Administration Guide" /><link rel="prev" href="_gpu_dependent_workloads.html" title="Chapter 11. GPU-Dependent Workloads" /><link rel="next" href="backup-and-restore-with-velero.html" title="Chapter 13. Backup and Restore with Velero" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Administration Guide"><span class="book-icon">Administration Guide</span></a><span> › </span><a class="crumb" href="_cluster_disaster_recovery.html">Cluster Disaster Recovery</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Administration Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pr01.html"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="_about_this_guide.html"><span class="number">1 </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="_cluster_management.html"><span class="number">2 </span><span class="name">Cluster Management</span></a></li><li class="inactive"><a href="_software_management.html"><span class="number">3 </span><span class="name">Software Management</span></a></li><li class="inactive"><a href="_cluster_updates.html"><span class="number">4 </span><span class="name">Cluster Updates</span></a></li><li class="inactive"><a href="_upgrading_suse_caas_platform.html"><span class="number">5 </span><span class="name">Upgrading SUSE CaaS Platform</span></a></li><li class="inactive"><a href="_security.html"><span class="number">6 </span><span class="name">Security</span></a></li><li class="inactive"><a href="_logging.html"><span class="number">7 </span><span class="name">Logging</span></a></li><li class="inactive"><a href="_monitoring.html"><span class="number">8 </span><span class="name">Monitoring</span></a></li><li class="inactive"><a href="_storage.html"><span class="number">9 </span><span class="name">Storage</span></a></li><li class="inactive"><a href="_integration.html"><span class="number">10 </span><span class="name">Integration</span></a></li><li class="inactive"><a href="_gpu_dependent_workloads.html"><span class="number">11 </span><span class="name">GPU-Dependent Workloads</span></a></li><li class="inactive"><a href="_cluster_disaster_recovery.html"><span class="number">12 </span><span class="name">Cluster Disaster Recovery</span></a></li><li class="inactive"><a href="backup-and-restore-with-velero.html"><span class="number">13 </span><span class="name">Backup and Restore with Velero</span></a></li><li class="inactive"><a href="_miscellaneous.html"><span class="number">14 </span><span class="name">Miscellaneous</span></a></li><li class="inactive"><a href="_troubleshooting_3.html"><span class="number">15 </span><span class="name">Troubleshooting</span></a></li><li class="inactive"><a href="_glossary.html"><span class="number">16 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="_contributors.html"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="_gnu_licenses.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 11. GPU-Dependent Workloads" href="_gpu_dependent_workloads.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 13. Backup and Restore with Velero" href="backup-and-restore-with-velero.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Administration Guide"><span class="book-icon">Administration Guide</span></a><span> › </span><a class="crumb" href="_cluster_disaster_recovery.html">Cluster Disaster Recovery</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 11. GPU-Dependent Workloads" href="_gpu_dependent_workloads.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 13. Backup and Restore with Velero" href="backup-and-restore-with-velero.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="_cluster_disaster_recovery"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.1</span></div><div><h1 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Disaster Recovery</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="_cluster_disaster_recovery.html#_backing_up_etcd_cluster_data"><span class="number">12.1 </span><span class="name">Backing Up etcd Cluster Data</span></a></span></dt><dt><span class="section"><a href="_cluster_disaster_recovery.html#_recovering_master_nodes"><span class="number">12.2 </span><span class="name">Recovering Master Nodes</span></a></span></dt></dl></div></div><p>Etcd is a crucial component of Kubernetes - the <span class="strong"><strong>etcd cluster</strong></span> stores the entire Kubernetes cluster state, which means critical configuration data, specifications, as well as the statuses of the running workloads. It also serves as the backend for service discovery. <a class="xref" href="backup-and-restore-with-velero.html" title="Chapter 13. Backup and Restore with Velero">Chapter 13, <em>Backup and Restore with Velero</em></a> explains how to use Velero to backup, restore and migrate data. However, the Kubernetes cluster needs to be accessible for Velero to operate. And since the Kubernetes cluster can become inaccessible for many reasons, for example when all of its master nodes are lost, <span class="strong"><strong>it is important to periodically backup etcd cluster data</strong></span>.</p><div class="sect1" id="_backing_up_etcd_cluster_data"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backing Up etcd Cluster Data</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_backing_up_etcd_cluster_data">#</a></h2></div></div></div><p>This chapter describes the backup of <code class="literal">etcd</code> cluster data running on master nodes of SUSE CaaS Platform.</p><div class="sect2" id="_data_to_backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Data To Backup</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_data_to_backup">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create backup directories on external storage.</p><div class="verbatim-wrap highlight bash"><pre class="screen">BACKUP_DIR=CaaSP_Backup_`date +%Y%m%d%H%M%S`
mkdir /${BACKUP_DIR}</pre></div></li><li class="listitem "><p>Copy the following files/folders into the backup directory:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The <code class="literal">skuba</code> command-line binary: for the running cluster. Used to replace nodes from cluster.</p></li><li class="listitem "><p>The cluster definition folder: Directory created during bootstrap holding the cluster certificates and configuration.</p></li><li class="listitem "><p>The <code class="literal">etcd</code> cluster database: Holds all non-persistent cluster data.
Can be used to recover master nodes. Please refer to the next section for steps to create an <code class="literal">etcd</code> cluster database backup.</p></li></ul></div></li><li class="listitem "><p>(Optional) Make backup directory into a compressed file, and remove the original backup directory.</p><div class="verbatim-wrap highlight bash"><pre class="screen">tar cfv ${BACKUP_DIR}.tgz /${BACKUP_DIR}
rm -rf /${BACKUP_DIR}</pre></div></li></ol></div></div><div class="sect2" id="_creating_an_etcd_cluster_database_backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an etcd Cluster Database Backup</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_creating_an_etcd_cluster_database_backup">#</a></h3></div></div></div><div class="sect3" id="_procedure"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_procedure">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Mount external storage device to all master nodes.
This is only required if the following step is using local hostpath as volume storage.</p></li><li class="listitem "><p>Create backup.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Find the size of the database to be backed up</p><div class="verbatim-wrap highlight bash"><pre class="screen">ls -sh /var/lib/etcd/member/snap/db</pre></div><div id="id-1.14.3.4.2.2.2.2.1.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The backup size depends on the cluster. Ensure each of the backups has sufficient space.
The available size should be more than the database snapshot file.</p><p>You should also have a rotation method to clean up the unneeded snapshots over time.</p><p>When there is insufficient space available during backup, pods will fail to be in <code class="literal">Running</code> state and <code class="literal">no space left on device</code> errors will show in pod logs.</p><p>The below example manifest shows a binding to a local <code class="literal">hostPath</code>.
We strongly recommend using other storage methods instead.</p></div></li><li class="listitem "><p>Modify the script example</p><p>Replace <code class="literal">&lt;STORAGE_MOUNT_POINT&gt;</code> with the directory in which to store the backup.
The directory must exist on every node in cluster.</p><p>Replace <code class="literal">&lt;IN_CLUSTER_ETCD_IMAGE&gt;</code> with the <code class="literal">etcd</code> image used in the cluster.
This can be retrieved by accessing any one of the nodes in the cluster and running:</p><div class="verbatim-wrap"><pre class="screen">grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'</pre></div></li><li class="listitem "><p>Create a backup deployment</p><p>Run the following script:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ETCD_SNAPSHOT="&lt;STORAGE_MOUNT_POINT&gt;/etcd_snapshot"
ETCD_IMAGE="&lt;IN_CLUSTER_ETCD_IMAGE&gt;"
MANIFEST="etcd-backup.yaml"

cat &lt;&lt; *EOF* &gt; ${MANIFEST}
apiVersion: batch/v1
kind: Job
metadata:
  name: etcd-backup
  namespace: kube-system
  labels:
    jobgroup: backup
spec:
  template:
    metadata:
      name: etcd-backup
      labels:
        jobgroup: backup
    spec:
      containers:
      - name: etcd-backup
        image: ${ETCD_IMAGE}
        env:
        - name: ETCDCTL_API
          value: "3"
        command: ["/bin/sh"]
        args: ["-c", "etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-\$(date +%Y-%m-%d_%H:%M:%S_%Z).db"]
        volumeMounts:
        - mountPath: /etc/kubernetes/pki/etcd
          name: etcd-certs
          readOnly: true
        - mountPath: /backup
          name: etcd-backup
      restartPolicy: OnFailure
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
      - effect: NoSchedule
        operator: Exists
      hostNetwork: true
      volumes:
      - name: etcd-certs
        hostPath:
          path: /etc/kubernetes/pki/etcd
          type: DirectoryOrCreate
      - name: etcd-backup
        hostPath:
          path: ${ETCD_SNAPSHOT}
          type: Directory
*EOF*

kubectl create -f ${MANIFEST}</pre></div><p>If you are using local <code class="literal">hostPath</code> and not using a shared storage device, the <code class="literal">etcd</code> backup will be created to any one of the master nodes.
To find the node associated with each <code class="literal">etcd</code> backup run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get pods --namespace kube-system --selector=job-name=etcd-backup -o wide</pre></div></li></ol></div></li></ol></div></div></div><div class="sect2" id="_scheduling_etcd_cluster_backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scheduling etcd Cluster Backup</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_scheduling_etcd_cluster_backup">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Mount external storage device to all master nodes.
This is only required if the following step is using local <code class="literal">hostPath</code> as volume storage.</p></li><li class="listitem "><p>Create Cronjob.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Find the size of the database to be backed up</p><div id="id-1.14.3.5.2.2.2.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The backup size depends on the cluster. Ensure each of the backups has sufficient space.
The available size should be more than the database snapshot file.</p><p>You should also have a rotation method to clean up the unneeded snapshots over time.</p><p>When there is insufficient space available during backup, pods will fail to be in <code class="literal">Running</code> state and <code class="literal">no space left on device</code> errors will show in pod logs.</p><p>The below example manifest shows a binding to a local <code class="literal">hostPath</code>.
We strongly recommend using other storage methods instead.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">ls -sh /var/lib/etcd/member/snap/db</pre></div></li><li class="listitem "><p>Modify the script example</p><p>Replace <code class="literal">&lt;STORAGE_MOUNT_POINT&gt;</code> with directory to store for backup. The directory must exist on every node in cluster.</p><p>Replace <code class="literal">&lt;IN_CLUSTER_ETCD_IMAGE&gt;</code> with etcd image used in cluster.
This can be retrieved by accessing any one of the nodes in the cluster and running:</p><div class="verbatim-wrap"><pre class="screen">grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'</pre></div></li><li class="listitem "><p>Create a backup schedule deployment</p><p>Run the following script:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ETCD_SNAPSHOT="&lt;STORAGE_MOUNT_POINT&gt;/etcd_snapshot"
ETCD_IMAGE="&lt;IN_CLUSTER_ETCD_IMAGE&gt;"

# SCHEDULE in Cron format. https://crontab.guru/
SCHEDULE="0 1 * * *"

# *_HISTORY_LIMIT is the number of maximum history keep in the cluster.
SUCCESS_HISTORY_LIMIT="3"
FAILED_HISTORY_LIMIT="3"

MANIFEST="etcd-backup.yaml"

cat &lt;&lt; *EOF* &gt; ${MANIFEST}
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  startingDeadlineSeconds: 100
  schedule: "${SCHEDULE}"
  successfulJobsHistoryLimit: ${SUCCESS_HISTORY_LIMIT}
  failedJobsHistoryLimit: ${FAILED_HISTORY_LIMIT}
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: etcd-backup
            image: ${ETCD_IMAGE}
            env:
            - name: ETCDCTL_API
              value: "3"
            command: ["/bin/sh"]
            args: ["-c", "etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-\$(date +%Y-%m-%d_%H:%M:%S_%Z).db"]
            volumeMounts:
            - mountPath: /etc/kubernetes/pki/etcd
              name: etcd-certs
              readOnly: true
            - mountPath: /backup
              name: etcd-backup
          restartPolicy: OnFailure
          nodeSelector:
            node-role.kubernetes.io/master: ""
          tolerations:
          - effect: NoSchedule
            operator: Exists
          hostNetwork: true
          volumes:
          - name: etcd-certs
            hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
          - name: etcd-backup
            # hostPath is only one of the types of persistent volume. Suggest to setup external storage instead.
            hostPath:
              path: ${ETCD_SNAPSHOT}
              type: Directory
*EOF*

kubectl create -f ${MANIFEST}</pre></div></li></ol></div></li></ol></div></div></div><div class="sect1" id="_recovering_master_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering Master Nodes</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_recovering_master_nodes">#</a></h2></div></div></div><p>This chapter describes how to recover SUSE CaaS Platform master nodes.</p><div class="sect2" id="_replacing_a_single_master_node"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Single Master Node</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_replacing_a_single_master_node">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Remove the failed master node with skuba.</p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with failed cluster master node name.</p><div class="verbatim-wrap"><pre class="screen">skuba node remove &lt;NODE_NAME&gt;</pre></div></li><li class="listitem "><p>Delete failed master node from <code class="literal">known_hosts</code>.</p><p>Replace` &lt;NODE_IP&gt;` with failed master node IP address.</p><div class="verbatim-wrap"><pre class="screen">sed -i "/&lt;NODE_IP&gt;/d" known_hosts</pre></div></li><li class="listitem "><p>Prepare a new instance.</p></li><li class="listitem "><p>Use <code class="literal">skuba</code> to join master node from step 3.</p><p>Replace <code class="literal">&lt;NODE_IP&gt;</code> with the new master node ip address.</p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with the new master node name.</p><p>Replace <code class="literal">&lt;USER_NAME&gt;</code> with user name.</p><div class="verbatim-wrap"><pre class="screen">skuba node join --role=master --user=&lt;USER_NAME&gt; --sudo --target &lt;NODE_IP&gt; &lt;NODE_NAME&gt;</pre></div></li></ol></div></div><div class="sect2" id="_recovering_all_master_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering All Master Nodes</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_recovering_all_master_nodes">#</a></h3></div></div></div><p>Ensure cluster version for backup/restore should be the same. Cross-version restoration in any domain is likely to  encounter data/API compatibility issues.</p><div class="sect3" id="_prerequisites_7"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_prerequisites_7">#</a></h4></div></div></div><p>You will only need to restore database on one of the master node (<code class="literal">master-0</code>) to regain control-plane access.
etcd will sync the database to all master nodes in the cluster once restored.
This does not mean, however, that the nodes will automatically be added back to the cluster.
You must join one master node to the cluster, restore the database and then continue adding your remaining master nodes (which then will sync automatically).</p><p>Do the following on <code class="literal">master-0</code>. Remote restore is not supported.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install one of the required software packages (<code class="literal">etcdctl</code>, Docker or Podman).</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper install etcdctl</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper install docker
sudo systemctl start docker

ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

sudo docker pull ${ETCD_IMAGE}</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper install podman

ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

sudo podman pull ${ETCD_IMAGE}</pre></div></li></ul></div></li><li class="listitem "><p>Have access to <code class="literal">etcd</code> snapshot from backup device.</p></li></ol></div></div><div class="sect3" id="_procedure_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_procedure_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Stop <code class="literal">etcd</code> on all master nodes.</p><div class="verbatim-wrap"><pre class="screen">mv /etc/kubernetes/manifests/etcd.yaml /tmp/</pre></div><p>You can check <code class="literal">etcd</code> container does not exist with <code class="literal">crictl ps | grep etcd</code></p></li><li class="listitem "><p>Purge <code class="literal">etcd</code> data on all master nodes.</p><div class="verbatim-wrap"><pre class="screen">sudo rm -rf /var/lib/etcd</pre></div></li><li class="listitem "><p>Login to <code class="literal">master-0</code> via SSH.</p></li><li class="listitem "><p>Restore <code class="literal">etcd</code> data.</p><p>Replace <code class="literal">&lt;SNAPSHOT_DIR&gt;</code> with directory to the etcd snapshot,
for example: <code class="literal">/share/backup/etcd_snapshot</code></p><p>Replace <code class="literal">&lt;SNAPSHOT&gt;</code> with the name of the <code class="literal">etcd</code> snapshot,
for example: <code class="literal">etcd-snapshot-2019-11-08_05:19:20_GMT.db</code></p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with <code class="literal">master-0</code> cluster node name,
for example: <code class="literal">skuba-master-1</code></p><p>Replace <code class="literal">&lt;NODE_IP&gt;</code> with <code class="literal">master-0</code> cluster node IP address.</p><div id="id-1.14.4.4.4.2.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_IP&gt;</code> must be visible from inside the node.</p><div class="verbatim-wrap highlight bash"><pre class="screen">ip addr | grep &lt;NODE_IP&gt;</pre></div></div><div id="id-1.14.4.4.4.2.4.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_NAME&gt;</code> and <code class="literal">&lt;NODE_IP&gt;</code> must exist after <code class="literal">--initial-cluster</code> in <code class="literal">/etc/kubernetes/manifests/etcd.yaml</code></p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">SNAPSHOT="&lt;SNAPSHOT_DIR&gt;/&lt;SNAPSHOT&gt;"
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo ETCDCTL_API=3 etcdctl snapshot restore ${SNAPSHOT}\
 --data-dir /var/lib/etcd\
 --name ${NODE_NAME}\
 --initial-cluster ${NODE_NAME}=https://${NODE_IP}:2380\
 --initial-advertise-peer-urls https://${NODE_IP}:2380</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">SNAPSHOT="&lt;SNAPSHOT&gt;"
SNAPSHOT_DIR="&lt;SNAPSHOT_DIR&gt;"
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo docker run\
 -v ${SNAPSHOT_DIR}:/etcd_snapshot\
 -v /var/lib:/var/lib\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl snapshot restore /etcd_snapshot/${SNAPSHOT}\
 --data-dir /var/lib/etcd\
 --name ${NODE_NAME}\
 --initial-cluster ${NODE_NAME}=https://${NODE_IP}:2380\
 --initial-advertise-peer-urls https://${NODE_IP}:2380"</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">SNAPSHOT="&lt;SNAPSHOT&gt;"
SNAPSHOT_DIR="&lt;SNAPSHOT_DIR&gt;"
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo podman run\
 -v ${SNAPSHOT_DIR}:/etcd_snapshot\
 -v /var/lib:/var/lib\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl snapshot restore /etcd_snapshot/${SNAPSHOT}\
 --data-dir /var/lib/etcd\
 --name ${NODE_NAME}\
 --initial-cluster ${NODE_NAME}=https://${NODE_IP}:2380\
 --initial-advertise-peer-urls https://${NODE_IP}:2380"</pre></div></li></ul></div></li><li class="listitem "><p>Start <code class="literal">etcd</code> on <code class="literal">master-0</code>.</p><div class="verbatim-wrap"><pre class="screen">mv /tmp/etcd.yaml /etc/kubernetes/manifests/</pre></div></li><li class="listitem "><p>You should be able to see <code class="literal">master-0</code> joined to the <code class="literal">etcd</code> cluster member list.</p><p>Replace <code class="literal">&lt;ENDPOINT_IP&gt;</code> with <code class="literal">master-0</code> cluster node IP address.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">sudo ETCDCTL_API=3 etcdctl\
 --endpoints=https://127.0.0.1:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo docker run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo podman run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"</pre></div></li></ul></div></li><li class="listitem "><p>Add another master node to the etcd cluster member list.</p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with cluster node name,
for example: <code class="literal">skuba-master-1</code></p><p>Replace <code class="literal">&lt;ENDPOINT_IP&gt;</code> with <code class="literal">master-0</code> cluster node IP address.</p><p>Replace <code class="literal">&lt;NODE_IP&gt;</code> with cluster node IP address.</p><div id="id-1.14.4.4.4.2.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_IP&gt;</code> must be visible from inside the node.</p><div class="verbatim-wrap highlight bash"><pre class="screen">ip addr | grep &lt;NODE_IP&gt;</pre></div></div><div id="id-1.14.4.4.4.2.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_NAME&gt;</code> and <code class="literal">&lt;NODE_IP&gt;</code> must exist after <code class="literal">--initial-cluster</code> in <code class="literal">/etc/kubernetes/manifests/etcd.yaml</code></p></div><div id="id-1.14.4.4.4.2.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Nodes must be restored in sequence.</p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo ETCDCTL_API=3 etcdctl\
 --endpoints=https://127.0.0.1:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\
 member add ${NODE_NAME} --peer-urls=https://${NODE_IP}:2380</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo docker run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\
 member add ${NODE_NAME} --peer-urls=https://${NODE_IP}:2380"</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo podman run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\
 member add ${NODE_NAME} --peer-urls=https://${NODE_IP}:2380"</pre></div></li></ul></div></li><li class="listitem "><p>Login to the node in step 7 via SSH.</p></li><li class="listitem "><p>Start <code class="literal">etcd</code>.</p><div class="verbatim-wrap"><pre class="screen">cp /tmp/etcd.yaml /etc/kubernetes/manifests/</pre></div></li><li class="listitem "><p>Repeat step 7, 8, 9 to recover all remaining master nodes.</p></li></ol></div></div><div class="sect3" id="_confirming_the_restoration"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Confirming the Restoration</span> <a title="Permalink" class="permalink" href="_cluster_disaster_recovery.html#_confirming_the_restoration">#</a></h4></div></div></div><p>After restoring, execute the below command to confirm the procedure. A successful restoration will show master nodes in <code class="literal">etcd</code> member list <code class="literal">started</code>, and all Kubernetes nodes in <code class="literal">STATUS Ready</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">sudo ETCDCTL_API=3 etcdctl\
 --endpoints=https://127.0.0.1:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list

# EXAMPLE
116c1458aef748bc, started, caasp-master-cluster-2, https://172.28.0.20:2380, https://172.28.0.20:2379
3d124d6ad11cf3dd, started, caasp-master-cluster-0, https://172.28.0.26:2380, https://172.28.0.26:2379
43d2c8b1d5179c01, started, caasp-master-cluster-1, https://172.28.0.6:2380, https://172.28.0.6:2379</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

# Replace &lt;ENDPOINT_IP&gt; with `master-0` cluster node IP address.
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo docker run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"

# EXAMPLE
116c1458aef748bc, started, caasp-master-cluster-2, https://172.28.0.20:2380, https://172.28.0.20:2379
3d124d6ad11cf3dd, started, caasp-master-cluster-0, https://172.28.0.26:2380, https://172.28.0.26:2379
43d2c8b1d5179c01, started, caasp-master-cluster-1, https://172.28.0.6:2380, https://172.28.0.6:2379</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

# Replace &lt;ENDPOINT_IP&gt; with `master-0` cluster node IP address.
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo podman run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"

# EXAMPLE
116c1458aef748bc, started, caasp-master-cluster-2, https://172.28.0.20:2380, https://172.28.0.20:2379
3d124d6ad11cf3dd, started, caasp-master-cluster-0, https://172.28.0.26:2380, https://172.28.0.26:2379
43d2c8b1d5179c01, started, caasp-master-cluster-1, https://172.28.0.6:2380, https://172.28.0.6:2379</pre></div></li><li class="listitem "><p>Kubectl:</p><div class="verbatim-wrap"><pre class="screen">kubectl get nodes

# EXAMPLE
NAME                          STATUS   ROLES    AGE      VERSION
caasp-master-cluster-0        Ready    master   28m      v1.16.2
caasp-master-cluster-1        Ready    master   20m      v1.16.2
caasp-master-cluster-2        Ready    master   12m      v1.16.2
caasp-worker-cluster-0        Ready    &lt;none&gt;   36m36s   v1.16.2</pre></div></li></ul></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="backup-and-restore-with-velero.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 13 </span>Backup and Restore with Velero</span></a><a class="nav-link" href="_gpu_dependent_workloads.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 11 </span>GPU-Dependent Workloads</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>