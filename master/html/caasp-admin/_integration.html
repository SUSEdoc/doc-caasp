<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Integration | Administration Guide | SUSE CaaS Platform 4.5.1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.1" /><meta name="book-title" content="Administration Guide" /><meta name="chapter-title" content="Chapter 10. Integration" /><meta name="description" content="Integration with external systems might require you to install additional packages to the base OS. Please refer to Section 3.1, “Software Installation”." /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="AdminGuide" /><link rel="home" href="index.html" title="Administration Guide" /><link rel="up" href="index.html" title="Administration Guide" /><link rel="prev" href="_storage.html" title="Chapter 9. Storage" /><link rel="next" href="_gpu_dependent_workloads.html" title="Chapter 11. GPU-Dependent Workloads" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Administration Guide"><span class="book-icon">Administration Guide</span></a><span> › </span><a class="crumb" href="_integration.html">Integration</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Administration Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pr01.html"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="_about_this_guide.html"><span class="number">1 </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="_cluster_management.html"><span class="number">2 </span><span class="name">Cluster Management</span></a></li><li class="inactive"><a href="_software_management.html"><span class="number">3 </span><span class="name">Software Management</span></a></li><li class="inactive"><a href="_cluster_updates.html"><span class="number">4 </span><span class="name">Cluster Updates</span></a></li><li class="inactive"><a href="_upgrading_suse_caas_platform.html"><span class="number">5 </span><span class="name">Upgrading SUSE CaaS Platform</span></a></li><li class="inactive"><a href="_security.html"><span class="number">6 </span><span class="name">Security</span></a></li><li class="inactive"><a href="_logging.html"><span class="number">7 </span><span class="name">Logging</span></a></li><li class="inactive"><a href="_monitoring.html"><span class="number">8 </span><span class="name">Monitoring</span></a></li><li class="inactive"><a href="_storage.html"><span class="number">9 </span><span class="name">Storage</span></a></li><li class="inactive"><a href="_integration.html"><span class="number">10 </span><span class="name">Integration</span></a></li><li class="inactive"><a href="_gpu_dependent_workloads.html"><span class="number">11 </span><span class="name">GPU-Dependent Workloads</span></a></li><li class="inactive"><a href="_cluster_disaster_recovery.html"><span class="number">12 </span><span class="name">Cluster Disaster Recovery</span></a></li><li class="inactive"><a href="backup-and-restore-with-velero.html"><span class="number">13 </span><span class="name">Backup and Restore with Velero</span></a></li><li class="inactive"><a href="_miscellaneous.html"><span class="number">14 </span><span class="name">Miscellaneous</span></a></li><li class="inactive"><a href="_troubleshooting_3.html"><span class="number">15 </span><span class="name">Troubleshooting</span></a></li><li class="inactive"><a href="_glossary.html"><span class="number">16 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="_contributors.html"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="_gnu_licenses.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 9. Storage" href="_storage.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 11. GPU-Dependent Workloads" href="_gpu_dependent_workloads.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Administration Guide"><span class="book-icon">Administration Guide</span></a><span> › </span><a class="crumb" href="_integration.html">Integration</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 9. Storage" href="_storage.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 11. GPU-Dependent Workloads" href="_gpu_dependent_workloads.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="_integration"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.1</span></div><div><h1 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integration</span> <a title="Permalink" class="permalink" href="_integration.html#">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="_integration.html#ses-integration"><span class="number">10.1 </span><span class="name">SUSE Enterprise Storage Integration</span></a></span></dt><dt><span class="section"><a href="_integration.html#_suse_cloud_application_platform_integration"><span class="number">10.2 </span><span class="name">SUSE Cloud Application Platform Integration</span></a></span></dt></dl></div></div><div id="id-1.12.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Integration with external systems might require you to install additional packages to the base OS.
Please refer to <a class="xref" href="_software_management.html#software-installation" title="3.1. Software Installation">Section 3.1, “Software Installation”</a>.</p></div><div class="sect1" id="ses-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage Integration</span> <a title="Permalink" class="permalink" href="_integration.html#ses-integration">#</a></h2></div></div></div><p>SUSE CaaS Platform offers SUSE Enterprise Storage as a storage solution for its containers.
This chapter describes the steps required for successful integration.</p><div class="sect2" id="_prerequisites_6"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="_integration.html#_prerequisites_6">#</a></h3></div></div></div><p>Before you start with integrating SUSE Enterprise Storage, you need to ensure the following:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The SUSE CaaS Platform cluster must have <code class="literal">ceph-common</code> and <code class="literal">xfsprogs</code> installed on all nodes.
You can check this by running <code class="literal">rpm -q ceph-common</code> and <code class="literal">rpm -q xfsprogs</code>.</p></li><li class="listitem "><p>The SUSE CaaS Platform cluster can communicate with all of the following SUSE Enterprise Storage nodes:
master, monitoring nodes, OSD nodes and the metadata server (in case you need a shared file system).
For more details refer to the SUSE Enterprise Storage documentation:
<a class="link" href="https://documentation.suse.com/ses/6/" target="_blank">https://documentation.suse.com/ses/6/</a>.</p></li></ul></div></div><div class="sect2" id="_procedures_according_to_type_of_integration"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedures According to Type of Integration</span> <a title="Permalink" class="permalink" href="_integration.html#_procedures_according_to_type_of_integration">#</a></h3></div></div></div><p>The steps will differ in small details depending on whether you are using RBD or
CephFS.</p><div class="sect3" id="_using_rbd_in_pods"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using RBD in Pods</span> <a title="Permalink" class="permalink" href="_integration.html#_using_rbd_in_pods">#</a></h4></div></div></div><p>RBD, also known as the Ceph Block Device or RADOS Block Device,
facilitates the storage of block-based data in the Ceph distributed storage system.
The procedure below describes steps to take when you need to use a RADOS Block Device in a Kubernetes Pod.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool" target="_blank">Create a Ceph Pool</a>:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create myPool 64 64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-pool" target="_blank">Create a Block Device Pool</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd pool init myPool</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#creating-a-block-device-image" target="_blank">Create a Block Device Image</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd create -s 2G myPool/image</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a>, and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolUser mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=myPool" | tr -d '\n' | base64</pre></div></li><li class="listitem "><p>Create the Secret containing <code class="literal">client.myPoolUser</code> key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
name: ceph-user
namespace: default
type: kubernetes.io/rbd
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1Jreplace== <span id="CO37-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO37-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: ceph-rbd-inline
spec:
  containers:
  - name: ceph-rbd-inline
    image: opensuse/leap
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/ceph_rbd <span id="CO38-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    rbd:
      monitors:
      - 10.244.2.136:6789 <span id="CO38-2"></span><span class="callout">2</span>
      - 10.244.3.123:6789
      - 10.244.4.7:6789
      pool: myPool <span id="CO38-3"></span><span class="callout">3</span>
      image: image <span id="CO38-4"></span><span class="callout">4</span>
      user: myPoolUser <span id="CO38-5"></span><span class="callout">5</span>
      secretRef:
        name: ceph-user <span id="CO38-6"></span><span class="callout">6</span>
      fsType: ext4
      readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Ceph volume image.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool user.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool user key.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/ceph-rbd-inline -- df -k | grep rbd
  Filesystem     1K-blocks    Used Available Use% Mounted on
  /dev/rbd0        1998672    6144   1976144   1% /mnt/ceph_rbd</pre></div></li></ol></div></div><div class="sect3" id="_using_rbd_in_persistent_volumes"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using RBD in Persistent Volumes</span> <a title="Permalink" class="permalink" href="_integration.html#_using_rbd_in_persistent_volumes">#</a></h4></div></div></div><p>The following procedure describes how to use RBD in a Persistent Volume:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool" target="_blank">Create a Ceph pool</a>:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create myPool 64 64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-pool" target="_blank">Create a Block Device Pool</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd pool init myPool</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#creating-a-block-device-image" target="_blank">Create a Block Device Image</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd create -s 2G myPool/image</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a>, and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolUser mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=myPool" | tr -d '\n' | base64</pre></div></li><li class="listitem "><p>Create the Secret containing <code class="literal">client.myPoolUser</code> key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
name: ceph-user
namespace: default
type: kubernetes.io/rbd
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1Jreplace== <span id="CO39-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO39-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
  name: ceph-rbd-pv
spec:
  capacity:
    storage: 2Gi <span id="CO40-1"></span><span class="callout">1</span>
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - 172.28.0.25:6789 <span id="CO40-2"></span><span class="callout">2</span>
    - 172.28.0.21:6789
    - 172.28.0.6:6789
    pool: myPool  <span id="CO40-3"></span><span class="callout">3</span>
    image: image <span id="CO40-4"></span><span class="callout">4</span>
    user: myPoolUser  <span id="CO40-5"></span><span class="callout">5</span>
    secretRef:
      name: ceph-user <span id="CO40-6"></span><span class="callout">6</span>
    fsType: ext4
    readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO40-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The size of the volume image. Reference to <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage" target="_blank">Setting requests and limits for local ephemeral storage</a> to see supported suffixes.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO40-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO40-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO40-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Ceph volume image name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO40-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool user.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO40-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool user key.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume Claim:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-rbd-pv
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  volumeName: ceph-rbd-pv</pre></div></div><div id="id-1.12.3.4.4.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Deleting Persistent Volume Claim does not remove RBD volume in the Ceph cluster.</p></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: ceph-rbd-pv
spec:
  containers:
  - name: ceph-rbd-pv
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/ceph_rbd <span id="CO41-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    persistentVolumeClaim:
      claimName: ceph-rbd-pv <span id="CO41-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO41-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO41-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Persistent Volume Claim name.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/ceph-rbd-pv -- df -k | grep rbd
  Filesystem     1K-blocks    Used Available Use% Mounted on
  /dev/rbd0        1998672    6144   1976144   1% /mnt/ceph_rbd</pre></div></li></ol></div></div><div class="sect3" id="_using_rbd_in_storage_classes"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using RBD in Storage Classes</span> <a title="Permalink" class="permalink" href="_integration.html#_using_rbd_in_storage_classes">#</a></h4></div></div></div><p>The following procedure describes how use RBD in Storage Class:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool" target="_blank">Create a Ceph pool</a>:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create myPool 64 64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as pool admin and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolAdmin mds 'allow *' mgr 'allow *' mon 'allow *' osd 'allow * pool=myPool'  | tr -d '\n' | base64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as pool user and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolUser mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=myPool" | tr -d '\n' | base64</pre></div></li><li class="listitem "><p>Create the Secret containing the block device pool admin key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
 name: ceph-admin
type: kubernetes.io/rbd
data:
  key: QVFCa0ZJVmZBQUFBQUJBQUp2VzdLbnNIOU1yYll1R0p6T2Zreplace== <span id="CO42-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO42-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device pool admin key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Secret containing the block device pool user key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
 name: ceph-user
type: kubernetes.io/rbd
data:
  key: QVFCa0ZJVmZBQUFBQUJBQUp2VzdLbnNIOU1yYll1R0p6T2Zreplace== <span id="CO43-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO43-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device pool user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Storage Class:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: ceph-rbd-sc
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.28.0.19:6789, 172.28.0.5:6789, 172.218:6789 <span id="CO44-1"></span><span class="callout">1</span>
  adminId: myPoolAdmin <span id="CO44-2"></span><span class="callout">2</span>
  adminSecretName: ceph-admin <span id="CO44-3"></span><span class="callout">3</span>
  adminSecretNamespace: default
  pool: myPool <span id="CO44-4"></span><span class="callout">4</span>
  userId: myPoolUser <span id="CO44-5"></span><span class="callout">5</span>
  userSecretName: ceph-user <span id="CO44-6"></span><span class="callout">6</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitory nodes IP and port separate by <code class="literal">,</code>. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool admin name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool admin key.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool user name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool user key.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume Claim:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-rbd-sc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi <span id="CO45-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The request volume size. Reference to <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage" target="_blank">Setting requests and limits for local ephemeral storage</a> to see supported suffixes.</p></td></tr></table></div><div id="id-1.12.3.4.5.3.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Deleting Persistent Volume Claim does not remove RBD volume in the Ceph cluster.</p></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: ceph-rbd-sc
spec:
  containers:
  - name:  ceph-rbd-sc
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/ceph_rbd <span id="CO46-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    persistentVolumeClaim:
      claimName: ceph-rbd-sc <span id="CO46-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Persistent Volume Claim name.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/ceph-rbd-sc -- df -k | grep rbd
  Filesystem     1K-blocks    Used Available Use% Mounted on
  /dev/rbd0        1998672    6144   1976144   1% /mnt/ceph_rbd</pre></div></li></ol></div></div><div class="sect3" id="_using_cephfs_in_pods"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using CephFS in Pods</span> <a title="Permalink" class="permalink" href="_integration.html#_using_cephfs_in_pods">#</a></h4></div></div></div><p>The procedure below describes how to use CephFS in Pod.</p><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Procedure: Using CephFS In Pods </span><a title="Permalink" class="permalink" href="_integration.html#id-1.12.3.4.6.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as CephFS user and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myCephFSUser mds 'allow *' mgr 'allow *' mon 'allow r' osd 'allow rw pool=cephfs_metadata,allow rwx pool=cephfs_data'  | tr -d '\n' | base64</pre></div><div id="id-1.12.3.4.6.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The <code class="literal">cephfs_data</code> pool should be pre-existed with SES deployment, if not you can create and initialize with:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create cephfs_data 256 256
ceph osd pool create cephfs_metadata 64 64
ceph fs new cephfs cephfs_metadata cephfs_data</pre></div></div><div id="id-1.12.3.4.6.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p><a class="link" href="https://docs.ceph.com/en/latest/cephfs/experimental-features/#multiple-file-systems-within-a-ceph-cluster" target="_blank">Multiple Filesystems Within a Ceph Cluster</a> is still an experimental feature, and disabled by default, to setup more than one filesystem requires to have this feature enabled.
See <a class="link" href="https://docs.ceph.com/en/latest/cephfs/createfs/#create-a-ceph-file-system" target="_blank">Create a Ceph File System</a> on how to create more filesystems.</p></div><div id="id-1.12.3.4.6.3.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Reference to <a class="link" href="https://docs.ceph.com/en/latest/cephfs/client-auth/#cephfs-client-capabilities" target="_blank">CephFS Client Capabilities</a> to see how to further restrict user authority.</p></div></li><li class="listitem "><p>Create the Secret containing the CephFS admin key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: ceph-user
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1J4ZStreplace== <span id="CO47-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The CephFS user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: cephfs-inline
spec:
  containers:
  - name: cephfs-inline
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/cephfs <span id="CO48-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    cephfs:
      monitors:
      - 172.28.0.19:6789 <span id="CO48-2"></span><span class="callout">2</span>
      - 172.28.0.5:6789
      - 172.28.0.18:6789
      user: myCephFSUser <span id="CO48-3"></span><span class="callout">3</span>
      secretRef:
        name: ceph-user <span id="CO48-4"></span><span class="callout">4</span>
      readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The CephFS user name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the CephFS user key.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/cephfs-inline -- df -k | grep cephfs
  Filesystem   1K-blocks    Used Available Use% Mounted on
  172.28.0.19:6789,172.28.0.5:6789,172.28.0.18:6789:/
                79245312       0  79245312   0% /mnt/cephfs</pre></div></li></ol></div></div><div class="sect3" id="_using_cephfs_in_persistent_volumes"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using CephFS in Persistent Volumes</span> <a title="Permalink" class="permalink" href="_integration.html#_using_cephfs_in_persistent_volumes">#</a></h4></div></div></div><p>The following procedure describes how to attach a CephFS static persistent volume to a pod:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as CephFS user and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myCephFSUser mds 'allow *' mgr 'allow *' mon 'allow r' osd 'allow rw pool=cephfs_metadata,allow rwx pool=cephfs_data'  | tr -d '\n' | base64</pre></div><div id="id-1.12.3.4.7.3.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The <code class="literal">cephfs_data</code> pool should be pre-existed with SES deployment, if not you can create and initialize with:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create cephfs_data 256 256
ceph osd pool create cephfs_metadata 64 64
ceph fs new cephfs cephfs_metadata cephfs_data</pre></div></div><div id="id-1.12.3.4.7.3.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p><a class="link" href="https://docs.ceph.com/en/latest/cephfs/experimental-features/#multiple-file-systems-within-a-ceph-cluster" target="_blank">Multiple Filesystems Within a Ceph Cluster</a> is still an experimental feature, and disabled by default, to setup more than one filesystem requires to have this feature enabled.
See <a class="link" href="https://docs.ceph.com/en/latest/cephfs/createfs/#create-a-ceph-file-system" target="_blank">Create a Ceph File System</a> on how to create more filesystem.</p></div><div id="id-1.12.3.4.7.3.1.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Reference to <a class="link" href="https://docs.ceph.com/en/latest/cephfs/client-auth/#cephfs-client-capabilities" target="_blank">CephFS Client Capabilities</a> to see how to further restrict user authority.</p></div></li><li class="listitem "><p>Create the Secret that contains the created CephFS admin key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: ceph-user
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1J4ZStreplace== <span id="CO49-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The CephFS user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
  name: cephfs-pv
spec:
  capacity:
    storage: 2Gi <span id="CO50-1"></span><span class="callout">1</span>
  accessModes:
    - ReadWriteOnce
  cephfs:
    monitors:
      - 172.28.0.19:6789 <span id="CO50-2"></span><span class="callout">2</span>
      - 172.28.0.5:6789
      - 172.28.0.18:6789
    user: myCephFSUser <span id="CO50-3"></span><span class="callout">3</span>
    secretRef:
      name: ceph-user <span id="CO50-4"></span><span class="callout">4</span>
    readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The desired volume size. Reference to <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage" target="_blank">Setting requests and limits for local ephemeral storage</a> to see supported suffixes.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The CephFS user name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the CephFS user key.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume Claim:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs-pv
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi <span id="CO51-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO51-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The request volume size.</p></td></tr></table></div><div id="id-1.12.3.4.7.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Deleting Persistent Volume Claim does not remove CephFS volume in the Ceph cluster.</p></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: cephfs-pv
spec:
  containers:
  - name: cephfs-pv
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/cephfs <span id="CO52-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    persistentVolumeClaim:
      claimName: cephfs-pv <span id="CO52-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Persistent Volume Claim name.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the CephFS is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/cephfs-pv -- df -k | grep cephfs
  Filesystem   1K-blocks    Used Available Use% Mounted on
  172.28.0.19:6789,172.28.0.5:6789,172.28.0.18:6789:/
                79245312       0  79245312   0% /mnt/cephfs</pre></div></li></ol></div></div></div></div><div class="sect1" id="_suse_cloud_application_platform_integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Cloud Application Platform Integration</span> <a title="Permalink" class="permalink" href="_integration.html#_suse_cloud_application_platform_integration">#</a></h2></div></div></div><p>For integration with SUSE Cloud Application Platform, refer to: <a class="link" href="https://documentation.suse.com/suse-cap/1.5.2/single-html/cap-guides/#cha-cap-depl-caasp" target="_blank">Deploying SUSE Cloud Application Platform on SUSE CaaS Platform</a>.</p></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="_gpu_dependent_workloads.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 11 </span>GPU-Dependent Workloads</span></a><a class="nav-link" href="_storage.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 9 </span>Storage</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>