<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Cluster Management | Administration Guide | SUSE CaaS Platform 4.5.2</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.2" /><meta name="book-title" content="Administration Guide" /><meta name="chapter-title" content="Chapter 2. Cluster Management" /><meta name="description" content="Cluster management refers to several processes in the life cycle of a cluster and its individual nodes: bootstrapping, joining and removing nodes. For maximum automation and ease SUSE CaaS Platform uses the skuba tool, which simplifies Kubernetes cluster creation and reconfiguration." /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="AdminGuide" /><link rel="home" href="index.html" title="Administration Guide" /><link rel="up" href="index.html" title="Administration Guide" /><link rel="prev" href="_about_this_guide.html" title="Chapter 1. About This Guide" /><link rel="next" href="_software_management.html" title="Chapter 3. Software Management" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Administration Guide"><span class="book-icon">Administration Guide</span></a><span> › </span><a class="crumb" href="_cluster_management.html">Cluster Management</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Administration Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pr01.html"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="_about_this_guide.html"><span class="number">1 </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="_cluster_management.html"><span class="number">2 </span><span class="name">Cluster Management</span></a></li><li class="inactive"><a href="_software_management.html"><span class="number">3 </span><span class="name">Software Management</span></a></li><li class="inactive"><a href="_cluster_updates.html"><span class="number">4 </span><span class="name">Cluster Updates</span></a></li><li class="inactive"><a href="_upgrading_suse_caas_platform.html"><span class="number">5 </span><span class="name">Upgrading SUSE CaaS Platform</span></a></li><li class="inactive"><a href="_security.html"><span class="number">6 </span><span class="name">Security</span></a></li><li class="inactive"><a href="_logging.html"><span class="number">7 </span><span class="name">Logging</span></a></li><li class="inactive"><a href="_monitoring.html"><span class="number">8 </span><span class="name">Monitoring</span></a></li><li class="inactive"><a href="_storage.html"><span class="number">9 </span><span class="name">Storage</span></a></li><li class="inactive"><a href="_integration.html"><span class="number">10 </span><span class="name">Integration</span></a></li><li class="inactive"><a href="_gpu_dependent_workloads.html"><span class="number">11 </span><span class="name">GPU-Dependent Workloads</span></a></li><li class="inactive"><a href="_cluster_disaster_recovery.html"><span class="number">12 </span><span class="name">Cluster Disaster Recovery</span></a></li><li class="inactive"><a href="backup-and-restore-with-velero.html"><span class="number">13 </span><span class="name">Backup and Restore with Velero</span></a></li><li class="inactive"><a href="_miscellaneous.html"><span class="number">14 </span><span class="name">Miscellaneous</span></a></li><li class="inactive"><a href="_troubleshooting_3.html"><span class="number">15 </span><span class="name">Troubleshooting</span></a></li><li class="inactive"><a href="_glossary.html"><span class="number">16 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="_contributors.html"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="_gnu_licenses.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 1. About This Guide" href="_about_this_guide.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 3. Software Management" href="_software_management.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Administration Guide"><span class="book-icon">Administration Guide</span></a><span> › </span><a class="crumb" href="_cluster_management.html">Cluster Management</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 1. About This Guide" href="_about_this_guide.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 3. Software Management" href="_software_management.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="_cluster_management"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.2</span></div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Management</span> <a title="Permalink" class="permalink" href="_cluster_management.html#">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="_cluster_management.html#_prerequisites"><span class="number">2.1 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="_cluster_management.html#_bootstrap_and_initial_configuration"><span class="number">2.2 </span><span class="name">Bootstrap and Initial Configuration</span></a></span></dt><dt><span class="section"><a href="_cluster_management.html#adding-nodes"><span class="number">2.3 </span><span class="name">Adding Nodes</span></a></span></dt><dt><span class="section"><a href="_cluster_management.html#removing-nodes"><span class="number">2.4 </span><span class="name">Removing Nodes</span></a></span></dt><dt><span class="section"><a href="_cluster_management.html#_reconfiguring_nodes"><span class="number">2.5 </span><span class="name">Reconfiguring Nodes</span></a></span></dt><dt><span class="section"><a href="_cluster_management.html#node-operations"><span class="number">2.6 </span><span class="name">Node Operations</span></a></span></dt><dt><span class="section"><a href="_cluster_management.html#shutdown-startup"><span class="number">2.7 </span><span class="name">Graceful Cluster Shutdown &amp; Startup</span></a></span></dt><dt><span class="section"><a href="_cluster_management.html#_post_startup_activities"><span class="number">2.8 </span><span class="name">Post Startup Activities</span></a></span></dt></dl></div></div><p>Cluster management refers to several processes in the life cycle of a cluster and
its individual nodes: bootstrapping, joining and removing nodes.
For maximum automation and ease SUSE CaaS Platform uses the <code class="literal">skuba</code> tool,
which simplifies Kubernetes cluster creation and reconfiguration.</p><div class="sect1" id="_prerequisites"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_prerequisites">#</a></h2></div></div></div><p>You must have the proper SSH keys for accessing the nodes set up and allow passwordless <code class="literal">sudo</code>
on the nodes in order to perform many of these steps. If you have followed the standard
deployment procedures this should already be the case.</p><p>Please note: If you are using a different management workstation than the one you have
used during the initial deployment, you might have to transfer the SSH identities
from the original management workstation.</p></div><div class="sect1" id="_bootstrap_and_initial_configuration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bootstrap and Initial Configuration</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_bootstrap_and_initial_configuration">#</a></h2></div></div></div><p>Bootstrapping the cluster is the initial process of starting up a minimal
viable cluster and joining the first master node. Only the first master node needs to be bootstrapped,
later nodes can simply be joined as described in <a class="xref" href="_cluster_management.html#adding-nodes" title="2.3. Adding Nodes">Section 2.3, “Adding Nodes”</a>.</p><p>Before bootstrapping any nodes to the cluster,
you need to create an initial cluster definition folder (initialize the cluster).
This is done using <code class="literal">skuba cluster init</code> and its <code class="literal">--control-plane</code> flag.</p><p>For a step by step guide on how to initialize the cluster, configure updates using <code class="literal">kured</code>
and subsequently bootstrap nodes to it, refer to the <span class="emphasis"><em>SUSE CaaS Platform Deployment Guide</em></span>.</p></div><div class="sect1" id="adding-nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Nodes</span> <a title="Permalink" class="permalink" href="_cluster_management.html#adding-nodes">#</a></h2></div></div></div><p>Once you have added the first master node to the cluster using <code class="literal">skuba node bootstrap</code>,
use the <code class="literal">skuba node join</code> command to add more nodes. Joining master or worker nodes to
an existing cluster should be done sequentially, meaning the nodes have to be added
one after another and not more of them in parallel.</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role &lt;MASTER/WORKER&gt; --user &lt;USER_NAME&gt; --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;</pre></div><p>The mandatory flags for the join command are <code class="literal">--role</code>, <code class="literal">--user</code>, <code class="literal">--sudo</code> and <code class="literal">--target</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">--role</code> serves to specify if the node is a <span class="strong"><strong>master</strong></span> or <span class="strong"><strong>worker</strong></span>.</p></li><li class="listitem "><p><code class="literal">--sudo</code> is for running the command with superuser privileges,
which is necessary for all node operations.</p></li><li class="listitem "><p><code class="literal">&lt;USER_NAME&gt;</code> is the name of the user that exists on your SLES machine (default: <code class="literal">sles</code>).</p></li><li class="listitem "><p><code class="literal">--target &lt;IP/FQDN&gt;</code> is the IP address or FQDN of the relevant machine.</p></li><li class="listitem "><p><code class="literal">&lt;NODE_NAME&gt;</code> is how you decide to name the node you are adding.</p></li></ul></div><div id="id-1.4.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>New master nodes that you didn’t initially include in your Terraform’s configuration have
to be manually added to your load balancer’s configuration.</p></div><p>To add a new <span class="strong"><strong>worker</strong></span> node, you would run something like:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role worker --user sles --sudo --target 10.86.2.164 worker1</pre></div><div class="sect2" id="_adding_nodes_from_template"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Nodes from Template</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_adding_nodes_from_template">#</a></h3></div></div></div><p>If you are using a virtual machine template for creating new cluster nodes,
you must make sure that <span class="strong"><strong>before</strong></span> joining the cloned machine to the cluster it is updated to the same software versions
than the other nodes in the cluster.</p><p>Refer to <a class="xref" href="_cluster_updates.html#handling-updates" title="4.1. Update Requirements">Section 4.1, “Update Requirements”</a>.</p><p>Nodes with mismatching package or container software
versions might not be fully functional.</p></div></div><div class="sect1" id="removing-nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing Nodes</span> <a title="Permalink" class="permalink" href="_cluster_management.html#removing-nodes">#</a></h2></div></div></div><div class="sect2" id="_temporary_removal"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Temporary Removal</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_temporary_removal">#</a></h3></div></div></div><p>If you wish to remove a node temporarily, the recommended approach is to first drain the node.</p><p>When you want to bring the node back, you only have to uncordon it.</p><div id="id-1.4.6.2.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>For instructions on how to perform these operations refer to <a class="xref" href="_cluster_management.html#node-operations" title="2.6. Node Operations">Section 2.6, “Node Operations”</a>.</p></div></div><div class="sect2" id="_permanent_removal"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Permanent Removal</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_permanent_removal">#</a></h3></div></div></div><div id="id-1.4.6.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Nodes removed with this method cannot be added back to the cluster or any other
skuba-initiated cluster. You must reinstall the entire node and then join it
again to the cluster.</p></div><p>The <code class="literal">skuba node remove</code> command serves to <span class="strong"><strong>permanently</strong></span> remove nodes.
Running this command will work even if the target virtual machine is down,
so it is the safest way to remove the node.</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node remove &lt;NODE_NAME&gt; [flags]</pre></div><div id="id-1.4.6.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Per default, node removal has an unlimited timeout on waiting for the node to drain.
If the node is unreachable it can not be drained and thus the removal will fail or get stuck indefinitely.
You can specify a time after which removal will be performed without waiting for the node to
drain with the flag <code class="literal">--drain-timeout &lt;DURATION&gt;</code>.</p><p>For example, waiting for the node to drain for 1 minute and 5 seconds:</p><div class="verbatim-wrap"><pre class="screen">skuba node remove caasp-worker1 --drain-timeout 1m5s</pre></div><p>For a list of supported time formats run <code class="literal">skuba node remove -h</code>.</p></div><div id="id-1.4.6.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>After the removal of a master node, you have to manually delete its entries
from your load balancer’s configuration.</p></div></div></div><div class="sect1" id="_reconfiguring_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring Nodes</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_reconfiguring_nodes">#</a></h2></div></div></div><p>To reconfigure a node, for example to change the node’s role from worker to master,
you will need to use a combination of commands.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Run <code class="literal">skuba node remove &lt;NODE_NAME&gt;</code>.</p></li><li class="listitem "><p>Reinstall the node from scratch.</p></li><li class="listitem "><p>Run <code class="literal">skuba node join --role &lt;DESIRED_ROLE&gt; --user &lt;USER_NAME&gt; --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;</code>.</p></li></ol></div></div><div class="sect1" id="node-operations"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Operations</span> <a title="Permalink" class="permalink" href="_cluster_management.html#node-operations">#</a></h2></div></div></div><div class="sect2" id="_uncordon_and_cordon"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Uncordon and Cordon</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_uncordon_and_cordon">#</a></h3></div></div></div><p>These to commands respectively define if a node is marked as <code class="literal">schedulable</code> or <code class="literal">unschedulable</code>.
This means that a node is allowed to or not allowed to receive any new workloads.
This can be useful when troubleshooting a node.</p><p>To mark a node as <code class="literal">unschedulable</code> run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl cordon &lt;NODE_NAME&gt;</pre></div><p>To mark a node as <code class="literal">schedulable</code> run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl uncordon &lt;NODE_NAME&gt;</pre></div></div><div class="sect2" id="_draining_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Draining Nodes</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_draining_nodes">#</a></h3></div></div></div><p>Draining a node consists of evicting all the running pods from the current node in order to perform maintenance.
This is a mandatory step in order to ensure a proper functioning of the workloads.
This is achieved using <code class="literal">kubectl</code>.</p><p>To drain a node run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl drain &lt;NODE_NAME&gt;</pre></div><p>This action will also implicitly cordon the node.
Therefore once the maintenance is done, uncordon the node to set it back to schedulable.</p><p>Refer to the official Kubernetes documentation for more information:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service</a></p></div></div><div class="sect1" id="shutdown-startup"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Graceful Cluster Shutdown &amp; Startup</span> <a title="Permalink" class="permalink" href="_cluster_management.html#shutdown-startup">#</a></h2></div></div></div><p>In some scenarios like maintenance windows in your datacenter or some disaster scenarios,
you will want to shut down the cluster in a controlled fashion and later on bring it back up safely.
Follow the following instructions to safely stop all workloads.</p><div class="sect2" id="_cluster_shutdown"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Shutdown</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_cluster_shutdown">#</a></h3></div></div></div><div id="id-1.4.9.3.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Document Scope</h6><p>This document is only concerned with shutting down the SUSE CaaS Platform cluster itself.</p></div><div id="id-1.4.9.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Storage Shutdown/Startup</h6><p>Any real time data streaming workloads will lose data if not rerouted to an alternative cluster.</p><p>Any workloads that hold data only in memory will lose this data.
Please check with the provider of your workload/application about proper data persistence in case of shutdown.</p><p>Any external storage services must be stopped/started separately. Please refer to the respective storage solution’s documentation.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a <a class="link" href="https://documentation.suse.com/suse-caasp/4.5//single-html/caasp-admin/#_backup" target="_blank">backup</a> of your cluster.</p></li><li class="listitem "><p>Scale all applications down to zero by using either the manifests or deployment names:</p><div id="id-1.4.9.3.4.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Do not scale down cluster services.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale --replicas=0 -f deployment.yaml</pre></div><p>or</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale deploy my-deployment --replicas=0</pre></div></li><li class="listitem "><p>Drain/cordon all worker nodes.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl drain &lt;node name&gt;</pre></div><div id="id-1.4.9.3.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Wait for the command to finish by itself, if it fails check for <a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/" target="_blank">Help</a></p></div></li><li class="listitem "><p>Run <code class="literal">kubectl get nodes</code> and make sure all your worker nodes have the status <code class="literal">Ready,SchedulingDisabled</code>.</p></li><li class="listitem "><p>Proceed to shutdown all your <code class="literal">worker</code> nodes on the machine level.</p></li><li class="listitem "><p>Now it is necessary to find out where the <code class="literal">etcd</code> leader is running, that is going to be the last node to shut down.
Find out which pods are running <code class="literal">etcd</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl get pods -n kube-system -o wide
NAME                         READY   STATUS    RESTARTS   AGE    IP              NODE                     NOMINATED NODE   READINESS GATES
...
etcd-master-pimp-general-00  1/1     Running   0          23m     10.84.73.114   master-pimp-general-00   &lt;none&gt;           &lt;none&gt;
...</pre></div></li><li class="listitem "><p>Then you need to get the list of active <code class="literal">etcd</code> members, this will also show which <code class="literal">master</code> node is currently the <code class="literal">etcd</code> leader.
Either run a terminal session on one of the <code class="literal">etcd</code> pods:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl exec -ti -n kube-system etcd-master01 -- sh

# Now run this command
etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list</pre></div><p>or directly execute the command on the pod:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl exec -ti -n kube-system etcd-master01 -- etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list</pre></div><p>The output will be the same. Note the boolean values at the end of each line.
The current <code class="literal">etcd</code> leader will have <code class="literal">true</code>.
In this case the node <code class="literal">master02</code> is the current <code class="literal">etcd</code> leader.</p><div class="verbatim-wrap"><pre class="screen">356ebc35f3e8b25, started, master02, https://172.28.0.16:2380, https://172.28.0.16:2379, true
bdef0dced3caa0d4, started, master01, https://172.28.0.15:2380, https://172.28.0.15:2379, false
f9ae57d57b369ede, started, master03, https://172.28.0.21:2380, https://172.28.0.21:2379, false</pre></div></li><li class="listitem "><p>Shutdown all other master nodes, leaving the current <code class="literal">etcd</code> leader for last.</p></li><li class="listitem "><p>Finally, shut down the <code class="literal">etcd</code> leader node.</p><div id="id-1.4.9.3.4.9.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>This is the first node that needs to be started back up.</p></div></li></ol></div></div><div class="sect2" id="_cluster_startup"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Startup</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_cluster_startup">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To start up your cluster again, first start your <code class="literal">etcd</code> leader and wait until you get status <code class="literal">Ready</code>, like this:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   NotReady   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   NotReady   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Start the rest of the <code class="literal">master</code> nodes, and wait for them to become <code class="literal">Ready</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Start all the workers, wait until you see them on status <code class="literal">Ready,SchedulingDisabled</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   Ready,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   Ready,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Run the command <code class="literal">kubectl uncordon &lt;WORKER-NODE&gt;</code>, for each of the worker nodes, your cluster status should now be completely <code class="literal">Ready</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   Ready   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   Ready   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Bring back all your processes by scaling them up again:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale --replicas=N -f deployment.yaml</pre></div><p>or</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale deploy my-deployment --replicas=N</pre></div><div id="id-1.4.9.4.2.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Replace N with the number of replicas you want running.</p></div></li></ol></div></div></div><div class="sect1" id="_post_startup_activities"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post Startup Activities</span> <a title="Permalink" class="permalink" href="_cluster_management.html#_post_startup_activities">#</a></h2></div></div></div><p>Verify that all of your workloads and applications have resumed operation properly.</p></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="_software_management.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 3 </span>Software Management</span></a><a class="nav-link" href="_about_this_guide.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 1 </span>About This Guide</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>