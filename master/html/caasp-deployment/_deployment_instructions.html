<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Deployment Instructions | Deployment Guide | SUSE CaaS Platform 4.5.2</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.2" /><meta name="book-title" content="Deployment Guide" /><meta name="chapter-title" content="Chapter 3. Deployment Instructions" /><meta name="description" content="If you are installing over one of the previous milestones, you must remove the RPM repository. SUSE CaaS Platform is now distributed as an extension for SUSE Linux Enterprise and no longer requires the separate repository." /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="DeploymentGuide" /><link rel="home" href="index.html" title="Deployment Guide" /><link rel="up" href="index.html" title="Deployment Guide" /><link rel="prev" href="_deployment_scenarios.html" title="Chapter 2. Deployment Scenarios" /><link rel="next" href="bootstrap.html" title="Chapter 4. Bootstrapping the Cluster" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Deployment Guide"><span class="book-icon">Deployment Guide</span></a><span> › </span><a class="crumb" href="_deployment_instructions.html">Deployment Instructions</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Deployment Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pr01.html"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="_about_this_guide.html"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="deployment-system-requirements.html"><span class="number">1 </span><span class="name">Requirements</span></a></li><li class="inactive"><a href="_deployment_scenarios.html"><span class="number">2 </span><span class="name">Deployment Scenarios</span></a></li><li class="inactive"><a href="_deployment_instructions.html"><span class="number">3 </span><span class="name">Deployment Instructions</span></a></li><li class="inactive"><a href="bootstrap.html"><span class="number">4 </span><span class="name">Bootstrapping the Cluster</span></a></li><li class="inactive"><a href="_cilium_network_policy_config_examples.html"><span class="number">5 </span><span class="name">Cilium Network Policy Config Examples</span></a></li><li class="inactive"><a href="_glossary.html"><span class="number">6 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="_contributors.html"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="_gnu_licenses.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 2. Deployment Scenarios" href="_deployment_scenarios.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 4. Bootstrapping the Cluster" href="bootstrap.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Deployment Guide"><span class="book-icon">Deployment Guide</span></a><span> › </span><a class="crumb" href="_deployment_instructions.html">Deployment Instructions</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 2. Deployment Scenarios" href="_deployment_scenarios.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 4. Bootstrapping the Cluster" href="bootstrap.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="_deployment_instructions"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.2</span></div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Instructions</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="_deployment_instructions.html#deployment-preparations"><span class="number">3.1 </span><span class="name">Deployment Preparations</span></a></span></dt><dt><span class="section"><a href="_deployment_instructions.html#_deployment_on_suse_openstack_cloud"><span class="number">3.2 </span><span class="name">Deployment on SUSE OpenStack Cloud</span></a></span></dt><dt><span class="section"><a href="_deployment_instructions.html#_deployment_on_vmware"><span class="number">3.3 </span><span class="name">Deployment on VMware</span></a></span></dt><dt><span class="section"><a href="_deployment_instructions.html#deployment-bare-metal"><span class="number">3.4 </span><span class="name">Deployment on Bare Metal or KVM</span></a></span></dt><dt><span class="section"><a href="_deployment_instructions.html#_deployment_on_existing_sles_installation"><span class="number">3.5 </span><span class="name">Deployment on Existing SLES Installation</span></a></span></dt><dt><span class="section"><a href="_deployment_instructions.html#_deployment_on_amazon_web_services_aws"><span class="number">3.6 </span><span class="name">Deployment on Amazon Web Services (AWS)</span></a></span></dt></dl></div></div><div id="id-1.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If you are installing over one of the previous milestones,  you must remove the
RPM repository. SUSE CaaS Platform is now distributed as an extension for
SUSE Linux Enterprise and no longer requires the separate repository.</p><p>If you do not remove the repository before installation, there might be conflicts
with the package dependencies that could render your installation nonfunctional.</p></div><div id="id-1.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Due to a naming convention conflict, all versions of SUSE CaaS Platform 4.x up to 4.5 will be released in the <code class="literal">4.0</code> module.
Starting with 4.5 the product will be delivered in the <code class="literal">4.5</code> module.</p></div><div class="sect1" id="deployment-preparations"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Preparations</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#deployment-preparations">#</a></h2></div></div></div><p>In order to deploy SUSE CaaS Platform you need a workstation running SUSE Linux Enterprise Server 15 SP2 or similar openSUSE equivalent.
This workstation is called the "Management machine". Important files are generated
and must be maintained on this machine, but it is not a member of the SUSE CaaS Platform cluster.</p><div class="sect2" id="ssh-configuration"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Basic SSH Key Configuration</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#ssh-configuration">#</a></h3></div></div></div><p><span class="strong"><strong>In order to successfully deploy SUSE CaaS Platform, you need to have SSH keys loaded into an SSH agent.</strong></span> This is important, because it is required in order to use the installation tools <code class="literal">skuba</code> and <code class="literal">terraform</code>.</p><div id="id-1.6.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The use of <code class="literal">ssh-agent</code> comes with some implications for security that you should take into consideration.</p><p><a class="link" href="http://rabexc.org/posts/pitfalls-of-ssh-agents" target="_blank">The pitfalls of using ssh-agent</a></p><p>To avoid these risks please make sure to either use <code class="literal">ssh-agent -t &lt;TIMEOUT&gt;</code> and specify a time
after which the agent will self-terminate, or terminate the agent yourself before logging out by running <code class="literal">ssh-agent -k</code>.</p></div><p>To log in to the created cluster nodes from the Management machine, you need to configure an SSH key pair.
This key pair needs to be trusted by the user account you will log in with into each cluster node; that user is called "sles" by default.
In order to use the installation tools <code class="literal">terraform</code> and <code class="literal">skuba</code>, this trusted keypair must be loaded into the SSH agent.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>If you do not have an existing ssh keypair to use, run:</p><div class="verbatim-wrap"><pre class="screen">ssh-keygen -t ecdsa</pre></div></li><li class="listitem "><p>The <code class="literal">ssh-agent</code> or a compatible program is sometimes started automatically by graphical desktop
environments. If that is not your situation, run:</p><div class="verbatim-wrap"><pre class="screen">eval "$(ssh-agent)"</pre></div><p>This will start the agent and set environment variables used for agent
communication within the current session. This has to be the same terminal session
that you run the <code class="literal">skuba</code> commands in. A new terminal usually requires a new ssh-agent.
In some desktop environments the ssh-agent will also automatically load the SSH keys.
To add an SSH key manually, use the <code class="literal">ssh-add</code> command:</p><div class="verbatim-wrap"><pre class="screen">ssh-add &lt;PATH_TO_KEY&gt;</pre></div><div id="id-1.6.4.3.5.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you are adding the SSH key manually, specify the full path.
For example: <code class="literal">/home/sles/.ssh/id_rsa</code></p></div></li></ol></div><p>You can load multiple keys into your agent using the <code class="literal">ssh-add &lt;PATH_TO_KEY&gt;</code> command.
Keys should be password protected as a security measure. The
ssh-add command will prompt for your password, then the agent caches the
decrypted key material for a configurable lifetime. The <code class="literal">-t lifetime</code> option to
ssh-add specifies a maximum time to cache the specific key. See <code class="literal">man ssh-add</code> for
more information.</p><div id="id-1.6.4.3.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Specify a key expiration time</h6><p>The ssh key is decrypted when loaded into the key agent.  Though the key itself is not
accesible from the agent, anyone with access to the agent’s control socket file can use
the private key contents to impersonate the key owner.  By default, socket access is
limited to the user who launched the agent.  None the less, it is good security practice
to specify an expiration time for the decrypted key using the <code class="literal">-t</code> option.
For example: <code class="literal">ssh-add -t 1h30m $HOME/.ssh/id.ecdsa</code> would expire the decrypted key in
1.5 hours.
.
Alternatively, <code class="literal">ssh-agent</code> can also be launched with <code class="literal">-t</code> to specify a default timeout.
For example: <code class="literal">eval $( ssh-agent -t 120s )</code> would default to a two minute (120 second)
timeout for keys added.  If timeouts are specified for both programs, the timeout from
<code class="literal">ssh-add</code> is used.
See <code class="literal">man ssh-agent</code> and <code class="literal">man ssh-add</code> for more information.</p></div><div id="id-1.6.4.3.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Usage of multiple identities with <code class="literal">ssh-agent</code></h6><p>Skuba will try all the identities loaded into the <code class="literal">ssh-agent</code> until one of
them grants access to the node, or until the SSH server’s maximum authentication attempts are exhausted.
This could lead to undesired messages in SSH or other security/authentication logs on your local machine.</p></div><div class="sect3" id="_forwarding_the_authentication_agent_connection"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forwarding the Authentication Agent Connection</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_forwarding_the_authentication_agent_connection">#</a></h4></div></div></div><p>It is also possible to <span class="strong"><strong>forward the authentication agent connection</strong></span> from a
host to another, which can be useful if you intend to run skuba on
a "jump host" and don’t want to copy your private key to this node.
This can be achieved using the <code class="literal">ssh -A</code> command. Please refer to the man page
of <code class="literal">ssh</code> to learn about the security implications of using this feature.</p></div></div><div class="sect2" id="registration-code"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registration Code</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#registration-code">#</a></h3></div></div></div><div id="id-1.6.4.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The registration code for SUSE CaaS Platform.4 also contains the activation
permissions for the underlying SUSE Linux Enterprise operating system. You can use your SUSE CaaS Platform
registration code to activate the SUSE Linux Enterprise Server 15 SP2 subscription during installation.</p></div><p>You need a subscription registration code to use SUSE CaaS Platform. You can retrieve your
registration code from SUSE Customer Center.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Login to <a class="link" href="https://scc.suse.com" target="_blank">https://scc.suse.com</a></p></li><li class="listitem "><p>Navigate to <span class="guimenu ">MY ORGANIZATIONS → &lt;YOUR ORG&gt;</span></p></li><li class="listitem "><p>Select the <span class="guimenu ">Subscriptions</span> tab from the menu bar at the top</p></li><li class="listitem "><p>Search for "CaaS Platform"</p></li><li class="listitem "><p>Select the version you wish to deploy (should be the highest available version)</p></li><li class="listitem "><p>Click on the Link in the <span class="guimenu ">Name</span> column</p></li><li class="listitem "><p>The registration code should be displayed as the first line under "Subscription Information"</p></li></ul></div><div id="id-1.6.4.4.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you can not find SUSE CaaS Platform in the list of subscriptions please contact
your local administrator responsible for software subscriptions or SUSE support.</p></div></div><div class="sect2" id="machine-id"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unique Machine IDs</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#machine-id">#</a></h3></div></div></div><p>During deployment of the cluster nodes, each machine will be assigned a unique ID in the <code class="literal">/etc/machine-id</code> file by Terraform or AutoYaST.
If you are using any (semi-)manual methods of deployments that involve cloning of machines and deploying from templates,
you must make sure to delete this file before creating the template.</p><p>If two nodes are deployed with the same <code class="literal">machine-id</code>, they will not be correctly recognized by <code class="literal">skuba</code>.</p><div id="machine-id-regen" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Regenerating Machine ID</h6><p>In case you are not using Terraform or AutoYaST you must regenerate machine IDs manually.</p><p>During the template preparation you will have removed the machine ID from the template image.
This ID is required for proper functionality in the cluster and must be (re-)generated on each machine.</p><p>Log in to each virtual machine created from the template and run:</p><div class="verbatim-wrap"><pre class="screen">rm /etc/machine-id
dbus-uuidgen --ensure
systemd-machine-id-setup
systemctl restart systemd-journald</pre></div><p>This will regenerate the <code class="literal">machine id</code> values for <code class="literal">DBUS</code> (<code class="literal">/var/lib/dbus/machine-id</code>) and <code class="literal">systemd</code> (<code class="literal">/etc/machine-id</code>) and restart the logging service to make use of the new IDs.</p></div></div><div class="sect2" id="_installation_tools"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation Tools</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_installation_tools">#</a></h3></div></div></div><p>For any deployment type you will need <code class="literal">skuba</code> and <code class="literal">Terraform</code>. These packages are
available from the SUSE CaaS Platform package sources. They are provided as an installation
"pattern" that will install dependencies and other required packages in one simple step.</p><p>Access to the packages requires the <code class="literal">SUSE CaaS Platform</code>, <code class="literal">Containers</code> and <code class="literal">Public Cloud</code> extension modules.
Enable the modules during the operating system installation or activate them using SUSE Connect.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo SUSEConnect -r  &lt;CAASP_REGISTRATION_CODE&gt; <span id="CO3-1"></span><span class="callout">1</span>
sudo SUSEConnect -p sle-module-containers/15.2/x86_64 <span id="CO3-2"></span><span class="callout">2</span>
sudo SUSEConnect -p sle-module-public-cloud/15.2/x86_64 <span id="CO3-3"></span><span class="callout">3</span>
sudo SUSEConnect -p caasp/4.5/x86_64 -r &lt;CAASP_REGISTRATION_CODE&gt; <span id="CO3-4"></span><span class="callout">4</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Activate SUSE Linux Enterprise</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Add the free <code class="literal">Containers</code> module</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Add the free <code class="literal">Public Cloud</code> module</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Add the SUSE CaaS Platform extension with your registration code</p></td></tr></table></div><p>Install the required tools:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern SUSE-CaaSP-Management</pre></div><p>This will install the <code class="literal">skuba</code> command line tool and <code class="literal">Terraform</code>; as well
as various default configurations and examples.</p><div id="id-1.6.4.6.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Using a Proxy Server</h6><p>Sometimes you need a proxy server to be able to connect to the SUSE Customer Center.
If you have not already configured a system-wide proxy, you can temporarily do
so for the duration of the current shell session like this:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Expose the environmental variable <code class="literal">http_proxy</code>:</p><div class="verbatim-wrap"><pre class="screen">export http_proxy=http://PROXY_IP_FQDN:PROXY_PORT</pre></div></li><li class="listitem "><p>Replace <code class="literal">&lt;PROXY_IP_FQDN&gt;</code> by the IP address or a fully qualified domain name (FQDN) of the
proxy server and <code class="literal">&lt;PROXY_PORT&gt;</code> by its port.</p></li><li class="listitem "><p>If you use a proxy server with basic authentication, create the file <code class="literal">$HOME/.curlrc</code>
with the following content:</p><div class="verbatim-wrap"><pre class="screen">--proxy-user "&lt;USER&gt;:&lt;PASSWORD&gt;"</pre></div><p>Replace <code class="literal">&lt;USER&gt;</code> and <code class="literal">&lt;PASSWORD&gt;</code> with the credentials of an allowed user for the proxy server, and consider limiting access to the file (<code class="literal">chmod 0600</code>).</p></li></ol></div></div></div><div class="sect2" id="loadbalancer"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#loadbalancer">#</a></h3></div></div></div><div id="id-1.6.4.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Setting up a load balancer is mandatory in any production environment.</p></div><p>SUSE CaaS Platform requires a load balancer to distribute workload between the deployed
master nodes of the cluster. A failure-tolerant SUSE CaaS Platform cluster will always
use more than one control plane node as well as more than one load balancer,
so there isn’t a single point of failure.</p><p>There are many ways to configure a load balancer. This documentation cannot
describe all possible combinations of load balancer configurations and thus
does not aim to do so. Please apply your organization’s load balancing best
practices.</p><p>For SUSE OpenStack Cloud, the Terraform configurations shipped with this version will automatically deploy
a suitable load balancer for the cluster.</p><p>For bare metal, KVM, or VMware, you must configure a load balancer manually and
allow it access to all master nodes created during <a class="xref" href="bootstrap.html" title="Chapter 4. Bootstrapping the Cluster">Chapter 4, <em>Bootstrapping the Cluster</em></a>.</p><p>The load balancer should be configured before the actual deployment. It is needed
during the cluster bootstrap, and also during upgrades. To simplify configuration,
you can reserve the IPs needed for the cluster nodes and pre-configure these in
the load balancer.</p><p>The load balancer needs access to port <code class="literal">6443</code> on the <code class="literal">apiserver</code> (all master nodes)
in the cluster. It also needs access to Gangway port <code class="literal">32001</code> and Dex port <code class="literal">32000</code>
on all master and worker nodes in the cluster for RBAC authentication.</p><p>We recommend performing regular HTTPS health checks on each master node <code class="literal">/healthz</code>
endpoint to verify that the node is responsive. This is particularly important during
upgrades, when a master node restarts the apiserver. During this rather short time
window, all requests have to go to another master node’s apiserver. The master node
that is being upgraded will have to be marked INACTIVE on the load balancer pool
at least during the restart of the apiserver. We provide reasonable defaults
for that on our default openstack load balancer Terraform configuration.</p><p>The following contains examples for possible load balancer configurations based on SUSE Linux Enterprise Server 15 SP2 and <code class="literal">nginx</code> or <code class="literal">HAProxy</code>.</p><div class="sect3" id="loadbalancer-nginx"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nginx TCP Load Balancer with Passive Checks</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#loadbalancer-nginx">#</a></h4></div></div></div><p>For TCP load balancing, we can use the <code class="literal">ngx_stream_module</code> module (available since version 1.9.0). In this mode, <code class="literal">nginx</code> will just forward the TCP packets to the master nodes.</p><p>The default mechanism is <span class="strong"><strong>round-robin</strong></span> so each request will be distributed to a different server.</p><div id="id-1.6.4.7.11.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The open source version of Nginx referred to in this guide only allows the use of
passive health checks. <code class="literal">nginx</code> will mark a node as unresponsive only after
a failed request. The original request is lost and not forwarded to an available
alternative server.</p><p>This load balancer configuration is therefore only suitable for testing and proof-of-concept (POC) environments.</p><p>For production environments, we recommend the use of <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/" target="_blank">SUSE Linux Enterprise High Availability Extension 15</a></p></div><div class="sect4" id="_configuring_the_load_balancer"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Load Balancer</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_configuring_the_load_balancer">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Register SLES and enable the "Server Applications" module:</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.2/x86_64</pre></div></li><li class="listitem "><p>Install Nginx:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in nginx</pre></div></li><li class="listitem "><p>Write the configuration in <code class="literal">/etc/nginx/nginx.conf</code>:</p><div class="verbatim-wrap"><pre class="screen">user  nginx;
worker_processes  auto;

load_module /usr/lib64/nginx/modules/ngx_stream_module.so;

error_log  /var/log/nginx/error.log;
error_log  /var/log/nginx/error.log  notice;
error_log  /var/log/nginx/error.log  info;

events {
    worker_connections  1024;
    use epoll;
}

stream {
    log_format proxy '$remote_addr [$time_local] '
                     '$protocol $status $bytes_sent $bytes_received '
                     '$session_time "$upstream_addr"';

    error_log  /var/log/nginx/k8s-masters-lb-error.log;
    access_log /var/log/nginx/k8s-masters-lb-access.log proxy;

    upstream k8s-masters {
        #hash $remote_addr consistent; <span id="CO4-1"></span><span class="callout">1</span>
        server master00:6443 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-2"></span><span class="callout">2</span>
        server master01:6443 weight=1 max_fails=2 fail_timeout=5s;
        server master02:6443 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 6443;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass k8s-masters;
    }

    upstream dex-backends {
        #hash $remote_addr consistent; <span id="CO4-3"></span><span class="callout">3</span>
        server master00:32000 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-4"></span><span class="callout">4</span>
        server master01:32000 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32000 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32000;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass dex-backends; <span id="CO4-5"></span><span class="callout">5</span>
    }

    upstream gangway-backends {
        #hash $remote_addr consistent; <span id="CO4-6"></span><span class="callout">6</span>
        server master00:32001 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-7"></span><span class="callout">7</span>
        server master01:32001 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32001 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32001;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass gangway-backends; <span id="CO4-8"></span><span class="callout">8</span>
    }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-1"><span class="callout">1</span></a> <a href="#CO4-3"><span class="callout">3</span></a> <a href="#CO4-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p><span class="strong"><strong>Note:</strong></span> To enable session persistence, uncomment the <code class="literal">hash</code> option
so the same client will always be redirected to the same server except if this
server is unavailable.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-2"><span class="callout">2</span></a> <a href="#CO4-4"><span class="callout">4</span></a> <a href="#CO4-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Replace the individual <code class="literal">masterXX</code> with the IP/FQDN of your actual master nodes (one entry each) in the <code class="literal">upstream k8s-masters</code> section.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-5"><span class="callout">5</span></a> <a href="#CO4-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Dex port <code class="literal">32000</code> and Gangway port <code class="literal">32001</code> must be accessible through the load balancer for RBAC authentication.</p></td></tr></table></div></li><li class="listitem "><p>Configure <code class="literal">firewalld</code> to open up port <code class="literal">6443</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload</pre></div></li><li class="listitem "><p>Start and enable Nginx. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now nginx</pre></div></li></ol></div></div><div class="sect4" id="_verifying_the_load_balancer"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the Load Balancer</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_verifying_the_load_balancer">#</a></h5></div></div></div><div id="id-1.6.4.7.11.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The SUSE CaaS Platform cluster must be up and running for this to produce any useful
results. This step can only be performed after <a class="xref" href="bootstrap.html" title="Chapter 4. Bootstrapping the Cluster">Chapter 4, <em>Bootstrapping the Cluster</em></a> is completed
successfully.</p></div><p>To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.</p><p>From your workstation, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">while true; do skuba cluster status; sleep 1; done;</pre></div><p>There should be no interruption in the  <span class="strong"><strong>skuba cluster status</strong></span> running command.</p><p>On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># tail -f /var/log/nginx/k8s-masters-lb-access.log
10.0.0.47 [17/May/2019:13:49:06 +0000] TCP 200 2553 1613 1.136 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:08 +0000] TCP 200 2553 1613 0.981 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:10 +0000] TCP 200 2553 1613 0.891 "10.0.0.7:6443"
10.0.0.47 [17/May/2019:13:49:12 +0000] TCP 200 2553 1613 0.895 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:15 +0000] TCP 200 2553 1613 1.157 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:17 +0000] TCP 200 2553 1613 0.897 "10.0.0.7:6443"</pre></div></div></div><div class="sect3" id="loadbalancer-haproxy"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HAProxy TCP Load Balancer with Active Checks</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#loadbalancer-haproxy">#</a></h4></div></div></div><div id="id-1.6.4.7.12.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Package Support</h6><p><code class="literal">HAProxy</code> is available as a supported package with a <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/" target="_blank">SUSE Linux Enterprise High Availability Extension 15</a> subscription.</p><p>Alternatively, you can install <code class="literal">HAProxy</code> from <a class="link" href="https://packagehub.suse.com/packages/haproxy/" target="_blank">SUSE Package Hub</a> but you will not receive product support for this component.</p></div><p><code class="literal">HAProxy</code> is a very powerful load balancer application which is suitable for production environments.
Unlike the open source version of <code class="literal">nginx</code> mentioned in the example above, <code class="literal">HAProxy</code> supports active health checking which is a vital function for reliable cluster health monitoring.</p><p>The version used at this date is the <code class="literal">1.8.7</code>.</p><div id="id-1.6.4.7.12.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The configuration of an HA cluster is out of the scope of this document.</p></div><p>The default mechanism is <span class="strong"><strong>round-robin</strong></span> so each request will be distributed to a different server.</p><p>The health-checks are executed every two seconds. If a connection fails, the check will be retried two times with a timeout of five seconds for each request.
If no connection succeeds within this interval (2x5s), the node will be marked as <code class="literal">DOWN</code> and no traffic will be sent until the checks succeed again.</p><div class="sect4" id="_configuring_the_load_balancer_2"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Load Balancer</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_configuring_the_load_balancer_2">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Register SLES and enable the "Server Applications" module:</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.2/x86_64</pre></div></li><li class="listitem "><p>Enable the source for the <code class="literal">haproxy</code> package:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>If you are using the SUSE Linux Enterprise High Availability Extension</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect --product sle-ha/15.2/x86_64 -r ADDITIONAL_REGCODE</pre></div></li><li class="listitem "><p>If you want the free (unsupported) package:</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect --product PackageHub/15.2/x86_64</pre></div></li></ul></div></li><li class="listitem "><p>Configure <code class="literal">/dev/log</code> for HAProxy chroot (optional)</p><p>This step is only required when <code class="literal">HAProxy</code> is configured to run in a jail directory (chroot). This is highly recommended since it increases the security of <code class="literal">HAProxy</code>.</p><p>Since <code class="literal">HAProxy</code> is chrooted, it’s necessary to make the log socket available inside the jail directory so <code class="literal">HAProxy</code> can send logs to the socket.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p /var/lib/haproxy/dev/ &amp;&amp; touch /var/lib/haproxy/dev/log</pre></div><p>This systemd service will take care of mounting the socket in the jail directory.</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &gt; /etc/systemd/system/bindmount-dev-log-haproxy-chroot.service &lt;&lt;EOF
[Unit]
Description=Mount /dev/log in HAProxy chroot
After=systemd-journald-dev-log.socket
Before=haproxy.service

[Service]
Type=oneshot
ExecStart=/bin/mount --bind /dev/log /var/lib/haproxy/dev/log

[Install]
WantedBy=multi-user.target
EOF</pre></div><p>Enabling the service will make the changes persistent after a reboot.</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now bindmount-dev-log-haproxy-chroot.service</pre></div></li><li class="listitem "><p>Install HAProxy:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in haproxy</pre></div></li><li class="listitem "><p>Write the configuration in <code class="literal">/etc/haproxy/haproxy.cfg</code>:</p><div id="id-1.6.4.7.12.8.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Replace the individual <code class="literal">&lt;MASTER_XX_IP_ADDRESS&gt;</code> with the IP of your actual master nodes (one entry each) in the <code class="literal">server</code> lines.
Feel free to leave the name argument in the <code class="literal">server</code> lines (<code class="literal">master00</code> and etc.) as is - it only serves as a label that will show up in the haproxy logs.</p></div><div class="verbatim-wrap"><pre class="screen">global
  log /dev/log local0 info <span id="CO5-1"></span><span class="callout">1</span>
  chroot /var/lib/haproxy <span id="CO5-2"></span><span class="callout">2</span>
  user haproxy
  group haproxy
  daemon

defaults
  mode       tcp
  log        global
  option     tcplog
  option     redispatch
  option     tcpka
  retries    2
  http-check     expect status 200 <span id="CO5-3"></span><span class="callout">3</span>
  default-server check check-ssl verify none
  timeout connect 5s
  timeout client 5s
  timeout server 5s
  timeout tunnel 86400s <span id="CO5-4"></span><span class="callout">4</span>

listen stats <span id="CO5-5"></span><span class="callout">5</span>
  bind    *:9000
  mode    http
  stats   hide-version
  stats   uri       /stats

listen apiserver <span id="CO5-6"></span><span class="callout">6</span>
  bind   *:6443
  option httpchk GET /healthz
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:6443
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:6443
  server master02 &lt;MASTER_02_IP_ADDRESS&gt;:6443

listen dex <span id="CO5-7"></span><span class="callout">7</span>
  bind   *:32000
  option httpchk GET /healthz
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:32000
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:32000
  server masetr02 &lt;MASTER_02_IP_ADDRESS&gt;:32000

listen gangway <span id="CO5-8"></span><span class="callout">8</span>
  bind   *:32001
  option httpchk GET /
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:32001
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:32001
  server master02 &lt;MASTER_02_IP_ADDRESS&gt;:32001</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Forward the logs to systemd journald, the log level can be set to <code class="literal">debug</code> to increase verbosity.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Define if it will run in a <code class="literal">chroot</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>This timeout is set to <code class="literal">24h</code> in order to allow long connections when accessing pod logs or port forwarding.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>URL to expose <code class="literal">HAProxy</code> stats on port <code class="literal">9000</code>, it is accessible at <code class="literal">http://loadbalancer:9000/stats</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The performed health checks will expect a <code class="literal">200</code> return code</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Kubernetes apiserver listening on port <code class="literal">6443</code>, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:6443/healthz</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Dex listening on port <code class="literal">32000</code>, it must be accessible through the load balancer for RBAC authentication, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:32000/healthz</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Gangway listening on port <code class="literal">32001</code>, it must be accessible through the load balancer for RBAC authentication, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:32001/</code></p></td></tr></table></div></li><li class="listitem "><p>Configure <code class="literal">firewalld</code> to open up port <code class="literal">6443</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload</pre></div></li><li class="listitem "><p>Start and enable <code class="literal">HAProxy</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now haproxy</pre></div></li></ol></div></div><div class="sect4" id="_verifying_the_load_balancer_2"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the Load Balancer</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_verifying_the_load_balancer_2">#</a></h5></div></div></div><div id="id-1.6.4.7.12.9.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The SUSE CaaS Platform cluster must be up and running for this to produce any useful
results. This step can only be performed after <a class="xref" href="bootstrap.html" title="Chapter 4. Bootstrapping the Cluster">Chapter 4, <em>Bootstrapping the Cluster</em></a> is completed
successfully.</p></div><p>To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.</p><p>From your workstation, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">while true; do skuba cluster status; sleep 1; done;</pre></div><p>There should be no interruption in the <span class="strong"><strong>skuba cluster status</strong></span> running command.</p><p>On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># journalctl -flu haproxy
haproxy[2525]: 10.0.0.47:59664 [30/Sep/2019:13:33:20.578] apiserver apiserver/master00 1/0/578 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59666 [30/Sep/2019:13:33:22.476] apiserver apiserver/master01 1/0/747 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59668 [30/Sep/2019:13:33:24.522] apiserver apiserver/master02 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59670 [30/Sep/2019:13:33:26.386] apiserver apiserver/master00 1/0/567 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59678 [30/Sep/2019:13:33:28.279] apiserver apiserver/master01 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59682 [30/Sep/2019:13:33:30.174] apiserver apiserver/master02 1/0/571 9727 -- 18/18/17/7/0 0/0</pre></div></div></div></div></div><div class="sect1" id="_deployment_on_suse_openstack_cloud"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment on SUSE OpenStack Cloud</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deployment_on_suse_openstack_cloud">#</a></h2></div></div></div><div id="id-1.6.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Preparation Required</h6><p>You must have completed <a class="xref" href="_deployment_instructions.html#deployment-preparations" title="3.1. Deployment Preparations">Section 3.1, “Deployment Preparations”</a> to proceed.</p></div><div class="sect2" id="_install_required_kernel_package"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install required kernel package</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_install_required_kernel_package">#</a></h3></div></div></div><p>The JeOS image used to install on SUSE OpenStack Cloud comes only with the basic kernel pre-installed.
This will be insufficient to run <code class="literal">cilium</code>. Please make sure that you install the
<code class="literal">kernel-default</code> package on all cluster nodes before proceeding.</p></div><div class="sect2" id="_overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_overview">#</a></h3></div></div></div><p>You will use Terraform to deploy the required master and worker cluster nodes (plus a load balancer) to SUSE OpenStack Cloud and then use the
<code class="literal">skuba</code> tool to bootstrap the Kubernetes cluster on top of those.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Download the SUSE OpenStack Cloud RC file.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Log in to SUSE OpenStack Cloud.</p></li><li class="listitem "><p>Click on your username in the upper right hand corner to reveal the drop-down menu.</p></li><li class="listitem "><p>Click on <span class="guimenu ">Download OpenStack RC File v3</span>.</p></li><li class="listitem "><p>Save the file to your workstation.</p></li><li class="listitem "><p>Load the file into your shell environment using the following command,
replacing DOWNLOADED_RC_FILE with the name your file:</p><div class="verbatim-wrap"><pre class="screen">source &lt;DOWNLOADED_RC_FILE&gt;.sh</pre></div></li><li class="listitem "><p>Enter the password for the RC file. This should be same the credentials that you use to log in to SUSE OpenStack Cloud.</p></li></ol></div></li><li class="listitem "><p>Get the SUSE Linux Enterprise Server 15 SP2 image.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Download the pre-built image of SUSE SUSE Linux Enterprise Server 15 SP2 for SUSE OpenStack Cloud from <a class="link" href="https://www.suse.com/download/sles/" target="_blank">https://www.suse.com/download/sles/</a>.</p></li><li class="listitem "><p>Upload the image to your SUSE OpenStack Cloud.</p></li></ol></div></li></ol></div><div id="id-1.6.5.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: The default user is 'sles'</h6><p>The SUSE Linux Enterprise Server 15 SP2 images for SUSE OpenStack Cloud come with predefined user <code class="literal">sles</code>, which you use to log in to the cluster nodes. This user has been configured for password-less 'sudo' and is the one recommended to be used by Terraform and <code class="literal">skuba</code>.</p></div></div><div class="sect2" id="_deploying_the_cluster_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cluster Nodes</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deploying_the_cluster_nodes">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Find the Terraform template files for SUSE OpenStack Cloud in <code class="literal">/usr/share/caasp/terraform/openstack</code> (which was installed as part of the management pattern - <code class="literal">sudo zypper in -t pattern SUSE-CaaSP-Management</code>).
Copy this folder to a location of your choice as the files need adjustment.</p><div class="verbatim-wrap"><pre class="screen">mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/openstack/ ~/caasp/deployment/
cd ~/caasp/deployment/openstack/</pre></div></li><li class="listitem "><p>Once the files are copied, rename the <code class="literal">terraform.tfvars.example</code> file to
<code class="literal">terraform.tfvars</code>:</p><div class="verbatim-wrap"><pre class="screen">mv terraform.tfvars.example terraform.tfvars</pre></div></li><li class="listitem "><p>Edit the <code class="literal">terraform.tfvars</code> file and add/modify the following variables:</p><div class="verbatim-wrap highlight json"><pre class="screen"># Name of the image to use
image_name = "SLES15-SP2-JeOS.x86_64-15.2-OpenStack-Cloud-GM"

# Identifier to make all your resources unique and avoid clashes with other users of this Terraform project
stack_name = "caasp" <span id="CO6-1"></span><span class="callout">1</span>

# Name of the internal network to be created
internal_net = "caasp-net" <span id="CO6-2"></span><span class="callout">2</span>

# Name of the internal subnet to be created
# IMPORTANT: If this variable is not set or empty,
# then it will be generated following a schema like
# internal_subnet = "${var.internal_net}-subnet"
internal_subnet = "caasp-subnet"

# Name of the internal router to be created
# IMPORTANT: If this variable is not set or empty,
# then it will be generated following a schema like
# internal_router = "${var.internal_net}-router"
internal_router = "caasp-router"

# Name of the external network to be used, the one used to allocate floating IPs
external_net = "floating"

# CIDR of the subnet for the internal network
subnet_cidr = "172.28.0.0/24"

# Number of master nodes
masters = 3 <span id="CO6-3"></span><span class="callout">3</span>

# Number of worker nodes
workers = 2 <span id="CO6-4"></span><span class="callout">4</span>

# Size of the master nodes
master_size = "t2.large"

# Size of the worker nodes
worker_size = "t2.large"

# Attach persistent volumes to workers
workers_vol_enabled = 0

# Size of the worker volumes in GB
workers_vol_size = 5

# Name of DNS domain
dnsdomain = "caasp.example.com"

# Set DNS Entry (0 is false, 1 is true)
dnsentry = 0

# Optional: Define the repositories to use
# repositories = {
#   repository1 = "http://repo.example.com/repository1/"
#   repository2 = "http://repo.example.com/repository2/"
# }
repositories = {} <span id="CO6-5"></span><span class="callout">5</span>

# Define required packages to be installed/removed. Do not change those.
packages = [  <span id="CO6-6"></span><span class="callout">6</span>
  "kernel-default",
  "-kernel-default-base",
  "new-package-to-install"
]

# ssh keys to inject into all the nodes
authorized_keys = [ <span id="CO6-7"></span><span class="callout">7</span>
  ""
]

# IMPORTANT: Replace these ntp servers with ones from your infrastructure
ntp_servers = ["0.example.ntp.org", "1.example.ntp.org", "2.example.ntp.org", "3.example.ntp.org"] <span id="CO6-8"></span><span class="callout">8</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p><code class="literal">stack_name</code>: Prefix for all machines of the cluster spawned by terraform.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p><code class="literal">internal_net</code>: the internal network name that will be created/used for the cluster in SUSE OpenStack Cloud.
<span class="strong"><strong>Note</strong></span>: This string will be used to generate the human readable IDs in SUSE OpenStack Cloud.
If you use a generic term, deployment is very likely to fail because the term is already in use by someone else. It’s a good idea to use your username or some other unique identifier.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p><code class="literal">masters</code>: Number of master nodes to be deployed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p><code class="literal">workers</code>: Number of worker nodes to be deployed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p><code class="literal">repositories</code>: A list of additional repositories to be added on each
machines. Leave empty if no additional packages need to be installed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p><code class="literal">packages</code>: Additional packages to be installed on the node.
<span class="strong"><strong>Note</strong></span>: Do not remove any of the pre-filled values in the <code class="literal">packages</code> section. This can render
your cluster unusable. You can add more packages but do not remove any of the
default packages listed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p><code class="literal">authorized_keys</code>: List of ssh public keys that will be injected into the
cluster nodes, allowing you to be able to log in into them via SSH as <code class="literal">sles</code>
user.  Copy and paste the text from the <span class="emphasis"><em>keyname</em></span>.pub file here, <span class="strong"><strong>not</strong></span> the
private key.  At least one of the keys must match a key loaded into your
<code class="literal">ssh-agent</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p><code class="literal">ntp_servers</code>: A list of <code class="literal">ntp</code> servers you would like to use with <code class="literal">chrony</code>.</p></td></tr></table></div><div id="id-1.6.5.5.2.3.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>You can set the timezone before deploying the nodes by modifying the following file:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">~/caasp/deployment/openstack/cloud-init/common.tpl</code></p></li></ul></div></div></li><li class="listitem "><p>(Optional) If you absolutely need to be able to SSH into your cluster nodes using password instead of key-based authentication, this is the best time to set it globally for all of your nodes. If you do this later, you will have to do it manually. To set this, modify the cloud-init configuration and comment out the related SSH configuration:
<code class="literal">~/caasp/deployment/openstack/cloud-init/common.tpl</code></p><div class="verbatim-wrap"><pre class="screen"># Workaround for bsc#1138557 . Disable root and password SSH login
# - sed -i -e '/^PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config
# - sed -i -e '/^#ChallengeResponseAuthentication/s/^.*$/ChallengeResponseAuthentication no/' /etc/ssh/sshd_config
# - sed -i -e '/^#PasswordAuthentication/s/^.*$/PasswordAuthentication no/' /etc/ssh/sshd_config
# - systemctl restart sshd</pre></div></li><li class="listitem "><p>Register your nodes by using the SUSE CaaS Platform Product Key or by registering nodes against local SUSE Repository Mirroring Server in <code class="literal">~/caasp/deployment/openstack/registration.auto.tfvars</code>:</p><p>Substitute <code class="literal">&lt;CAASP_REGISTRATION_CODE&gt;</code> for the code from <a class="xref" href="_deployment_instructions.html#registration-code" title="3.1.2. Registration Code">Section 3.1.2, “Registration Code”</a>.</p><div class="verbatim-wrap"><pre class="screen">## To register CaaSP product please use one of the following method
# - register against SUSE Customer Service, with SUSE CaaSP Product Key
# - register against local SUSE Repository Mirroring Server

# SUSE CaaSP Product Key
caasp_registry_code = "&lt;CAASP_REGISTRATION_CODE&gt;"

# SUSE Repository Mirroring Server Name (FQDN)
#rmt_server_name = "rmt.example.com"</pre></div><p>This is required so all the deployed nodes can automatically register with SUSE Customer Center and retrieve packages.</p></li><li class="listitem "><p>You can also enable Cloud Provider Integration with OpenStack in <code class="literal">~/caasp/deployment/openstack/cpi.auto.tfvars</code>:</p><div class="verbatim-wrap"><pre class="screen"># Enable CPI integration with OpenStack
cpi_enable = true

# Used to specify the name of to your custom CA file located in /etc/pki/trust/anchors/.
# Upload CUSTOM_CA_FILE to this path on nodes before joining them to your cluster.
#ca_file = "/etc/pki/trust/anchors/&lt;CUSTOM_CA_FILE&gt;"</pre></div></li><li class="listitem "><p>Now you can deploy the nodes by running:</p><div class="verbatim-wrap"><pre class="screen">terraform init
terraform plan
terraform apply</pre></div><p>Check the output for the actions to be taken. Type "yes" and confirm with Enter when ready.
Terraform will now provision all the machines and network infrastructure for the cluster.</p><div id="id-1.6.5.5.2.7.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Note down IP/FQDN for nodes</h6><p>The IP addresses of the generated machines will be displayed in the Terraform
output during the cluster node deployment. You need these IP addresses to
deploy SUSE CaaS Platform to the cluster.</p><p>If you need to find an IP address later on, you can run <code class="literal">terraform output</code> within the directory you performed the deployment from the <code class="literal">~/caasp/deployment/openstack</code> directory or perform the following steps:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Log in to SUSE OpenStack Cloud and click on <span class="guimenu ">Network</span> › <span class="guimenu ">Load Balancers</span>. Find the one with the string you entered in the Terraform configuration above, for example "testing-lb".</p></li><li class="listitem "><p>Note down the "Floating IP". If you have configured an FQDN for this IP, use the host name instead.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/deploy-loadbalancer-ip.png" target="_blank"><img src="images/deploy-loadbalancer-ip.png" width="" alt="deploy loadbalancer ip" /></a></div></div></li><li class="listitem "><p>Now click on <span class="guimenu ">Compute</span> › <span class="guimenu ">Instances</span>.</p></li><li class="listitem "><p>Switch the filter dropdown box to <code class="literal">Instance Name</code> and enter the string you specified for <code class="literal">stack_name</code> in the <code class="literal">terraform.tfvars</code> file.</p></li><li class="listitem "><p>Find the floating IPs on each of the nodes of your cluster.</p></li></ol></div></div></li></ol></div></div><div class="sect2" id="_logging_in_to_the_cluster_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging in to the Cluster Nodes</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_logging_in_to_the_cluster_nodes">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Connecting to the cluster nodes can be accomplished only via SSH key-based authentication thanks to the ssh-public key injection done earlier via Terraform. You can use the predefined <code class="literal">sles</code> user to log in.</p><p>If the ssh-agent is running in the background, run:</p><div class="verbatim-wrap"><pre class="screen">ssh sles@&lt;NODE_IP_ADDRESS&gt;</pre></div><p>Without the ssh-agent running, run:</p><div class="verbatim-wrap"><pre class="screen">ssh sles@&lt;NODE_IP_ADDRESS&gt; -i &lt;PATH_TO_YOUR_SSH_PRIVATE_KEY&gt;</pre></div></li><li class="listitem "><p>Once connected, you can execute commands using password-less <code class="literal">sudo</code>. In addition to that, you can also set a password if you prefer to.</p><p>To set the <span class="strong"><strong>root password</strong></span>, run:</p><div class="verbatim-wrap"><pre class="screen">sudo passwd</pre></div><p>To set the <span class="strong"><strong>sles user’s password</strong></span>, run:</p><div class="verbatim-wrap"><pre class="screen">sudo passwd sles</pre></div></li></ol></div><div id="id-1.6.5.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Password authentication has been disabled</h6><p>Under the default settings you always need your SSH key to access the machines.
Even after setting a password for either <code class="literal">root</code> or <code class="literal">sles</code> user, you will be unable
to log in via SSH using their respective passwords. You will most likely receive a
<code class="literal">Permission denied (publickey)</code> error. This mechanism has been deliberately disabled
because of security best practices. However, if this setup does not fit your workflows,
you can change it at your own risk by modifying the SSH configuration:
under <code class="literal">/etc/ssh/sshd_config</code></p><p>To allow password SSH authentication, set:</p><div class="verbatim-wrap"><pre class="screen">+ PasswordAuthentication yes</pre></div><p>To allow login as root via SSH, set:</p><div class="verbatim-wrap"><pre class="screen">+ PermitRootLogin yes</pre></div><p>For the changes to take effect, you need to restart the SSH service by running:</p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart sshd.service</pre></div></div></div><div class="sect2" id="_container_runtime_proxy"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Container Runtime Proxy</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_container_runtime_proxy">#</a></h3></div></div></div><div id="id-1.6.5.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>CRI-O proxy settings must be adjusted on all nodes before joining the cluster!</p><p>Please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o</a></p></div><p>In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">SUSE CaaS Platform Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O</a></p></div></div><div class="sect1" id="_deployment_on_vmware"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment on VMware</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deployment_on_vmware">#</a></h2></div></div></div><div id="id-1.6.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Preparation Required</h6><p>You must have completed <a class="xref" href="_deployment_instructions.html#deployment-preparations" title="3.1. Deployment Preparations">Section 3.1, “Deployment Preparations”</a> to proceed.</p></div><div class="sect2" id="_environment_description"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Environment Description</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_environment_description">#</a></h3></div></div></div><div id="id-1.6.6.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>These instructions are based on <code class="literal">VMware ESXi 6.7</code>.</p></div><div id="id-1.6.6.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Enable Jumbo Frames</h6><p>The SUSE CaaS Platform cluster components use long request headers (in excess of 1300 Bytes) that might not fit into a default configuration.
You must enable "Jumbo Frames" on the switch ports connecting to your ESXi hosts. Any MTU value above 1500 is considered a "Jumbo Frame" but minor increases can still be too small to hold all headers. Please set the maximum transmission unit (MTU) to 9000. This is the maximum value for vSphere and should cover all payloads.</p><p>To enable "Jumbo Frames" refer to: <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.networking.doc/GUID-53F968D9-2F91-41DA-B7B2-48394D997F2A.html" target="_blank">https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.networking.doc/GUID-53F968D9-2F91-41DA-B7B2-48394D997F2A.html</a></p><p>Please check with your hardware vendor that the network adapters used by your cluster support "Jumbo Frames".</p></div><div id="id-1.6.6.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>VMware vSphere doesn’t offer a load-balancer solution. Please expose port <code class="literal">6443</code>
for the Kubernetes api-servers on the master nodes on a local load balancer using
round-robin 1:1 port forwarding.</p></div></div><div class="sect2" id="_choose_a_setup_type"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Choose a Setup Type</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_choose_a_setup_type">#</a></h3></div></div></div><p>You can deploy SUSE CaaS Platform onto existing VMware instances using AutoYaST or
create a VMware template that will also create the instances.
You must choose one method to deploy the entire cluster!</p><p>Please follow the instructions for you chosen method below and ignore the instructions
for the other method.</p></div><div class="sect2" id="_setup_with_autoyast"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup with AutoYaST</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_setup_with_autoyast">#</a></h3></div></div></div><div id="id-1.6.6.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you choose the AutoYaST method, please ignore all the following steps for the
VMware template creation.</p></div><p>For each VM deployment, follow the AutoYaST installation method used for deployment on
bare metal machines as described in <a class="xref" href="_deployment_instructions.html#deployment-bare-metal" title="3.4. Deployment on Bare Metal or KVM">Section 3.4, “Deployment on Bare Metal or KVM”</a>.</p></div><div class="sect2" id="_setup_using_the_vmware_template"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup Using the VMware Template</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_setup_using_the_vmware_template">#</a></h3></div></div></div><div class="sect3" id="choose-disk-format-template"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Choose a Disk Format for the Template</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#choose-disk-format-template">#</a></h4></div></div></div><p>Before creating the template, it is important to select the right format of the root hard disk for the node. This format is then used by default when creating new instances with Terraform.</p><p>For the majority of cases, we recommend using <span class="strong"><strong>Thick Provision Lazy Zeroed</strong></span>. This format is quick to create, provides good performance, and avoids the risk of running out of disk space due to over-provisioning.</p><div id="id-1.6.6.6.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>It is not possible to resize a disk when using <span class="strong"><strong>Thick Provision Eager Zeroed</strong></span>.
For this reason, the Terraform variables <code class="literal">master_disk_size</code> and <code class="literal">worker_disk_size</code> must be set to the exact same size as in the original template.</p></div><p><a class="link" href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.storage.doc%2FGUID-4C0F4D73-82F2-4B81-8AA7-1DD752A8A5AC.html" target="_blank">Official VMware documentation</a> describes these formats as follows:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.6.6.2.6.1"><span class="term ">Thick Provision Lazy Zeroed</span></dt><dd><p>Creates a virtual disk in a default thick format. Space required for the virtual disk is allocated when the disk is created. Data remaining on the physical device is not erased during creation, but is zeroed out on demand later on first write from the virtual machine. Virtual machines do not read stale data from the physical device.</p></dd><dt id="id-1.6.6.6.2.6.2"><span class="term ">Thick Provision Eager Zeroed</span></dt><dd><p>A type of thick virtual disk that supports clustering features such as Fault Tolerance. Space required for the virtual disk is allocated at creation time. In contrast to the thick provision lazy zeroed format, the data remaining on the physical device is zeroed out when the virtual disk is created. It might take longer to create virtual disks in this format than to create other types of disks. Increasing the size of an Eager Zeroed Thick virtual disk causes a significant stun time for the virtual machine.</p></dd><dt id="id-1.6.6.6.2.6.3"><span class="term ">Thin Provision</span></dt><dd><p>Use this format to save storage space. For the thin disk, you provision as much datastore space as the disk would require based on the value that you enter for the virtual disk size. However, the thin disk starts small and at first, uses only as much datastore space as the disk needs for its initial operations. If the thin disk needs more space later, it can grow to its maximum capacity and occupy the entire datastore space provisioned to it.</p></dd></dl></div><div id="id-1.6.6.6.2.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Select the Disk Format Thoughtfully</h6><p>It is not possible to change the format in Terraform later.
Once you have selected one format, you can only create instances with that format and it is not possible to switch.</p></div></div><div class="sect3" id="_vm_preparation_for_creating_a_template"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VM Preparation for Creating a Template</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_vm_preparation_for_creating_a_template">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Upload the ISO image SLE-15-SP2-Full-x86_64-GM-Media1.iso to the desired VMware datastore.</p></li></ol></div><p>Now you can create a new base VM for SUSE CaaS Platform within the designated resource
pool through the vSphere WebUI:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a "New Virtual Machine".</p></li><li class="listitem "><p>Define a name for the virtual machine (VM).</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step1.png" target="_blank"><img src="images/vmware_step1.png" alt="vmware step1" /></a></div></div></li><li class="listitem "><p>Select the folder where the VM will be stored.</p></li><li class="listitem "><p>Select a <code class="literal">Compute Resource</code> that will run the VM.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step2.png" target="_blank"><img src="images/vmware_step2.png" alt="vmware step2" /></a></div></div></li><li class="listitem "><p>Select the storage to be used by the VM.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step3.png" target="_blank"><img src="images/vmware_step3.png" alt="vmware step3" /></a></div></div></li><li class="listitem "><p>Select <code class="literal">ESXi 6.7 and later</code> from compatibility.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step4.png" target="_blank"><img src="images/vmware_step4.png" alt="vmware step4" /></a></div></div></li><li class="listitem "><p>Select <span class="guimenu ">Guest OS Family</span> › <span class="guimenu ">Linux</span> and <span class="guimenu ">Guest OS Version</span> › <span class="guimenu ">SUSE Linux Enterprise 15 (64-bit)</span>.</p><p><span class="strong"><strong>Note</strong></span>: You will manually select the correct installation medium in the next step.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step5.png" target="_blank"><img src="images/vmware_step5.png" alt="vmware step5" /></a></div></div></li><li class="listitem "><p>Now customize the hardware settings.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step6.png" target="_blank"><img src="images/vmware_step6.png" alt="vmware step6" /></a></div></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Select <span class="guimenu ">CPU</span> › <span class="guimenu ">2</span>.</p></li><li class="listitem "><p>Select <span class="guimenu ">Memory</span> › <span class="guimenu ">4096 MB</span>.</p></li><li class="listitem "><p>Select <span class="guimenu ">New Hard disk</span> › <span class="guimenu ">40 GB</span>, <span class="guimenu ">New Hard disk</span> › <span class="guimenu ">Disk Provisioning</span> &gt; See <a class="xref" href="_deployment_instructions.html#choose-disk-format-template" title="3.3.4.1. Choose a Disk Format for the Template">Section 3.3.4.1, “Choose a Disk Format for the Template”</a> to select the appropriate disk format. For <span class="strong"><strong>Thick Provision Eager Zeroed</strong></span>, use this value for Terraform variables <code class="literal">master_disk_size</code> and <code class="literal">worker_disk_size</code></p></li><li class="listitem "><p>Select <span class="guimenu ">New SCSI Controller</span> › <span class="guimenu ">LSI Logic Parallel SCSI controller (default)</span> and change it to "VMware Paravirtualized".</p></li><li class="listitem "><p>Select <span class="guimenu ">New Network</span> › <span class="guimenu ">VM Network</span>, <span class="guimenu ">New Network</span> › <span class="guimenu ">Adapter Type</span> › <span class="guimenu ">VMXNET3</span>.</p><p>("VM Network" sets up a bridged network which provides a public IP address reachable within a company.)</p></li><li class="listitem "><p>Select <span class="guimenu ">New CD/DVD</span> › <span class="guimenu ">Datastore ISO File</span>.</p></li><li class="listitem "><p>Check the box <span class="guimenu ">New CD/DVD</span> › <span class="guimenu ">Connect At Power On</span> to be able boot from ISO/DVD.</p></li><li class="listitem "><p>Then click on "Browse" next to the <code class="literal">CD/DVD Media</code> field to select the downloaded ISO image on the desired datastore.</p></li><li class="listitem "><p>Go to the VM Options tab.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step6b.png" target="_blank"><img src="images/vmware_step6b.png" alt="vmware step6b" /></a></div></div></li><li class="listitem "><p>Select <span class="guimenu ">Boot Options</span>.</p></li><li class="listitem "><p>Select <span class="guimenu ">Firmware</span> › <span class="guimenu ">BIOS</span>.</p></li><li class="listitem "><p>Confirm the process with <span class="guimenu ">Next</span>.</p></li></ol></div></li></ol></div><div class="sect4" id="_suse_linux_enterprise_server_installation"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.3.4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Linux Enterprise Server Installation</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_suse_linux_enterprise_server_installation">#</a></h5></div></div></div><p>Power on the newly created VM and install the system over graphical remote console:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Enter registration code for SUSE Linux Enterprise in YaST.</p></li><li class="listitem "><p>Confirm the update repositories prompt with "Yes".</p></li><li class="listitem "><p>Remove the check mark in the "Hide Development Versions" box.</p></li><li class="listitem "><p>Make sure the following modules are selected on the "Extension and Module Selection" screen:</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_extension.png" target="_blank"><img src="images/vmware_extension.png" alt="vmware extension" /></a></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SUSE CaaS Platform 4.0 x86_64</p><div id="id-1.6.6.6.3.5.3.4.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Due to a naming convention conflict, all versions of SUSE CaaS Platform 4.x up to 4.5 will be released in the <code class="literal">4.0</code> module.
Starting with 4.5 the product will be delivered in the <code class="literal">4.5</code> module.</p></div></li><li class="listitem "><p>Basesystem Module</p></li><li class="listitem "><p>Containers Module (this will automatically be checked when you select SUSE CaaS Platform)</p></li><li class="listitem "><p>Public Cloud Module</p></li></ul></div></li><li class="listitem "><p>Enter the registration code to unlock the SUSE CaaS Platform extension.</p></li><li class="listitem "><p>Select <span class="guimenu ">System Role</span> › <span class="guimenu ">Minimal</span> on the "System Role" screen.</p></li><li class="listitem "><p>Click on "Expert Partitioner" to redesign the default partition layout.</p></li><li class="listitem "><p>Select "Start with current proposal".</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step8.png" target="_blank"><img src="images/vmware_step8.png" alt="vmware step8" /></a></div></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Keep <code class="literal">sda1</code> as BIOS partition.</p></li><li class="listitem "><p>Remove the root <code class="literal">/</code> partition.</p><p>Select the device in "System View" on the left (default: <code class="literal">/dev/sda2</code>) and click "Delete". Confirm with "Yes".</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step9.png" target="_blank"><img src="images/vmware_step9.png" alt="vmware step9" /></a></div></div></li><li class="listitem "><p>Remove the <code class="literal">/home</code> partition.</p></li><li class="listitem "><p>Remove the <code class="literal">swap</code> partition.</p></li></ol></div></li><li class="listitem "><p>Select the <code class="literal">/dev/sda/</code> device in "System View" and then click <span class="guimenu ">Partitions</span> › <span class="guimenu ">Add Partition</span>.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step10.png" target="_blank"><img src="images/vmware_step10.png" alt="vmware step10" /></a></div></div></li><li class="listitem "><p>Accept the default maximum size (remaining size of the hard disk defined earlier without the boot partition).</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step11.png" target="_blank"><img src="images/vmware_step11.png" alt="vmware step11" /></a></div></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Confirm with "Next".</p></li><li class="listitem "><p>Select <span class="guimenu ">Role</span> › <span class="guimenu ">Operating System</span></p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step12.png" target="_blank"><img src="images/vmware_step12.png" alt="vmware step12" /></a></div></div></li><li class="listitem "><p>Confirm with "Next".</p></li><li class="listitem "><p>Accept the default settings.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step13.png" target="_blank"><img src="images/vmware_step13.png" alt="vmware step13" /></a></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Filesystem: BtrFS</p></li><li class="listitem "><p>Enable Snapshots</p></li><li class="listitem "><p>Mount Device</p></li><li class="listitem "><p>Mount Point <code class="literal">/</code></p></li></ul></div></li></ol></div></li><li class="listitem "><p>You should be left with two partitions. Now click "Accept".</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step7.png" target="_blank"><img src="images/vmware_step7.png" alt="vmware step7" /></a></div></div></li><li class="listitem "><p>Confirm the partitioning changes.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step14.png" target="_blank"><img src="images/vmware_step14.png" alt="vmware step14" /></a></div></div></li><li class="listitem "><p>Click "Next".</p></li><li class="listitem "><p>Configure your timezone and click "Next".</p></li><li class="listitem "><p>Create a user with the username <code class="literal">sles</code> and specify a password.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Check the box <span class="guimenu ">Local User</span> › <span class="guimenu ">Use this password for system administrator</span>.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step15.png" target="_blank"><img src="images/vmware_step15.png" alt="vmware step15" /></a></div></div></li></ol></div></li><li class="listitem "><p>Click "Next".</p></li><li class="listitem "><p>On the "Installation Settings" screen:</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>In the "Security" section:</p><div class="orderedlist "><ol class="orderedlist" type="i"><li class="listitem "><p>Disable the Firewall (click on <code class="literal">(disable)</code>).</p></li><li class="listitem "><p>Enable the SSH service (click on <code class="literal">(enable)</code>).</p></li></ol></div></li><li class="listitem "><p>Scroll to the <code class="literal">kdump</code> section of the software description and click on the title.</p></li></ol></div></li><li class="listitem "><p>In the "Kdump Start-Up" screen, select <span class="guimenu ">Enable/Disable Kdump</span> › <span class="guimenu ">Disable Kdump</span>.</p><div id="id-1.6.6.6.3.5.3.18.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p><code class="literal">Kdump</code> needs to be disabled because it defines a certain memory limit. If you later
wish to deploy the template on a machine with different memory allocation (e.g. template created for 4GB, new machine has 2GB),
the results of Kdump will be useless.</p><p>You can always configure <code class="literal">Kdump</code> on the machine after deploying from the template.</p><p>Refer to: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-tuning/#cha-tuning-kdump-basic" target="_blank">SUSE Linux Enterprise Server 15 SP2 System Analysis and Tuning Guide: Basic Kdump Configuration</a></p></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Confirm with "OK".</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step16.png" target="_blank"><img src="images/vmware_step16.png" alt="vmware step16" /></a></div></div></li></ol></div></li><li class="listitem "><p>Click "Install". Confirm the installation by clicking "Install" in the pop-up dialog.</p></li><li class="listitem "><p>Finish the installation and confirm system reboot with "OK".</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/vmware_step17.png" target="_blank"><img src="images/vmware_step17.png" alt="vmware step17" /></a></div></div></li></ol></div></div></div><div class="sect3" id="_preparation_of_the_vm_as_a_template"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation of the VM as a Template</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_preparation_of_the_vm_as_a_template">#</a></h4></div></div></div><p>In order to run SUSE CaaS Platform on the created VMs, you must configure and install some additional packages
like <code class="literal">sudo</code>, <code class="literal">cloud-init</code> and <code class="literal">open-vm-tools</code>.</p><div id="id-1.6.6.6.4.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Activate extensions during SUSE Linux Enterprise installation with YaST</h6><p>Steps 1-4 may be skipped, if they were already performed in YaST during the SUSE Linux Enterprise installation.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Register the SUSE Linux Enterprise Server 15 SP2 system. Substitute <code class="literal">&lt;CAASP_REGISTRATION_CODE&gt;</code> for the code from <a class="xref" href="_deployment_instructions.html#registration-code" title="3.1.2. Registration Code">Section 3.1.2, “Registration Code”</a>.</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect -r CAASP_REGISTRATION_CODE</pre></div></li><li class="listitem "><p>Register the <code class="literal">Containers</code> module (free of charge):</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect -p sle-module-containers/15.2/x86_64</pre></div></li><li class="listitem "><p>Register the <code class="literal">Public Cloud</code> module for basic <code class="literal">cloud-init</code> package (free of charge):</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect -p sle-module-public-cloud/15.2/x86_64</pre></div></li><li class="listitem "><p>Register the SUSE CaaS Platform module. Substitute <code class="literal">&lt;CAASP_REGISTRATION_CODE&gt;</code> for the code from <a class="xref" href="_deployment_instructions.html#registration-code" title="3.1.2. Registration Code">Section 3.1.2, “Registration Code”</a>.</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect -p caasp/4.5/x86_64 -r CAASP_REGISTRATION_CODE</pre></div></li><li class="listitem "><p>Install required packages. As root, run:</p><div class="verbatim-wrap"><pre class="screen">zypper in sudo cloud-init cloud-init-vmware-guestinfo open-vm-tools</pre></div></li><li class="listitem "><p>Enable the installed <code class="literal">cloud-init</code> services. As root, run:</p><div class="verbatim-wrap"><pre class="screen">systemctl enable cloud-init cloud-init-local cloud-config cloud-final</pre></div><p>}</p></li><li class="listitem "><p>Deregister from <code class="literal">scc</code>:</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect -d; SUSEConnect --cleanup</pre></div></li><li class="listitem "><p>Do a cleanup of the SLE image for converting into a VMware template. As root, run:</p><div class="verbatim-wrap"><pre class="screen">rm /etc/machine-id /var/lib/zypp/AnonymousUniqueId \
/var/lib/systemd/random-seed /var/lib/dbus/machine-id \
/var/lib/wicked/*</pre></div></li><li class="listitem "><p>Clean up btrfs snapshots and create one with initial state:</p><div class="verbatim-wrap"><pre class="screen">snapper list
snapper delete &lt;list_of_nums_of_unneeded_snapshots&gt;
snapper create -d "Initial snapshot for caasp template" -t single</pre></div></li><li class="listitem "><p>Power down the VM. As root, run:</p><div class="verbatim-wrap"><pre class="screen">shutdown -h now</pre></div></li></ol></div></div><div class="sect3" id="_creating_the_vmware_template"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating the VMware Template</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_creating_the_vmware_template">#</a></h4></div></div></div><p>Now you can convert the VM into a template in VMware (or repeat this action block for each VM).</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>In the vSphere WebUI, right-click on the VM and select <span class="guimenu ">Template</span> › <span class="guimenu ">Convert to Template</span>.
Name it reasonably so you can later identify the template. The template will be created.</p></li></ol></div></div><div class="sect3" id="_deploying_vms_from_the_template"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.3.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying VMs from the Template</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deploying_vms_from_the_template">#</a></h4></div></div></div><div class="sect4" id="_using_terraform"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.3.4.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Terraform</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_using_terraform">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Find the Terraform template files for VMware in <code class="literal">/usr/share/caasp/terraform/vmware</code> which was installed as part of the management
pattern (<code class="literal">sudo zypper in patterns-caasp-Management</code>).
Copy this folder to a location of your choice; as the files need to be adjusted.</p><div class="verbatim-wrap"><pre class="screen">mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/vmware/ ~/caasp/deployment/
cd ~/caasp/deployment/vmware/</pre></div></li><li class="listitem "><p>Once the files are copied, rename the <code class="literal">terraform.tfvars.example</code> file to
<code class="literal">terraform.tfvars</code>:</p><div class="verbatim-wrap"><pre class="screen">mv terraform.tfvars.example terraform.tfvars</pre></div></li><li class="listitem "><p>Edit the <code class="literal">terraform.tfvars</code> file and add/modify the following variables:</p></li></ol></div><div class="verbatim-wrap highlight json"><pre class="screen"># datastore to use in vSphere
vsphere_datastore = "STORAGE-0" <span id="CO7-1"></span><span class="callout">1</span>

# datastore_ cluster to use in vSphere
vsphere_datastore_cluster = "STORAGE-CLUSTER-0" <span id="CO7-2"></span><span class="callout">2</span>

# datacenter to use in vSphere
vsphere_datacenter = "DATACENTER" <span id="CO7-3"></span><span class="callout">3</span>

# network to use in vSphere
vsphere_network = "VM Network" <span id="CO7-4"></span><span class="callout">4</span>

# resource pool the machines will be running in
vsphere_resource_pool = "esxi1/Resources" <span id="CO7-5"></span><span class="callout">5</span>

# template name the machines will be copied from
template_name = "sles15-sp2-caasp" <span id="CO7-6"></span><span class="callout">6</span>

# IMPORTANT: Replace by "efi" string in case your template was created by using EFI firmware
firmware = "bios"

# prefix that all of the booted machines will use
# IMPORTANT: please enter unique identifier below as value of
# stack_name variable to not interfere with other deployments
stack_name = "caasp" <span id="CO7-7"></span><span class="callout">7</span>

# Number of master nodes
masters = 1 <span id="CO7-8"></span><span class="callout">8</span>

# Optional: Size of the root disk in GB on master node
master_disk_size = 50 <span id="CO7-9"></span><span class="callout">9</span>

# Number of worker nodes
workers = 2 <span id="CO7-10"></span><span class="callout">10</span>

# Optional: Size of the root disk in GB on worker node
worker_disk_size = 40 <span id="CO7-11"></span><span class="callout">11</span>

# Username for the cluster nodes. Must exist on base OS.
username = "sles" <span id="CO7-12"></span><span class="callout">12</span>

# Optional: Define the repositories to use
# repositories = {
#   repository1 = "http://repo.example.com/repository1/"
#   repository2 = "http://repo.example.com/repository2/"
# }
repositories = {} <span id="CO7-13"></span><span class="callout">13</span>

# Minimum required packages. Do not remove them.
# Feel free to add more packages
packages = [ <span id="CO7-14"></span><span class="callout">14</span>
]

# ssh keys to inject into all the nodes
authorized_keys = [ <span id="CO7-15"></span><span class="callout">15</span>
  "ssh-rsa &lt;example_key&gt; example@example.com"
]

# IMPORTANT: Replace these ntp servers with ones from your infrastructure
ntp_servers = ["0.example.ntp.org", "1.example.ntp.org", "2.example.ntp.org", "3.example.ntp.org"] <span id="CO7-16"></span><span class="callout">16</span>

# Controls whether or not the guest network waiter waits for a routable address.
# Default is True and should not be changed unless you hit the upstream bug: https://github.com/hashicorp/terraform-provider-vsphere/issues/1127
wait_for_guest_net_routable = true <span id="CO7-17"></span><span class="callout">17</span></pre></div><div id="id-1.6.6.6.6.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Only one of <code class="literal">vsphere_datastore</code> or <code class="literal">vsphere_datastore_cluster</code> can be set at the same time. Proceed to comment or delete the unused one from your <code class="literal">terraform.tfvars</code></p></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p><code class="literal">vsphere_datastore</code>: The datastore to use. This option is mutually exclusive with <code class="literal">vsphere_datastore_cluster</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p><code class="literal">vsphere_datastore_cluster</code>: The datastore cluster to use. This option is mutually exclusive with <code class="literal">vsphere_datastore</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p><code class="literal">vsphere_datacenter</code>: The datacenter to use.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p><code class="literal">vsphere_network</code>: The network to use.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p><code class="literal">vsphere_resource_pool</code>: The root resource pool or an user-created child resource pool. Refer to <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.resmgmt.doc/GUID-60077B40-66FF-4625-934A-641703ED7601.html" target="_blank">https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.resmgmt.doc/GUID-60077B40-66FF-4625-934A-641703ED7601.html</a> for detailed information.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p><code class="literal">template_name</code>: The name of the template created according to instructions.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p><code class="literal">stack_name</code>: Prefix for all machines of the cluster spawned by terraform.
<span class="strong"><strong>Note</strong></span>: This string will be used to generate the human readable IDs in SUSE OpenStack Cloud.
If you use a generic term, deployment very likely to fail because the term is already in use by someone else. It’s a good idea to use your username or some other unique identifier.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p><code class="literal">masters</code>: Number of master nodes to be deployed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p><code class="literal">master_disk_size</code>: Size of the root disk in GB.
<span class="strong"><strong>Note</strong></span>: The value must be at least the same size as the source template. It is only possible to increase the size of a disk.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-10"><span class="callout">10</span></a> </p></td><td valign="top" align="left"><p><code class="literal">workers</code>: Number of worker nodes to be deployed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-11"><span class="callout">11</span></a> </p></td><td valign="top" align="left"><p><code class="literal">worker_disk_size</code>: Size of the root disk in GB.
<span class="strong"><strong>Note</strong></span>: The value must be at least the same size as the source template. It is only possible to increase the size of a disk.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-12"><span class="callout">12</span></a> </p></td><td valign="top" align="left"><p><code class="literal">username</code>: Login username for the nodes.
<span class="strong"><strong>Note</strong></span>: Leave this as the default <code class="literal">sles</code>. The username must exist on the used base operating system. It will not be created.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-13"><span class="callout">13</span></a> </p></td><td valign="top" align="left"><p><code class="literal">repositories</code>: A list of additional repositories to be added on each
machines. Leave empty if no additional packages need to be installed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-14"><span class="callout">14</span></a> </p></td><td valign="top" align="left"><p><code class="literal">packages</code>: Additional packages to be installed on the node.
<span class="strong"><strong>Note</strong></span>: Do not remove any of the pre-filled values in the <code class="literal">packages</code> section. This can render
your cluster unusable. You can add more packages but do not remove any of the
default packages listed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-15"><span class="callout">15</span></a> </p></td><td valign="top" align="left"><p><code class="literal">authorized_keys</code>: List of ssh-public-keys that will be able to log in to the
deployed machines.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-16"><span class="callout">16</span></a> </p></td><td valign="top" align="left"><p><code class="literal">ntp_servers</code>: A list of <code class="literal">ntp</code> servers you would like to use with <code class="literal">chrony</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-17"><span class="callout">17</span></a> </p></td><td valign="top" align="left"><p><code class="literal">wait_for_guest_net_routable</code>: true or false to disable the routable check. Default is true.
<span class="strong"><strong>Note</strong></span>: This should only be changed if terraform times out while creating VMs as mentioned on the upstream bug: <a class="link" href="https://github.com/hashicorp/terraform-provider-vsphere/issues/1127" target="_blank">https://github.com/hashicorp/terraform-provider-vsphere/issues/1127</a></p></td></tr></table></div><div class="orderedlist "><ol class="orderedlist" start="4" type="1"><li class="listitem "><p>Enter the registration code for your nodes in <code class="literal">~/caasp/deployment/vmware/registration.auto.tfvars</code>:</p><p>Substitute <code class="literal">&lt;CAASP_REGISTRATION_CODE&gt;</code> for the code from <a class="xref" href="_deployment_instructions.html#registration-code" title="3.1.2. Registration Code">Section 3.1.2, “Registration Code”</a>.</p><div class="verbatim-wrap highlight json"><pre class="screen"># SUSE CaaSP Product Product Key
caasp_registry_code = "CAASP_REGISTRATION_CODE"</pre></div><p>This is required so all the deployed nodes can automatically register with SUSE Customer Center and retrieve packages.</p></li><li class="listitem "><p>You can also enable Cloud Provider Integration with vSphere.</p><div class="verbatim-wrap"><pre class="screen"># Enable CPI integration with vSphere
cpi_enable = true</pre></div></li><li class="listitem "><p>When cpi is enabled, hostnames set from the DHCP server will automatically be disabled. This sets <sub>vSphere</sub> virtual machine’s hostname with a naming convention (<code class="literal">"&lt;stack_name&gt;-master-&lt;index&gt;"</code> or <code class="literal">"&lt;stack_name&gt;-worker-&lt;index&gt;"</code>). This can be used as node name when using <code class="literal">skuba</code> command to bootstrapping or joining nodes.</p><div id="id-1.6.6.6.6.2.6.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>It is mandatory that each virtual machine’s hostname must match its cluster node name.</p></div><div class="verbatim-wrap"><pre class="screen"># Set node's hostname from DHCP server
hostname_from_dhcp = false</pre></div><p>Once the files are adjusted, <code class="literal">terraform</code> needs to know about the <code class="literal">vSphere</code> server
and the login details for it; these can be exported as environment variables or
entered every time <code class="literal">terraform</code> is invoked.</p></li><li class="listitem "><p>Additionally, the <code class="literal">ssh-key</code> that is specified in the <code class="literal">tfvars</code> file must be added
to the key agent so the machine running <code class="literal">skuba</code> can <code class="literal">ssh</code> into the machines:</p><div class="verbatim-wrap"><pre class="screen">export VSPHERE_SERVER="&lt;server_address"
export VSPHERE_USER="&lt;username&gt;"
export VSPHERE_PASSWORD="&lt;password&gt;"
export VSPHERE_ALLOW_UNVERIFIED_SSL=true # In case you are using custom certificate for accessing vsphere API

ssh-add &lt;path_to_private_ssh_key_from_tfvars&gt;</pre></div><div id="id-1.6.6.6.6.2.6.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Specify a key expiration time</h6><p>The ssh key is decrypted when loaded into the key agent.  Though the key itself is not
accesible, anyone with access to the agent’s control socket file can use the private key
contents to impersonate the key owner.  By default, socket access is limited to the user
who launched the agent.  None the less, it is still good security practice to specify an
expiration time for the decrypted key using the <code class="literal">-t</code> option.</p><p>For example: <code class="literal">ssh-add -t 1h30m $HOME/.ssh/id.ecdsa</code> would expire the decrypted key in 1.5
hours.
See <code class="literal">man ssh-agent</code> and <code class="literal">man ssh-add</code> for more information.</p></div></li><li class="listitem "><p>Run Terraform to create the required machines for use with <code class="literal">skuba</code>:</p><div class="verbatim-wrap"><pre class="screen">terraform init
terraform plan
terraform apply</pre></div></li></ol></div></div><div class="sect4" id="_setup_by_hand"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.3.4.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup by Hand</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_setup_by_hand">#</a></h5></div></div></div><div id="id-1.6.6.6.6.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Full instructions for the manual setup and configuration are currently not in scope of this document.</p></div><p>Deploy the template to your created VMs. After that, boot into the node and configure
the OS as needed.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Power on the newly created VMs</p></li><li class="listitem "><p>Generate new machine IDs on each node</p></li><li class="listitem "><p>You need to know the FQDN/IP for each of the created VMs during the bootstrap process</p></li><li class="listitem "><p>Continue with bootstrapping/joining of nodes</p></li></ol></div><div id="id-1.6.6.6.6.3.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>To manually generate the unique <code class="literal">machine-id</code> please refer to: <a class="xref" href="_deployment_instructions.html#machine-id-regen" title="Important: Regenerating Machine ID">Important: Regenerating Machine ID</a>.</p></div></div></div></div><div class="sect2" id="_container_runtime_proxy_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Container Runtime Proxy</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_container_runtime_proxy_2">#</a></h3></div></div></div><div id="id-1.6.6.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>CRI-O proxy settings must be adjusted on all nodes before joining the cluster!</p></div><p>In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">SUSE CaaS Platform Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O</a></p></div></div><div class="sect1" id="deployment-bare-metal"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment on Bare Metal or KVM</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#deployment-bare-metal">#</a></h2></div></div></div><div id="id-1.6.7.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Preparation Required</h6><p>You must have completed <a class="xref" href="_deployment_instructions.html#deployment-preparations" title="3.1. Deployment Preparations">Section 3.1, “Deployment Preparations”</a> to proceed.</p></div><div id="id-1.6.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If deploying on KVM virtual machines, you may use a tool such as <code class="literal">virt-manager</code>
to configure the virtual machines and begin the SUSE Linux Enterprise Server 15 SP2 installation.</p></div><div class="sect2" id="_environment_description_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Environment Description</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_environment_description_2">#</a></h3></div></div></div><div id="id-1.6.7.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You must have a load balancer configured as described in <a class="xref" href="_deployment_instructions.html#loadbalancer" title="3.1.5. Load Balancer">Section 3.1.5, “Load Balancer”</a>.</p></div><div id="id-1.6.7.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The AutoYaST file found in <code class="literal">skuba</code> is a template. It has the base requirements.
This AutoYaST file should act as a guide and should be updated with your company’s standards.</p></div><div id="id-1.6.7.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>To account for hardware/platform-specific setup criteria (legacy BIOS vs. (U)EFI, drive partitioning, networking, etc.),
you must adjust the AutoYaST file to your needs according to the requirements.</p><p>Refer to the official AutoYaST documentation for more information: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#book-autoyast" target="_blank">AutoYaST Guide</a>.</p></div><div class="sect3" id="_machine_configuration_prerequisites"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Machine Configuration Prerequisites</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_machine_configuration_prerequisites">#</a></h4></div></div></div><p>Deployment with AutoYaST will require a minimum <span class="strong"><strong>disk size of 40 GB</strong></span>.
That space is reserved for container images without any workloads (10 GB),
for the root partition (30 GB) and the EFI system partition (200 MB).</p></div></div><div class="sect2" id="_autoyast_preparation"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">AutoYaST Preparation</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_autoyast_preparation">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>On the management machine, get an example AutoYaST file from <code class="literal">/usr/share/caasp/autoyast/bare-metal/autoyast.xml</code>,
(which was installed earlier on as part of the management pattern (<code class="literal">sudo zypper in -t pattern SUSE-CaaSP-Management</code>).</p></li><li class="listitem "><p>Copy the file to a suitable location to modify it. Name the file <code class="literal">autoyast.xml</code>.</p></li><li class="listitem "><p>Modify the following places in the AutoYaST file (and any additional places as required by your specific configuration/environment):</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p><code class="literal">&lt;ntp-client&gt;</code></p><p>Change the pre-filled value to your organization’s NTP server. Provide multiple servers if possible by adding new <code class="literal">&lt;ntp_server&gt;</code> subentries.</p></li><li class="listitem "><p><code class="literal">&lt;timezone&gt;</code></p><p>Adjust the timezone your nodes will be set to. Refer to: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#id-1.7.5.13.6" target="_blank">SUSE Linux Enterprise Server AutoYaST Guide: Country Settings</a></p></li><li class="listitem "><p><code class="literal">&lt;username&gt;sles&lt;/username&gt;</code></p><p>Insert your authorized key in the placeholder field.</p></li><li class="listitem "><p><code class="literal">&lt;users&gt;</code></p><p>You can add additional users by creating new blocks in the configuration containing their data.</p><div id="id-1.6.7.5.2.3.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If the users are configured to not have a password like in the example, ensure the system’s <code class="literal">sudoers</code> file is updated.
Without updating the sudoers file the user will only be able to perform basic operations that will prohibit many administrative tasks.</p><p>The default AutoYaST file provides examples for a disabled <code class="literal">root</code> user and a <code class="literal">sles</code> user with authorized key SSH access.</p><p>The password for root can be enabled by using the <code class="literal">passwd</code> command.</p></div></li><li class="listitem "><p><code class="literal">&lt;suse_register&gt;</code></p><p>Insert the email address and SUSE CaaS Platform registration code in the placeholder fields. This activates SUSE Linux Enterprise Server 15 SP2.</p></li><li class="listitem "><p><code class="literal">&lt;addon&gt;</code></p><p>Insert the SUSE CaaS Platform registration code in the placeholder field. This enables the SUSE CaaS Platform extension module.
Update the AutoYaST file with your registration keys and your company’s best practices and hardware configurations.</p><div id="id-1.6.7.5.2.3.2.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Your SUSE CaaS Platform registration key can be used to both activate SUSE Linux Enterprise Server 15 SP2 and enable the extension.</p></div></li></ol></div><p>Refer to the official AutoYaST documentation for more information: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#book-autoyast" target="_blank">AutoYaST Guide</a>.</p></li><li class="listitem "><p>Host the AutoYaST files on a Web server reachable inside the network you are installing the cluster in.</p></li></ol></div><div class="sect3" id="_deploying_with_local_repository_mirroring_tool_rmt_server"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying with local Repository Mirroring Tool (RMT) server</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deploying_with_local_repository_mirroring_tool_rmt_server">#</a></h4></div></div></div><p>In order to use a local Repository Mirroring Tool (RMT) server for deployment of packages, you need to specify
the server configuration in your AutoYaST file. To do so add the following section:</p><div class="verbatim-wrap highlight xml"><pre class="screen">&lt;suse_register&gt;
&lt;do_registration config:type="boolean"&gt;true&lt;/do_registration&gt;
&lt;install_updates config:type="boolean"&gt;true&lt;/install_updates&gt;

&lt;reg_server&gt;https://rmt.example.org&lt;/reg_server&gt; <span id="CO8-1"></span><span class="callout">1</span>
&lt;reg_server_cert&gt;https://rmt.example.org/rmt.crt&lt;/reg_server_cert&gt; <span id="CO8-2"></span><span class="callout">2</span>
&lt;reg_server_cert_fingerprint_type&gt;SHA1&lt;/reg_server_cert_fingerprint_type&gt;
&lt;reg_server_cert_fingerprint&gt;0C:A4:A1:06:AD:E2:A2:AA:D0:08:28:95:05:91:4C:07:AD:13:78:FE&lt;/reg_server_cert_fingerprint&gt; <span id="CO8-3"></span><span class="callout">3</span>
&lt;slp_discovery config:type="boolean"&gt;false&lt;/slp_discovery&gt;
&lt;addons config:type="list"&gt;
  &lt;addon&gt;
    &lt;name&gt;sle-module-containers&lt;/name&gt;
    &lt;version&gt;15.2&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
  &lt;addon&gt;
    &lt;name&gt;sle-module-public-cloud&lt;/name&gt;
    &lt;version&gt;15.2&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
  &lt;addon&gt;
    &lt;name&gt;caasp&lt;/name&gt;
    &lt;version&gt;4.5&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
&lt;/addons&gt;
&lt;/suse_register&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Provide FQDN of the Repository Mirroring Tool (RMT) server</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Provide the location on the server where the certificate can be found</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Provide the certificate fingerprint for the Repository Mirroring Tool (RMT) server</p></td></tr></table></div></div></div><div class="sect2" id="_provisioning_the_cluster_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning the Cluster Nodes</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_provisioning_the_cluster_nodes">#</a></h3></div></div></div><p>Once the AutoYaST file is available in the network that the machines will be configured in, you can start deploying machines.</p><p>The default production scenario consists of 6 nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 load balancer</p></li><li class="listitem "><p>3 masters</p></li><li class="listitem "><p>2 workers</p></li></ul></div><p>Depending on the type of load balancer you wish to use, you need to deploy at least 5 machines to serve as cluster nodes and provide a load balancer from the environment.</p><p>The load balancer must point at the machines that are assigned to be used as <code class="literal">master</code> nodes in the future cluster.</p><div id="id-1.6.7.6.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you do not wish to use infrastructure load balancers, please deploy additional machines and refer to <a class="xref" href="_deployment_instructions.html#loadbalancer" title="3.1.5. Load Balancer">Section 3.1.5, “Load Balancer”</a>.</p></div><p>Install SUSE Linux Enterprise Server 15 SP2 from your preferred medium and follow the steps for <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#invoking-autoinst" target="_blank">Invoking the Auto-Installation Process</a></p><p>Provide <code class="literal">autoyast=https://[webserver/path/to/autoyast.xml]</code> during the SUSE Linux Enterprise Server 15 SP2 installation.</p><div class="sect3" id="_suse_linux_enterprise_server_installation_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Linux Enterprise Server Installation</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_suse_linux_enterprise_server_installation_2">#</a></h4></div></div></div><div id="id-1.6.7.6.10.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Use AutoYaST and make sure to use a staged frozen patchlevel via RMT/SUSE Manager to ensure a 100% reproducible setup.
<a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-rmt/#cha-rmt-client" target="_blank">RMT Guide</a></p></div><p>Once the machines have been installed using the AutoYaST file, you are now ready proceed with <a class="xref" href="bootstrap.html" title="Chapter 4. Bootstrapping the Cluster">Chapter 4, <em>Bootstrapping the Cluster</em></a>.</p></div></div><div class="sect2" id="_container_runtime_proxy_3"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Container Runtime Proxy</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_container_runtime_proxy_3">#</a></h3></div></div></div><div id="id-1.6.7.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>CRI-O proxy settings must be adjusted on all nodes before joining the cluster!</p><p>Please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o</a></p><p>In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">SUSE CaaS Platform Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O</a></p></div></div></div><div class="sect1" id="_deployment_on_existing_sles_installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment on Existing SLES Installation</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deployment_on_existing_sles_installation">#</a></h2></div></div></div><p>If you already have a running SUSE Linux Enterprise Server 15 SP2 installation, you can add SUSE CaaS Platform
to this installation using SUSE Connect. You also need to enable the "Containers" and "Public Cloud"
modules because it contains some dependencies required by SUSE CaaS Platform.</p><div class="sect2" id="_requirements"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_requirements">#</a></h3></div></div></div><div id="id-1.6.8.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Preparation Required</h6><p>You must have completed <a class="xref" href="_deployment_instructions.html#deployment-preparations" title="3.1. Deployment Preparations">Section 3.1, “Deployment Preparations”</a> to proceed.</p></div><div class="sect3" id="_dedicated_cluster_nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Dedicated Cluster Nodes</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_dedicated_cluster_nodes">#</a></h4></div></div></div><div id="id-1.6.8.3.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Adding a machine with an existing use case (e.g. web server) as a cluster node is not supported!</p></div><p>SUSE CaaS Platform requires dedicated machines as cluster nodes.</p><p>The instructions in this document are meant to add SUSE CaaS Platform to an existing SUSE Linux Enterprise
installation that has no other active use case.</p><p>For example: You have installed a machine with SUSE Linux Enterprise but it has not yet been commissioned to run
a specific application and you decide now to make it a SUSE CaaS Platform cluster node.</p></div><div class="sect3" id="_disabling_swap"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabling Swap</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_disabling_swap">#</a></h4></div></div></div><p>When using a pre-existing SUSE Linux Enterprise installation, <code class="literal">swap</code> will be enabled. You must disable <code class="literal">swap</code>
for all cluster nodes before performing the cluster bootstrap.</p><p>On all nodes that are meant to join the cluster; run:</p><div class="verbatim-wrap"><pre class="screen">sudo swapoff -a</pre></div><p>Then modify <code class="literal">/etc/fstab</code> on each node to remove the <code class="literal">swap</code> entries.</p><div id="id-1.6.8.3.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>It is recommended to reboot the machine to finalize these changes and prevent accidental reactivation of
<code class="literal">swap</code> during an automated reboot of the machine later on.</p></div></div></div><div class="sect2" id="_adding_suse_caas_platform_repositories"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding SUSE CaaS Platform repositories</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_adding_suse_caas_platform_repositories">#</a></h3></div></div></div><p>Retrieve your SUSE CaaS Platform registration code and run the following.
Substitute <code class="literal">&lt;CAASP_REGISTRATION_CODE&gt;</code> for the code from <a class="xref" href="_deployment_instructions.html#registration-code" title="3.1.2. Registration Code">Section 3.1.2, “Registration Code”</a>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -p sle-module-containers/15.2/x86_64
SUSEConnect -p sle-module-public-cloud/15.2/x86_64

SUSEConnect -p caasp/4.5/x86_64 -r &lt;CAASP_REGISTRATION_CODE&gt;</pre></div><p>Repeat all preparation steps for any cluster nodes you wish to join.
You can then proceed with <a class="xref" href="bootstrap.html" title="Chapter 4. Bootstrapping the Cluster">Chapter 4, <em>Bootstrapping the Cluster</em></a>.</p></div></div><div class="sect1" id="_deployment_on_amazon_web_services_aws"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment on Amazon Web Services (AWS)</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deployment_on_amazon_web_services_aws">#</a></h2></div></div></div><p>Deployment on Amazon Web Services (AWS) is currently a tech preview.</p><div id="id-1.6.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Preparation Required</h6><p>You must have completed <a class="xref" href="_deployment_instructions.html#deployment-preparations" title="3.1. Deployment Preparations">Section 3.1, “Deployment Preparations”</a> to proceed.</p></div><p>You will use Terraform to deploy the whole infrastructure described in
<a class="xref" href="_deployment_instructions.html#architecture-aws" title="3.6.1. AWS Deployment">Section 3.6.1, “AWS Deployment”</a>. Then you will use the <code class="literal">skuba</code> tool to bootstrap the
Kubernetes cluster on top of it.</p><div class="sect2" id="architecture-aws"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">AWS Deployment</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#architecture-aws">#</a></h3></div></div></div><p>The AWS deployment created by our Terraform template files leads to the
creation of the infrastructure described in the next paragraphs.</p><div class="sect3" id="_network"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_network">#</a></h4></div></div></div><p>All of the infrastructure is created inside of a user specified AWS region.
The resources are currently all located inside of the same availability
zone.</p><p>The Terraform template files create a dedicated Amazon Virtual Private Cloud (<a class="link" href="https://aws.amazon.com/vpc/" target="_blank">VPC</a>)
with <span class="strong"><strong>two subnets</strong></span>: "public" and "private". Instances inside of the <span class="strong"><strong>public subnet</strong></span> have
<a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html" target="_blank">Elasic IP addresses</a>
associated, hence they are reachable from the internet. Instances inside of the <span class="strong"><strong>private subnet</strong></span> are not reachable from the internet.
However they can still reach external resources; for example they can still
perform operations like downloading updates and pulling container images from
external container registries. Communication between the public and the private subnet is allowed.
All the control plane instances are currently located inside of the public
subnet. Worker instances are inside of the private subnet.</p><p>Both control plane and worker nodes have tailored
<a class="link" href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html" target="_blank">Security Groups</a>
assigned to them. These are based on the networking requirements described
in <a class="xref" href="deployment-system-requirements.html#sysreq-networking" title="1.4. Networking">Section 1.4, “Networking”</a>.</p></div><div class="sect3" id="_load_balancer"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.6.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_load_balancer">#</a></h4></div></div></div><p>The Terraform template files take care of creating a
<a class="link" href="https://aws.amazon.com/elasticloadbalancing/" target="_blank">Classic Load Balancer</a>
which exposes the Kubernetes API service deployed on the control plane
nodes.</p><p>The load balancer exposes the following ports:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">6443</code>: Kubernetes API server</p></li><li class="listitem "><p><code class="literal">32000</code>: Dex (OIDC Connect)</p></li><li class="listitem "><p><code class="literal">32001</code>: Gangway (RBAC Authenticate)</p></li></ul></div></div><div class="sect3" id="architecture-aws-vpc-peering"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.6.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Join Already Existing VPCs</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#architecture-aws-vpc-peering">#</a></h4></div></div></div><p>The Terraform template files allow the user to have the
SUSE CaaS Platform VPC join one or more existing VPCs.</p><p>This is achieved by the creation of
<a class="link" href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html" target="_blank">VPC peering links</a>
and dedicated
<a class="link" href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html" target="_blank">Route tables</a>.</p><p>This feature allows SUSE CaaS Platform to access and be accessed by resources defined
inside of other VPCs. For example, this capability can be used to register all
the SUSE CaaS Platform instances against a SUSE Manager server running inside of a
private VPC.</p><p>Current limitations:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The VPCs must belong to the same AWS region.</p></li><li class="listitem "><p>The VPCs must be owned by the same user who is creating the SUSE CaaS Platform
infrastructure via Terraform.</p></li></ul></div></div><div class="sect3" id="aws-iam-profile"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.6.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">IAM Profiles</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#aws-iam-profile">#</a></h4></div></div></div><p>The
<a class="link" href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws" target="_blank">AWS Cloud Provider</a>
integration for Kubernetes requires special
<a class="link" href="https://aws.amazon.com/iam/" target="_blank">IAM</a> profiles to be associated with the control
plane and worker instances. Terraform can create these profiles or can leverage existing ones.
It all depends on the rights of the user invoking Terraform.</p><p>The Terraform <a class="link" href="https://www.terraform.io/docs/providers/aws/index.html" target="_blank">AWS provider</a>
requires your credentials. These can be obtained by following these steps:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Log in to the AWS console.</p></li><li class="listitem "><p>Click on your username in the upper right hand corner to reveal the drop-down menu.</p></li><li class="listitem "><p>Click on <span class="guimenu ">My Security Credentials</span>.</p></li><li class="listitem "><p>Click <span class="guimenu ">Create Access Key</span> on the "Security Credentials" tab.</p></li><li class="listitem "><p>Note down the newly created <span class="emphasis"><em>Access</em></span> and <span class="emphasis"><em>Secret</em></span> keys.</p></li></ul></div></div></div><div class="sect2" id="_deploying_the_infrastructure"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Infrastructure</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_deploying_the_infrastructure">#</a></h3></div></div></div><p>On the management machine, find the Terraform template files for AWS in
<code class="literal">/usr/share/caasp/terraform/aws</code>. These files have been installed as part of
the management pattern (<code class="literal">sudo zypper in -t pattern SUSE-CaaSP-Management</code>).</p><p>Copy this folder to a location of your choice as the files need adjustment.</p><div class="verbatim-wrap"><pre class="screen">mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/aws/ ~/caasp/deployment/
cd ~/caasp/deployment/aws/</pre></div><p>Once the files are copied, rename the <code class="literal">terraform.tfvars.example</code> file to
<code class="literal">terraform.tfvars</code>:</p><div class="verbatim-wrap"><pre class="screen">cp terraform.tfvars.example terraform.tfvars</pre></div><p>Edit the <code class="literal">terraform.tfvars</code> file and add/modify the following variables:</p><div class="verbatim-wrap highlight json"><pre class="screen"># prefix that all of the booted machines will use
# IMPORTANT, please enter unique identifier below as value of
# stack_name variable to not interfere with other deployments
stack_name = "caasp" <span id="CO9-1"></span><span class="callout">1</span>

# Number of master nodes
masters = 1 <span id="CO9-2"></span><span class="callout">2</span>

# Number of worker nodes
workers = 2 <span id="CO9-3"></span><span class="callout">3</span>

# ssh keys to inject into all the nodes
# EXAMPLE:
# authorized_keys = [
#   "ssh-rsa &lt;key-content&gt;"
# ]
authorized_keys = [ <span id="CO9-4"></span><span class="callout">4</span>
  "ssh-rsa &lt;example_key&gt; example@example.com"
]

# To register CaaSP product please use ONLY ONE of the following method
#
# SUSE CaaSP Product Product Key:
#caasp_registry_code = ""  <span id="CO9-5"></span><span class="callout">5</span>
#
# SUSE Repository Mirroring Server Name (FQDN):
#rmt_server_name = "rmt.example.com"  <span id="CO9-6"></span><span class="callout">6</span>

# List of VPC IDs to join via VPC peer link
#peer_vpc_ids = ["vpc-id1", "vpc-id2"] <span id="CO9-7"></span><span class="callout">7</span>

# Name of the IAM profile to associate to control plane nodes
# Leave empty to have terraform create one.
# This is required to have AWS CPI support working properly.
#
# Note well: you must  have the right set of permissions.
# iam_profile_master = "caasp-k8s-master-vm-profile" <span id="CO9-8"></span><span class="callout">8</span>

# Name of the IAM profile to associate to worker nodes.
# Leave empty to have terraform create one.
# This is required to have AWS CPI support working properly.
#
# Note well: you must  have the right set of permissions.
#iam_profile_worker = "caasp-k8s-worker-vm-profile" <span id="CO9-9"></span><span class="callout">9</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p><code class="literal">stack_name</code>: Prefix for all machines of the cluster spawned by terraform.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p><code class="literal">masters</code>: Number of master nodes to be deployed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p><code class="literal">workers</code>: Number of worker nodes to be deployed.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p><code class="literal">authorized_keys</code>: List of ssh-public-keys that will be able to log into
the deployed machines.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p><code class="literal">caasp_registry_code</code>: SUSE CaaS Platform Product Key for registering
the product against SUSE Customer Center.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p><code class="literal">caasp_registry_code</code>: register against a local SUSE Repository
Mirroring Server.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p><code class="literal">peer_vpc_ids</code>: List of already existing VPCs to join via dedicated VPC
peering links.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p><code class="literal">iam_profile_master</code>: Name of the IAM profile to associate with the control
plane instance. Leave empty to have Terraform create it.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p><code class="literal">iam_profile_worker</code>: Name of the IAM profile to associate with the worker
instances. Leave empty to have Terraform create it.</p></td></tr></table></div><div id="id-1.6.9.6.10" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>You can set timezone and other parameters before deploying the nodes
by modifying the cloud-init template:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">~/caasp/deployment/aws/cloud-init/cloud-init.yaml.tpl</code></p></li></ul></div></div><p>You can enter the registration code for your nodes in
<code class="literal">~/caasp/deployment/aws/registration.auto.tfvars</code> instead of the
<code class="literal">terraform.tfvars</code> file.</p><p>Substitute <code class="literal">CAASP_REGISTRATION_CODE</code> for the code from <a class="xref" href="_deployment_instructions.html#registration-code" title="3.1.2. Registration Code">Section 3.1.2, “Registration Code”</a>.</p><div class="verbatim-wrap highlight json"><pre class="screen"># SUSE CaaSP Product Key
caasp_registry_code = "&lt;CAASP_REGISTRATION_CODE&gt;"</pre></div><p>This is required so all the deployed nodes can automatically register
with SUSE Customer Center and retrieve packages.</p><p>The last step before deploying is to provide the credentials
as environment variables that Terraform will automatically retrieve:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>AWS_ACCESS_KEY_ID: This is the AWS access key.</p></li><li class="listitem "><p>AWS_SECRET_ACCESS_KEY This is the AWS secret key.</p></li><li class="listitem "><p>AWS_DEFAULT_REGION This is the AWS region. A list of region names
can be found in <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions)" target="_blank">the official AWS documentation</a>.</p></li></ul></div><p>To do so, source the following variables,
for security reasons turn off bash history:</p><div class="verbatim-wrap highlight bash"><pre class="screen">set +o history
export AWS_ACCESS_KEY_ID="AKIAIOSFODNN7EXAMPLE"
export AWS_SECRET_ACCESS_KEY="wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
export AWS_DEFAULT_REGION="eu-central-1"
set -o history</pre></div><p>It can also be stored in a file, for example <code class="literal">aws-credentials</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">AWS_ACCESS_KEY_ID="AKIAIOSFODNN7EXAMPLE"
AWS_SECRET_ACCESS_KEY="wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
AWS_DEFAULT_REGION="eu-central-1"</pre></div><p>And sourced:</p><div class="verbatim-wrap highlight bash"><pre class="screen">set -a; source aws-credentials; set +a</pre></div><p>Now you can deploy the nodes by running:</p><div class="verbatim-wrap"><pre class="screen">terraform init
terraform plan
terraform apply</pre></div><p>Check the output for the actions to be taken. Type "yes" and confirm with
<span class="keycap">Enter</span> when ready.
Terraform will now provision all the cluster infrastructure.</p><div id="id-1.6.9.6.26" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Public IPs for Nodes</h6><p><code class="literal">skuba</code> currently cannot access nodes through a bastion host, so all
the nodes in the cluster must be directly reachable from the machine where
<code class="literal">skuba</code> is being run.
<code class="literal">skuba</code> could be run from one of the master nodes or from a pre-existing bastion
host located inside of a joined VPC as described in
<a class="xref" href="_deployment_instructions.html#architecture-aws-vpc-peering" title="3.6.1.3. Join Already Existing VPCs">Section 3.6.1.3, “Join Already Existing VPCs”</a>.</p></div><div id="id-1.6.9.6.27" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Note Down IP/FQDN For the Nodes</h6><p>The IP addresses and FQDN of the generated machines will be displayed in the
Terraform output during the cluster node deployment. You need these information
later to deploy SUSE CaaS Platform.</p><p>These information can be obtained at any time by executing the
<code class="literal">terraform output</code> command within the directory from which you executed
Terraform.</p></div></div><div class="sect2" id="_logging_into_the_cluster_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging into the Cluster Nodes</span> <a title="Permalink" class="permalink" href="_deployment_instructions.html#_logging_into_the_cluster_nodes">#</a></h3></div></div></div><p>Connecting to the cluster nodes can be accomplished only via SSH key-based
authentication thanks to the ssh-public key injection done earlier via
<code class="literal">cloud-init</code>. You can use the predefined <code class="literal">ec2-user</code> user to log in.</p><p>If the ssh-agent is running in the background, run:</p><div class="verbatim-wrap"><pre class="screen">ssh ec2-user@&lt;node-ip-address&gt;</pre></div><p>Without the ssh-agent running, run:</p><div class="verbatim-wrap"><pre class="screen">ssh ec2-user@&lt;node-ip-address&gt; -i &lt;path-to-your-ssh-private-key&gt;</pre></div><p>Once connected, you can execute commands using password-less <code class="literal">sudo</code>.</p></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="bootstrap.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 4 </span>Bootstrapping the Cluster</span></a><a class="nav-link" href="_deployment_scenarios.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 2 </span>Deployment Scenarios</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>