<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Requirements | Deployment Guide | SUSE CaaS Platform 4.5.1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.1" /><meta name="book-title" content="Deployment Guide" /><meta name="chapter-title" content="Chapter 1. Requirements" /><meta name="description" content="Currently we support the following platforms to deploy on:" /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="DeploymentGuide" /><link rel="home" href="index.html" title="Deployment Guide" /><link rel="up" href="index.html" title="Deployment Guide" /><link rel="prev" href="_about_this_guide.html" title="About This Guide" /><link rel="next" href="_deployment_scenarios.html" title="Chapter 2. Deployment Scenarios" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Deployment Guide"><span class="book-icon">Deployment Guide</span></a><span> › </span><a class="crumb" href="deployment-system-requirements.html">Requirements</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Deployment Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pr01.html"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="_about_this_guide.html"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="deployment-system-requirements.html"><span class="number">1 </span><span class="name">Requirements</span></a></li><li class="inactive"><a href="_deployment_scenarios.html"><span class="number">2 </span><span class="name">Deployment Scenarios</span></a></li><li class="inactive"><a href="_deployment_instructions.html"><span class="number">3 </span><span class="name">Deployment Instructions</span></a></li><li class="inactive"><a href="bootstrap.html"><span class="number">4 </span><span class="name">Bootstrapping the Cluster</span></a></li><li class="inactive"><a href="_cilium_network_policy_config_examples.html"><span class="number">5 </span><span class="name">Cilium Network Policy Config Examples</span></a></li><li class="inactive"><a href="_glossary.html"><span class="number">6 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="_contributors.html"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="_gnu_licenses.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="About This Guide" href="_about_this_guide.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 2. Deployment Scenarios" href="_deployment_scenarios.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Deployment Guide"><span class="book-icon">Deployment Guide</span></a><span> › </span><a class="crumb" href="deployment-system-requirements.html">Requirements</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="About This Guide" href="_about_this_guide.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 2. Deployment Scenarios" href="_deployment_scenarios.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="deployment-system-requirements"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.1</span></div><div><h1 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="deployment-system-requirements.html#_platform"><span class="number">1.1 </span><span class="name">Platform</span></a></span></dt><dt><span class="section"><a href="deployment-system-requirements.html#_nodes"><span class="number">1.2 </span><span class="name">Nodes</span></a></span></dt><dt><span class="section"><a href="deployment-system-requirements.html#_hardware"><span class="number">1.3 </span><span class="name">Hardware</span></a></span></dt><dt><span class="section"><a href="deployment-system-requirements.html#sysreq-networking"><span class="number">1.4 </span><span class="name">Networking</span></a></span></dt></dl></div></div><div class="sect1" id="_platform"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Platform</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_platform">#</a></h2></div></div></div><p>Currently we support the following platforms to deploy on:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SUSE OpenStack Cloud 8</p></li><li class="listitem "><p>VMware ESXi 6.7</p></li><li class="listitem "><p>KVM</p></li><li class="listitem "><p>Bare Metal x86_64</p></li><li class="listitem "><p>Amazon Web Services (technological preview)</p></li></ul></div><p>SUSE CaaS Platform itself is based on <span class="strong"><strong>SLE 15 SP2</strong></span>.</p><p>The steps for obtaining the correct installation image for each platform type
are detailed in the respective platform deployment instructions.</p></div><div class="sect1" id="_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nodes</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_nodes">#</a></h2></div></div></div><p>SUSE CaaS Platform consists of a number of (virtual) machines that run as a cluster.</p><p>You will need at least two machines:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 master node</p></li><li class="listitem "><p>1 worker node</p></li></ul></div><p>SUSE CaaS Platform 4.5.1 supports deployments with a single or multiple master nodes.
Production environments must be deployed with multiple master nodes for resilience.</p><p>All communication to the cluster is done through a load balancer talking to the respective nodes.
For that reason any failure tolerant environment must provide at least two load balancers for incoming communication.</p><p>The minimal viable failure tolerant production environment configuration consists of:</p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Cluster nodes: </span><a title="Permalink" class="permalink" href="deployment-system-requirements.html#id-1.4.3.8">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>3 master nodes</p></li><li class="listitem "><p>2 worker nodes</p></li></ul></div><div id="id-1.4.3.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Dedicated Cluster Nodes</h6><p>All cluster nodes must be dedicated (virtual) machines reserved for the purpose of running SUSE CaaS Platform.</p></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Additional systems: </span><a title="Permalink" class="permalink" href="deployment-system-requirements.html#id-1.4.3.10">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Fault tolerant load balancing solution</p><p>(for example SUSE Linux Enterprise High Availability Extension with <code class="literal">pacemaker</code> and <code class="literal">haproxy</code>)</p></li><li class="listitem "><p>1 management workstation</p></li></ul></div></div><div class="sect1" id="_hardware"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_hardware">#</a></h2></div></div></div><div class="sect2" id="_management_workstation"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Management Workstation</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_management_workstation">#</a></h3></div></div></div><p>In order to deploy and control a SUSE CaaS Platform cluster you will need at least one
machine capable of running <code class="literal">skuba</code>. This typically is a regular desktop workstation or laptop
running SLE 15 SP2 or later.</p><p>The <code class="literal">skuba</code> CLI package is available from the SUSE CaaS Platform module.
You will need a valid SUSE Linux Enterprise and SUSE CaaS Platform subscription to install this tool on the workstation.</p><div id="id-1.4.4.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Time Synchronization</h6><p>It is vital that the management workstation runs an NTP client and that time synchronization is configured to the same NTP servers, which you will use later to synchronize the cluster nodes.</p></div></div><div class="sect2" id="_storage_sizing"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Sizing</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_storage_sizing">#</a></h3></div></div></div><p>The storage sizes in the following lists are absolute minimum configurations.</p><p>Sizing of the storage for worker nodes depends largely on the expected amount of container images, their size and change rate.
The basic operating system for all nodes might also include snapshots (when using <code class="literal">btrfs</code>) that can quickly fill up existing space.</p><p>We recommend provisioning a separate storage partition for container images on each (worker) node that can be adjusted in size when needed.
Storage for <code class="literal">/var/lib/containers</code> on the worker nodes should be approximately 50GB in addition to the base OS storage.</p><div class="sect3" id="_master_nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Master Nodes</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_master_nodes">#</a></h4></div></div></div><p>Up to 5 worker nodes <span class="strong"><strong>(minimum)</strong></span>:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 2</p></li><li class="listitem "><p>RAM: 4 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Up to 10 worker nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 2</p></li><li class="listitem "><p>RAM: 8 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Up to 100 worker nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 4</p></li><li class="listitem "><p>RAM: 16 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Up to 250 worker nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 8</p></li><li class="listitem "><p>RAM: 16 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><div id="id-1.4.4.3.5.10" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Using a minimum of 2 (v)CPUs is a hard requirement, deploying a cluster with less processing units is not possible.</p></div></div><div class="sect3" id="_worker_nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Worker nodes</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_worker_nodes">#</a></h4></div></div></div><div id="id-1.4.4.3.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The worker nodes must have sufficient memory, CPU and disk space for the Pods/containers/applications that are planned to be hosted on these workers.</p></div><p>A worker node requires the following resources:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>CPU cores: 1.250</p></li><li class="listitem "><p>RAM: 1.2 GB</p></li></ul></div><p>Based on these values, the <span class="strong"><strong>minimal</strong></span> configuration of a worker node is:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: Depending on workloads, minimum 20-30 GB to hold the base OS and required packages. Mount additional storage volumes as needed.</p></li><li class="listitem "><p>(v)CPU: 2</p></li><li class="listitem "><p>RAM: 2 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Calculate the size of the required (v)CPU by adding up the base requirements, the estimated additional essential cluster components (logging agent, monitoring agent, configuration management, etc.) and the estimated CPU workloads:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1.250 (base requirements) + 0.250 (estimated additional cluster components) + estimated workload CPU requirements</p></li></ul></div><p>Calculate the size of the RAM using a similar formula:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1.2 GB (base requirements) + 500 MB (estimated additional cluster components) + estimated workload RAM requirements</p></li></ul></div><div id="id-1.4.4.3.6.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>These values are provided as a guide to work in most cases. They may vary based on the type of the running workloads.</p></div></div></div><div class="sect2" id="_storage_performance"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Performance</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_storage_performance">#</a></h3></div></div></div><p>For master nodes you must ensure storage performance of at least 50 to 500 sequential IOPS with disk bandwidth depending on your cluster size. It is highly recommended to use SSD.</p><div class="verbatim-wrap"><pre class="screen">"Typically 50 sequential IOPS (for example, a 7200 RPM disk) is required.
For heavily loaded clusters, 500 sequential IOPS (for example, a typical local SSD
or a high performance virtualized block device) is recommended."</pre></div><div class="verbatim-wrap"><pre class="screen">"Typically 10MB/s will recover 100MB data within 15 seconds.
For large clusters, 100MB/s or higher is suggested for recovering 1GB data
within 15 seconds."</pre></div><p><a class="link" href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks" target="_blank">https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks</a></p><p>This is extremely important to ensure a proper functioning of the critical component <code class="literal">etcd</code>.</p><p>It is possible to preliminary validate these requirements by using <code class="literal">fio</code>. This tool allows us to simulate <code class="literal">etcd</code> I/O (input/output) and to find out from the output statistics whether or not the storage is suitable.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install the tool:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in -y fio</pre></div></li><li class="listitem "><p>Run the testing:</p><div class="verbatim-wrap highlight bash"><pre class="screen">fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-etcd-dir --size=22m --bs=2300 --name=test-etcd-io</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Replace <code class="literal">test-etcd-dir</code> with a directory located on the same disk as the incoming etcd data under <code class="literal">/var/lib/etcd</code></p></li></ul></div></li></ol></div><p>From the outputs, the interesting part is <code class="literal">fsync/fdatasync/sync_file_range</code> where the values are expressed in microseconds (usec). A disk is considered sufficient when the value of the <code class="literal">99.00th</code> percentile is below 10000usec (10ms).</p><p>Be careful though, this benchmark is for etcd only and does not take into consideration external disk usage. This means that a value slightly under 10ms should be taken with precaution as other workloads will have an impact on the disks.</p><div id="id-1.4.4.4.11" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>If the storage is very slow, the values can be expressed directly in milliseconds.</p></div><p>Let’s see two different examples:</p><div class="verbatim-wrap"><pre class="screen">[...]
  fsync/fdatasync/sync_file_range:
    sync (usec): min=251, max=1894, avg=377.78, stdev=69.89
    sync percentiles (usec):
     |  1.00th=[  273],  5.00th=[  285], 10.00th=[  297], 20.00th=[  330],
     | 30.00th=[  343], 40.00th=[  355], 50.00th=[  367], 60.00th=[  379],
     | 70.00th=[  400], 80.00th=[  424], 90.00th=[  465], 95.00th=[  506],
     | 99.00th=[  594], 99.50th=[  635], 99.90th=[  725], 99.95th=[  742], <span id="CO1-1"></span><span class="callout">1</span>
     | 99.99th=[ 1188]
[...]</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Here we get a value of 594usec (0.5ms) so the storage meets the requirements.</p></td></tr></table></div><div class="verbatim-wrap"><pre class="screen">[...]
  fsync/fdatasync/sync_file_range:
    sync (msec): min=10, max=124, avg=17.62, stdev= 3.38
    sync percentiles (usec):
     |  1.00th=[11731],  5.00th=[11994], 10.00th=[12911], 20.00th=[16712],
     | 30.00th=[17695], 40.00th=[17695], 50.00th=[17695], 60.00th=[17957],
     | 70.00th=[17957], 80.00th=[17957], 90.00th=[19530], 95.00th=[22676],
     | 99.00th=[28705], 99.50th=[30016], 99.90th=[41681], 99.95th=[59507], <span id="CO2-1"></span><span class="callout">1</span>
     | 99.99th=[89654]
[...]</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO2-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Here we get a value of 28705usec (28ms) so the storage clearly does not meet the requirements.</p></td></tr></table></div></div></div><div class="sect1" id="sysreq-networking"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#sysreq-networking">#</a></h2></div></div></div><p>The management workstation needs at least the following networking permissions:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SSH access to all machines in the cluster</p></li><li class="listitem "><p>Access to the <code class="literal">apiserver</code> (the load balancer should expose it, port <code class="literal">6443</code>), that will in turn talk to any master in the cluster</p></li><li class="listitem "><p>Access to Dex on the configured <code class="literal">NodePort</code> (the load balancer should expose it, port <code class="literal">32000</code>) so when the OIDC token has expired, <code class="literal">kubectl</code> can request a new token using the refresh token</p></li></ul></div><div id="id-1.4.5.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>It is good security practice not to expose the kubernetes API server on the public internet.
Use network firewalls that only allow access from trusted subnets.</p></div><div class="sect2" id="_sub_network_sizing"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sub-Network Sizing</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_sub_network_sizing">#</a></h3></div></div></div><div id="id-1.4.5.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The service subnet and pod subnet must not overlap.</p><p>Please plan generously for workload and the expected size of the networks before bootstrapping.</p></div><p>The default pod subnet is <code class="literal">10.244.0.0/16</code>. It allows for 65536 IP addresses overall.
Assignment of CIDR’s is by default <code class="literal">/24</code> (254 usable IP addresses per node).</p><p>The default node allocation of <code class="literal">/24</code> means a hard cluster node limit of 256 since this is the number of <code class="literal">/24</code> ranges that fit in a <code class="literal">/16</code> range.</p><p>Depending on the size of the nodes that you are planning to use (in terms of resources), or on the number of nodes you are planning to have,
the CIDR can be adjusted to be bigger on a per node basis but the cluster would accommodate less nodes overall.</p><p>If you are planning to use more or less pods per node or have a higher number of nodes, you can adjust these settings to match your requirements.
Please make sure that the networks are suitably sized to adjust to future changes in the cluster.</p><p>You can also adjust the service subnet size, this subnet must not overlap with the pod CIDR, and it should be big enough to accommodate all services.</p><p>For more advanced network requirements please refer to: <a class="link" href="https://docs.cilium.io/en/v1.6/concepts/ipam/#address-management" target="_blank">https://docs.cilium.io/en/v1.6/concepts/ipam/#address-management</a></p></div><div class="sect2" id="_ports"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ports</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_ports">#</a></h3></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="middle">Node</th><th align="left" valign="middle">Port</th><th align="left" valign="middle">Protocol</th><th align="left" valign="middle">Accessibility</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td rowspan="8" align="left" valign="middle"><p>All nodes</p></td><td align="left" valign="middle"><p>22</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="bottom"><p>SSH (required in public clouds)</p></td></tr><tr><td align="left" valign="middle"><p>4240</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Cilium health check</p></td></tr><tr><td align="left" valign="middle"><p>8472</p></td><td align="left" valign="middle"><p>UDP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Cilium VXLAN</p></td></tr><tr><td align="left" valign="middle"><p>10250</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Kubelet (API server → kubelet communication)</p></td></tr><tr><td align="left" valign="middle"><p>10256</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>kube-proxy health check</p></td></tr><tr><td align="left" valign="middle"><p>30000 - 32767</p></td><td align="left" valign="middle"><p>TCP + UDP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Range of ports used by Kubernetes when allocating services of type <code class="literal">NodePort</code></p></td></tr><tr><td align="left" valign="middle"><p>32000</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>External</p></td><td align="left" valign="middle"><p>Dex (OIDC Connect)</p></td></tr><tr><td align="left" valign="middle"><p>32001</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>External</p></td><td align="left" valign="middle"><p>Gangway (RBAC Authenticate)</p></td></tr><tr><td rowspan="3" align="left" valign="middle"><p>Masters</p></td><td align="left" valign="middle"><p>2379</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="bottom"><p>etcd (client communication)</p></td></tr><tr><td align="left" valign="middle"><p>2380</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>etcd (server-to-server traffic)</p></td></tr><tr><td align="left" valign="middle"><p>6443</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal / External</p></td><td align="left" valign="middle"><p>Kubernetes API server</p></td></tr></tbody></table></div></div><div class="sect2" id="_ip_addresses"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">IP Addresses</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_ip_addresses">#</a></h3></div></div></div><div id="id-1.4.5.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Using IPv6 addresses is currently not supported.</p></div><p>All nodes must be assigned static IPv4 addresses, which must not be changed manually afterwards.</p><div id="id-1.4.5.7.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Plan carefully for required IP ranges and future scenarios as
it is not possible to reconfigure the IP ranges once the deployment is complete.</p></div></div><div class="sect2" id="_ip_forwarding"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">IP Forwarding</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_ip_forwarding">#</a></h3></div></div></div><p>The <a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/cluster-administration/networking/" target="_blank">Kubernetes networking model</a> requires that your nodes have IP forwarding enabled in the kernel.
<code class="literal">skuba</code> checks this value when installing your cluster and installs a rule in <code class="literal">/etc/sysctl.d/90-skuba-net-ipv4-ip-forward.conf</code> to make it persistent.</p><p>Other software can potentially install rules with higher priority overriding this value and causing machines to not behave as expected after rebooting.</p><p>You can manually check if this is enabled using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># sysctl net.ipv4.ip_forward

net.ipv4.ip_forward = 1</pre></div><p><code class="literal">net.ipv4.ip_forward</code> must be set to <code class="literal">1</code>. Additionally, you can check in what order persisted rules are processed by running <code class="literal">sysctl --system -a</code>.</p></div><div class="sect2" id="_networking_whitelist"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Whitelist</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_networking_whitelist">#</a></h3></div></div></div><p>Besides the SUSE provided packages and containers, SUSE CaaS Platform is typically used with third party provided containers and charts.</p><p>The following SUSE provided resources must be available:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">URL</th><th align="left" valign="top">Name</th><th align="left" valign="top">Purpose</th></tr></thead><tbody><tr><td align="left" valign="top"><p>scc.suse.com</p></td><td align="left" valign="top"><p>SUSE Customer Center</p></td><td align="left" valign="top"><p>Allow registration and license activation</p></td></tr><tr><td align="left" valign="top"><p>registry.suse.com</p></td><td align="left" valign="top"><p>SUSE container registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr><tr><td align="left" valign="top"><p>*.cloudfront.net</p></td><td align="left" valign="top"><p>Cloudfront</p></td><td align="left" valign="top"><p>CDN/distribution backend for <code class="literal">registry.suse.com</code></p></td></tr><tr><td align="left" valign="top"><p>kubernetes-charts.suse.com</p></td><td align="left" valign="top"><p>SUSE helm charts repository</p></td><td align="left" valign="top"><p>Provide helm charts</p></td></tr><tr><td align="left" valign="top"><p>updates.suse.com</p></td><td align="left" valign="top"><p>SUSE package update channel</p></td><td align="left" valign="top"><p>Provide package updates</p></td></tr></tbody></table></div><p>If you wish to use Upstream / Third-Party resources, please also allow the following:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">URL</th><th align="left" valign="top">Name</th><th align="left" valign="top">Purpose</th></tr></thead><tbody><tr><td align="left" valign="top"><p>k8s.gcr.io</p></td><td align="left" valign="top"><p>Google Container Registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr><tr><td align="left" valign="top"><p>kubernetes-charts.storage.googleapis.com</p></td><td align="left" valign="top"><p>Google Helm charts repository</p></td><td align="left" valign="top"><p>Provide helm charts</p></td></tr><tr><td align="left" valign="top"><p>docker.io</p></td><td align="left" valign="top"><p>Docker Container Registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr><tr><td align="left" valign="top"><p>quay.io</p></td><td align="left" valign="top"><p>Red Hat Container Registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr></tbody></table></div><p>Please note that not all installation scenarios will need all of these resources.</p><div id="id-1.4.5.9.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you are deploying into an air gap scenario, you must ensure that the resources required
from these locations are present and available on your internal mirror server.</p></div></div><div class="sect2" id="_communication"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Communication</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_communication">#</a></h3></div></div></div><p>Please make sure that all your Kubernetes components can communicate with each other.
This might require the configuration of routing when using multiple network adapters per node.</p><p>Refer to: <a class="link" href="https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters" target="_blank">https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters</a>.</p><p>Configure firewall and other network security to allow communication on the default ports required by Kubernetes: <a class="link" href="https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports" target="_blank">https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports</a></p></div><div class="sect2" id="_performance"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_performance">#</a></h3></div></div></div><p>All master nodes of the cluster must have a minimum 1Gb/s network connection to fulfill the requirements for etcd.</p><div class="verbatim-wrap"><pre class="screen">"1GbE is sufficient for common etcd deployments. For large etcd clusters,
a 10GbE network will reduce mean time to recovery."</pre></div><p><a class="link" href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#network" target="_blank">https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#network</a></p></div><div class="sect2" id="_security"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Security</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_security">#</a></h3></div></div></div><p>Do not grant access to the kubeconfig file or any workstation configured with this configuration to unauthorized personnel.
In the current state, full administrative access is granted to the cluster.</p><p>Authentication is done via the kubeconfig file generated during deployment. This file will grant full access to the cluster and all workloads.
Apply best practices for access control to workstations configured to administer the SUSE CaaS Platform cluster.</p><p>The SUSE CaaS Platform leverages Kubernetes role-based access control (RBAC) for authentication and will need to have an external authentication server such as LDAP, Active Directory, or similar to validate the user’s entity and grant different user roles or cluster role permission.</p></div><div class="sect2" id="_replicas"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replicas</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_replicas">#</a></h3></div></div></div><p>Some addon services are desired to be highly available. These services require enough cluster nodes available to run replicas of their services.</p><p>When the cluster is deployed with enough nodes for replica sizes, those service distributions will be balanced across the cluster.</p><p>For clusters deployed with a node number lower than the default replica sizes, services will still try to find a suitable node to run on.
However it is likely you will see services all running on the same nodes, defeating the purpose of high availability.</p><p>You can check the deployment replica size after node bootstrap. The number of cluster nodes should be equal or greater than the <code class="literal">DESIRED</code> replica size.</p><div class="verbatim-wrap"><pre class="screen">kubectl get rs -n kube-system</pre></div><p>After deployment, if the number of healthy nodes falls below the number required for fulfilling the replica sizing, service replicas will show in <code class="literal">Pending</code> state until either the unhealthy node recovers or a new node is joined to cluster.</p><p>The following describes two methods for replica management if you wish to work with a cluster below the default replica size requirement.</p><div class="sect3" id="_update_replica_number"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.4.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update replica number</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_update_replica_number">#</a></h4></div></div></div><p>One method is to update the number of overall replicas being created by a service.
Please consult the documentation of your respective service what the replica limits for proper high availability are.
In case the replica number is too high for the cluster, you must increase the cluster size to provide more resources.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Update deployment replica size before node joining.</p><div id="id-1.4.5.13.9.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You can use the same steps to increase the replica size again if more resources become available later on.</p></div><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system scale --replicas=&lt;DESIRED_REPLICAS&gt; deployment &lt;NAME&gt;</pre></div></li><li class="listitem "><p>Join new nodes.</p></li></ol></div></div><div class="sect3" id="_re_distribute_replicas"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.4.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-distribute replicas</span> <a title="Permalink" class="permalink" href="deployment-system-requirements.html#_re_distribute_replicas">#</a></h4></div></div></div><p>When multiple replicas are running on the same pod you will want to redistribute those manually to ensure proper high availability,</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Find the pod for re-distribution.
Check the <code class="literal">NAME</code> and <code class="literal">NODE</code> column for duplicated pods.</p><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system get pod -o wide</pre></div></li><li class="listitem "><p>Delete duplicated pod. This will trigger another pod creation.</p><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system delete pod &lt;POD_NAME&gt;</pre></div></li></ol></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="_deployment_scenarios.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 2 </span>Deployment Scenarios</span></a><a class="nav-link" href="_about_this_guide.html"><span class="prev-icon">←</span><span class="nav-label">About This Guide</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>