<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Deployment on Bare Metal or KVM | QuickStart Guide | SUSE CaaS Platform 4.5.1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.1" /><meta name="book-title" content="QuickStart Guide" /><meta name="chapter-title" content="Chapter 3. Deployment on Bare Metal or KVM" /><meta name="description" content="You must have completed Chapter 2, Deployment Preparations to proceed." /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="QuickstartGuide" /><link rel="home" href="index.html" title="QuickStart Guide" /><link rel="up" href="index.html" title="QuickStart Guide" /><link rel="prev" href="deployment-preparations.html" title="Chapter 2. Deployment Preparations" /><link rel="next" href="_glossary.html" title="Chapter 4. Glossary" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="QuickStart Guide"><span class="book-icon">QuickStart Guide</span></a><span> › </span><a class="crumb" href="deployment-bare-metal.html">Deployment on Bare Metal or KVM</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>QuickStart Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pr01.html"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="deployment-system-requirements.html"><span class="number">1 </span><span class="name">Requirements</span></a></li><li class="inactive"><a href="deployment-preparations.html"><span class="number">2 </span><span class="name">Deployment Preparations</span></a></li><li class="inactive"><a href="deployment-bare-metal.html"><span class="number">3 </span><span class="name">Deployment on Bare Metal or KVM</span></a></li><li class="inactive"><a href="_glossary.html"><span class="number">4 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="_contributors.html"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="_gnu_licenses.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 2. Deployment Preparations" href="deployment-preparations.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 4. Glossary" href="_glossary.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="QuickStart Guide"><span class="book-icon">QuickStart Guide</span></a><span> › </span><a class="crumb" href="deployment-bare-metal.html">Deployment on Bare Metal or KVM</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 2. Deployment Preparations" href="deployment-preparations.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 4. Glossary" href="_glossary.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="deployment-bare-metal"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.1</span></div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment on Bare Metal or KVM</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="deployment-bare-metal.html#_environment_description"><span class="number">3.1 </span><span class="name">Environment Description</span></a></span></dt><dt><span class="section"><a href="deployment-bare-metal.html#_autoyast_preparation"><span class="number">3.2 </span><span class="name">AutoYaST Preparation</span></a></span></dt><dt><span class="section"><a href="deployment-bare-metal.html#_provisioning_the_cluster_nodes"><span class="number">3.3 </span><span class="name">Provisioning the Cluster Nodes</span></a></span></dt><dt><span class="section"><a href="deployment-bare-metal.html#_container_runtime_proxy"><span class="number">3.4 </span><span class="name">Container Runtime Proxy</span></a></span></dt><dt><span class="section"><a href="deployment-bare-metal.html#bootstrap"><span class="number">3.5 </span><span class="name">Bootstrapping the Cluster</span></a></span></dt></dl></div></div><div id="id-1.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Preparation Required</h6><p>You must have completed <a class="xref" href="deployment-preparations.html" title="Chapter 2. Deployment Preparations">Chapter 2, <em>Deployment Preparations</em></a> to proceed.</p></div><div id="id-1.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If deploying on KVM virtual machines, you may use a tool such as <code class="literal">virt-manager</code>
to configure the virtual machines and begin the SUSE Linux Enterprise Server 15 SP2 installation.</p></div><div class="sect1" id="_environment_description"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Environment Description</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_environment_description">#</a></h2></div></div></div><div id="id-1.5.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You must have a load balancer configured as described in <a class="xref" href="deployment-preparations.html#loadbalancer" title="2.5. Load Balancer">Section 2.5, “Load Balancer”</a>.</p></div><div id="id-1.5.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The AutoYaST file found in <code class="literal">skuba</code> is a template. It has the base requirements.
This AutoYaST file should act as a guide and should be updated with your company’s standards.</p></div><div id="id-1.5.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>To account for hardware/platform-specific setup criteria (legacy BIOS vs. (U)EFI, drive partitioning, networking, etc.),
you must adjust the AutoYaST file to your needs according to the requirements.</p><p>Refer to the official AutoYaST documentation for more information: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#book-autoyast" target="_blank">AutoYaST Guide</a>.</p></div><div class="sect2" id="_machine_configuration_prerequisites"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Machine Configuration Prerequisites</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_machine_configuration_prerequisites">#</a></h3></div></div></div><p>Deployment with AutoYaST will require a minimum <span class="strong"><strong>disk size of 40 GB</strong></span>.
That space is reserved for container images without any workloads (10 GB),
for the root partition (30 GB) and the EFI system partition (200 MB).</p></div></div><div class="sect1" id="_autoyast_preparation"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">AutoYaST Preparation</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_autoyast_preparation">#</a></h2></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>On the management machine, get an example AutoYaST file from <code class="literal">/usr/share/caasp/autoyast/bare-metal/autoyast.xml</code>,
(which was installed earlier on as part of the management pattern (<code class="literal">sudo zypper in -t pattern SUSE-CaaSP-Management</code>).</p></li><li class="listitem "><p>Copy the file to a suitable location to modify it. Name the file <code class="literal">autoyast.xml</code>.</p></li><li class="listitem "><p>Modify the following places in the AutoYaST file (and any additional places as required by your specific configuration/environment):</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p><code class="literal">&lt;ntp-client&gt;</code></p><p>Change the pre-filled value to your organization’s NTP server. Provide multiple servers if possible by adding new <code class="literal">&lt;ntp_server&gt;</code> subentries.</p></li><li class="listitem "><p><code class="literal">&lt;timezone&gt;</code></p><p>Adjust the timezone your nodes will be set to. Refer to: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#id-1.7.5.13.6" target="_blank">SUSE Linux Enterprise Server AutoYaST Guide: Country Settings</a></p></li><li class="listitem "><p><code class="literal">&lt;username&gt;sles&lt;/username&gt;</code></p><p>Insert your authorized key in the placeholder field.</p></li><li class="listitem "><p><code class="literal">&lt;users&gt;</code></p><p>You can add additional users by creating new blocks in the configuration containing their data.</p><div id="id-1.5.5.2.3.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If the users are configured to not have a password like in the example, ensure the system’s <code class="literal">sudoers</code> file is updated.
Without updating the sudoers file the user will only be able to perform basic operations that will prohibit many administrative tasks.</p><p>The default AutoYaST file provides examples for a disabled <code class="literal">root</code> user and a <code class="literal">sles</code> user with authorized key SSH access.</p><p>The password for root can be enabled by using the <code class="literal">passwd</code> command.</p></div></li><li class="listitem "><p><code class="literal">&lt;suse_register&gt;</code></p><p>Insert the email address and SUSE CaaS Platform registration code in the placeholder fields. This activates SUSE Linux Enterprise Server 15 SP2.</p></li><li class="listitem "><p><code class="literal">&lt;addon&gt;</code></p><p>Insert the SUSE CaaS Platform registration code in the placeholder field. This enables the SUSE CaaS Platform extension module.
Update the AutoYaST file with your registration keys and your company’s best practices and hardware configurations.</p><div id="id-1.5.5.2.3.2.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Your SUSE CaaS Platform registration key can be used to both activate SUSE Linux Enterprise Server 15 SP2 and enable the extension.</p></div></li></ol></div><p>Refer to the official AutoYaST documentation for more information: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#book-autoyast" target="_blank">AutoYaST Guide</a>.</p></li><li class="listitem "><p>Host the AutoYaST files on a Web server reachable inside the network you are installing the cluster in.</p></li></ol></div><div class="sect2" id="_deploying_with_local_repository_mirroring_tool_rmt_server"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying with local Repository Mirroring Tool (RMT) server</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_deploying_with_local_repository_mirroring_tool_rmt_server">#</a></h3></div></div></div><p>In order to use a local Repository Mirroring Tool (RMT) server for deployment of packages, you need to specify
the server configuration in your AutoYaST file. To do so add the following section:</p><div class="verbatim-wrap highlight xml"><pre class="screen">&lt;suse_register&gt;
&lt;do_registration config:type="boolean"&gt;true&lt;/do_registration&gt;
&lt;install_updates config:type="boolean"&gt;true&lt;/install_updates&gt;

&lt;reg_server&gt;https://rmt.example.org&lt;/reg_server&gt; <span id="CO6-1"></span><span class="callout">1</span>
&lt;reg_server_cert&gt;https://rmt.example.org/rmt.crt&lt;/reg_server_cert&gt; <span id="CO6-2"></span><span class="callout">2</span>
&lt;reg_server_cert_fingerprint_type&gt;SHA1&lt;/reg_server_cert_fingerprint_type&gt;
&lt;reg_server_cert_fingerprint&gt;0C:A4:A1:06:AD:E2:A2:AA:D0:08:28:95:05:91:4C:07:AD:13:78:FE&lt;/reg_server_cert_fingerprint&gt; <span id="CO6-3"></span><span class="callout">3</span>
&lt;slp_discovery config:type="boolean"&gt;false&lt;/slp_discovery&gt;
&lt;addons config:type="list"&gt;
  &lt;addon&gt;
    &lt;name&gt;sle-module-containers&lt;/name&gt;
    &lt;version&gt;15.2&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
  &lt;addon&gt;
    &lt;name&gt;sle-module-public-cloud&lt;/name&gt;
    &lt;version&gt;15.2&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
  &lt;addon&gt;
    &lt;name&gt;caasp&lt;/name&gt;
    &lt;version&gt;4.5&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
&lt;/addons&gt;
&lt;/suse_register&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Provide FQDN of the Repository Mirroring Tool (RMT) server</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Provide the location on the server where the certificate can be found</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Provide the certificate fingerprint for the Repository Mirroring Tool (RMT) server</p></td></tr></table></div></div></div><div class="sect1" id="_provisioning_the_cluster_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning the Cluster Nodes</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_provisioning_the_cluster_nodes">#</a></h2></div></div></div><p>Once the AutoYaST file is available in the network that the machines will be configured in, you can start deploying machines.</p><p>The default production scenario consists of 6 nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 load balancer</p></li><li class="listitem "><p>3 masters</p></li><li class="listitem "><p>2 workers</p></li></ul></div><p>Depending on the type of load balancer you wish to use, you need to deploy at least 5 machines to serve as cluster nodes and provide a load balancer from the environment.</p><p>The load balancer must point at the machines that are assigned to be used as <code class="literal">master</code> nodes in the future cluster.</p><div id="id-1.5.6.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you do not wish to use infrastructure load balancers, please deploy additional machines and refer to <a class="xref" href="deployment-preparations.html#loadbalancer" title="2.5. Load Balancer">Section 2.5, “Load Balancer”</a>.</p></div><p>Install SUSE Linux Enterprise Server 15 SP2 from your preferred medium and follow the steps for <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#invoking-autoinst" target="_blank">Invoking the Auto-Installation Process</a></p><p>Provide <code class="literal">autoyast=https://[webserver/path/to/autoyast.xml]</code> during the SUSE Linux Enterprise Server 15 SP2 installation.</p><div class="sect2" id="_suse_linux_enterprise_server_installation"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Linux Enterprise Server Installation</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_suse_linux_enterprise_server_installation">#</a></h3></div></div></div><div id="id-1.5.6.10.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Use AutoYaST and make sure to use a staged frozen patchlevel via RMT/SUSE Manager to ensure a 100% reproducible setup.
<a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-rmt/#cha-rmt-client" target="_blank">RMT Guide</a></p></div><p>Once the machines have been installed using the AutoYaST file, you are now ready proceed with <a class="xref" href="deployment-bare-metal.html#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a>.</p></div></div><div class="sect1" id="_container_runtime_proxy"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Container Runtime Proxy</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_container_runtime_proxy">#</a></h2></div></div></div><div id="id-1.5.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>CRI-O proxy settings must be adjusted on all nodes before joining the cluster!</p><p>Please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o</a></p><p>In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">SUSE CaaS Platform Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O</a></p></div></div><div class="sect1" id="bootstrap"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bootstrapping the Cluster</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#bootstrap">#</a></h2></div></div></div><p>Bootstrapping the cluster is the initial process of starting up the cluster
and defining which of the nodes are masters and which are workers. For maximum automation of this process,
SUSE CaaS Platform uses the <code class="literal">skuba</code> package.</p><div class="sect2" id="_preparation"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_preparation">#</a></h3></div></div></div><div class="sect3" id="_install_skuba"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install <code class="literal">skuba</code></span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_install_skuba">#</a></h4></div></div></div><p>First you need to install <code class="literal">skuba</code> on a management machine, like your local workstation:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Add the SLE15 SP2 extension containing <code class="literal">skuba</code>. This also requires the "containers" and the "public cloud" module.</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -p sle-module-containers/15.2/x86_64
SUSEConnect -p sle-module-public-cloud/15.2/x86_64
SUSEConnect -p caasp/4.5/x86_64 -r &lt;PRODUCT_KEY&gt;</pre></div></li><li class="listitem "><p>Install the management pattern with:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in -t pattern SUSE-CaaSP-Management</pre></div></li></ol></div><div id="id-1.5.8.3.2.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Example deployment configuration files for each deployment scenario are installed
under <code class="literal">/usr/share/caasp/terraform/</code>, or in case of the bare metal deployment:
<code class="literal">/usr/share/caasp/autoyast/</code>.</p></div></div><div class="sect3" id="_container_runtime_proxy_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Container Runtime Proxy</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_container_runtime_proxy_2">#</a></h4></div></div></div><div id="id-1.5.8.3.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>CRI-O proxy settings must be adjusted manually on all nodes before joining the cluster!</p></div><p>In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">SUSE CaaS Platform Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O</a></p></div></div><div class="sect2" id="_cluster_deployment"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Deployment</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_cluster_deployment">#</a></h3></div></div></div><p>Make sure you have added the SSH identity (corresponding to the public SSH key distributed above)
to the ssh-agent on your workstation. For instructions on how to add the SSH identity,
refer to <a class="xref" href="deployment-preparations.html#ssh-configuration" title="2.1. Basic SSH Key Configuration">Section 2.1, “Basic SSH Key Configuration”</a>.</p><p>This is a requirement for <code class="literal">skuba</code> (<a class="link" href="https://github.com/SUSE/skuba#prerequisites" target="_blank">https://github.com/SUSE/skuba#prerequisites</a>).</p><p>By default <code class="literal">skuba</code> connects to the nodes as <code class="literal">root</code> user. A different user can
be specified by the following flags:</p><div class="verbatim-wrap highlight bash"><pre class="screen">--sudo --user &lt;USERNAME&gt;</pre></div><div id="id-1.5.8.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>You must configure <code class="literal">sudo</code> for the user to be able to authenticate without password.
Replace <code class="literal">&lt;USERNAME&gt;</code> with the user you created during installation. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">echo "&lt;USERNAME&gt; ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoers</pre></div></div><div class="sect3" id="_initializing_the_cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initializing the Cluster</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_initializing_the_cluster">#</a></h4></div></div></div><div id="id-1.5.8.4.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Secure configuration files access</h6><p>The directory created during this step contains configuration files
that allow full administrator access to your cluster.
Apply best practices for access control to this folder.</p></div><p>Now you can initialize the cluster on the deployed machines.
As <code class="literal">--control-plane</code> enter the IP/FQDN of your load balancer.
If you do not use a load balancer use your first master node.</p><div id="id-1.5.8.4.7.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If you are deploying on a cloud provider you must enable vendor specific integrations (CPI).
Please refer further below to <a class="xref" href="deployment-bare-metal.html#enabling-cpi" title="3.5.2.3.1. Enabling Cloud Provider Integration">Section 3.5.2.3.1, “Enabling Cloud Provider Integration”</a>.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB_IP/FQDN&gt; &lt;CLUSTER_NAME&gt;</pre></div><p><code class="literal">cluster init</code> generates the folder named <code class="literal">&lt;CLUSTER_NAME&gt;</code> and initializes the directory that will hold the configuration (<code class="literal">kubeconfig</code>) for the cluster.</p><div id="id-1.5.8.4.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost cannot be used.</p></div><div class="sect4" id="_transitioning_from_docker_to_cri_o"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Transitioning from Docker to CRI-O</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_transitioning_from_docker_to_cri_o">#</a></h5></div></div></div><p>SUSE CaaS Platform 4.5.1 <span class="strong"><strong>default configuration</strong></span> uses the CRI-O Container Engine in conjunction with Docker Linux capabilities.
This means SUSE CaaS Platform 4.5.1 containers run on top of CRI-O with the following additional
Linux capabilities: <code class="literal">audit_write</code>, <code class="literal">setfcap</code> and <code class="literal">mknod</code>.
This measure ensures a transparent transition and seamless compatibility with workloads running
on the previous SUSE CaaS Platform versions and out-of-the-box Docker compatibility.</p><p>In case you wish to use <span class="strong"><strong>unmodified CRI-O</strong></span>,
use the <code class="literal">--strict-capability-defaults</code> option during the initial setup when you run <code class="literal">skuba cluster init</code>,
which will create the vanilla CRI-O configuration:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --strict-capability-defaults</pre></div><p>Please be aware that this might result in
incompatibility with your previously running workloads,
unless you explicitly define the additional Linux capabilities required
on top of CRI-O defaults.</p><div id="id-1.5.8.4.7.8.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>After the bootstrap of the Kubernetes cluster there will be no easy
way to revert this modification. Please choose wisely.</p></div></div></div><div class="sect3" id="_configuring_kubernetes_services"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Kubernetes Services</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_configuring_kubernetes_services">#</a></h4></div></div></div><p>Inspect the <code class="literal">kubeadm-init.conf</code> file inside your cluster definition and set extra configuration settings supported by <code class="literal">kubeadm</code>.
The latest supported version is v1beta1.
Later, when you later run <code class="literal">skuba node bootstrap</code>, <code class="literal">kubeadm</code> will read <code class="literal">kubeadm-init.conf</code>
and will forcefully set certain settings to the ones required by SUSE CaaS Platform.</p><div class="sect4" id="_network_settings"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Settings</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_network_settings">#</a></h5></div></div></div><p>The default network settings inside <code class="literal">kubeadm-init.conf</code> are viable for production clusters and adjusting them is optional.
If you however wish to change the pod and service subnets, it is important that you do so before the bootstrap.
The subnet ranges must be planned carefully,
because the settings cannot be adjusted after deployment is complete.
The default settings are the following:</p><div class="verbatim-wrap"><pre class="screen">networking:
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12</pre></div><p>The <code class="literal">podSubnet</code> IP range must be big enough to contain all IP addresses for all PODs planned for the cluster.
The subnet also mustn’t conflict with services from outside of the cluster - external databases, file services, etc.
This also holds for <code class="literal">serviceSubnet</code> - the IP range must not conflict with external services and needs to be broad enough for all services planned for the cluster.</p></div></div><div class="sect3" id="_cluster_configuration"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Configuration</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_cluster_configuration">#</a></h4></div></div></div><p>Before bootstrapping the cluster, it is advisable to perform some additional configuration.</p><div class="sect4" id="enabling-cpi"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Cloud Provider Integration</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#enabling-cpi">#</a></h5></div></div></div><p>Enable cloud provider integration to take advantage of the underlying cloud platforms
and automatically manage resources like the Load Balancer, Nodes (Instances), Network Routes
and Storage services.</p><p>If you want to enable cloud provider integration with different cloud platforms,
initialize the cluster with the flag <code class="literal">--cloud-provider &lt;CLOUD PROVIDER&gt;</code>.
The only currently available options are <code class="literal">aws</code>, <code class="literal">azure</code>, <code class="literal">openstack</code> and <code class="literal">vsphere</code>,
but more options are planned.</p><div id="id-1.5.8.4.9.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Cleanup</h6><p>By enabling CPI providers your Kubernetes cluster will be able to
provision cloud resources on its own (eg: Load Balancers, Persistent Volumes).
You will have to manually clean these resources before you destroy the cluster
with Terraform.</p><p>Not removing resources like Load Balancers created by the CPI will result in
Terraform timing out during <code class="literal">destroy</code> operations.</p><p>Persistent volumes created with the <code class="literal">retain</code> policy will exist inside of
the external cloud infrastructure even after the cluster is removed.</p></div><div class="sect5" id="_openstack_cpi"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.5.2.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack CPI</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_openstack_cpi">#</a></h6></div></div></div><p>Define the cluster using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB_IP/FQDN&gt; --cloud-provider openstack &lt;CLUSTER_NAME&gt;</pre></div><p>Running the above command will create a directory <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/openstack</code> with a
<code class="literal">README.md</code> and an <code class="literal">openstack.conf.template</code> in it. Copy <code class="literal">openstack.conf.template</code>
or create an <code class="literal">openstack.conf</code> file inside <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/openstack</code>,
according to the supported format.
The supported format and content can be found in the official Kubernetes documentation:</p><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack</a></p><div id="id-1.5.8.4.9.3.5.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The file <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/openstack/openstack.conf</code> must not be freely accessible.
Please remember to set proper file permissions for it, for example <code class="literal">600</code>.</p></div></div></div><div class="sect4" id="_example_openstack_cloud_provider_configuration"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example OpenStack Cloud Provider Configuration</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_example_openstack_cloud_provider_configuration">#</a></h5></div></div></div><p>You can find the required parameters in OpenStack RC File v3.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">[Global]
auth-url=&lt;OS_AUTH_URL&gt; <span id="CO7-1"></span><span class="callout">1</span>
username=&lt;OS_USERNAME&gt; <span id="CO7-2"></span><span class="callout">2</span>
password=&lt;OS_PASSWORD&gt; <span id="CO7-3"></span><span class="callout">3</span>
tenant-id=&lt;OS_PROJECT_ID&gt; <span id="CO7-4"></span><span class="callout">4</span>
domain-name=&lt;OS_USER_DOMAIN_NAME&gt; <span id="CO7-5"></span><span class="callout">5</span>
region=&lt;OS_REGION_NAME&gt; <span id="CO7-6"></span><span class="callout">6</span>
ca-file="/etc/pki/trust/anchors/SUSE_Trust_Root.pem" <span id="CO7-7"></span><span class="callout">7</span>
[LoadBalancer]
lb-version=v2 <span id="CO7-8"></span><span class="callout">8</span>
subnet-id=&lt;PRIVATE_SUBNET_ID&gt; <span id="CO7-9"></span><span class="callout">9</span>
floating-network-id=&lt;PUBLIC_NET_ID&gt; <span id="CO7-10"></span><span class="callout">10</span>
create-monitor=yes <span id="CO7-11"></span><span class="callout">11</span>
monitor-delay=1m <span id="CO7-12"></span><span class="callout">12</span>
monitor-timeout=30s <span id="CO7-13"></span><span class="callout">13</span>
monitor-max-retries=3 <span id="CO7-14"></span><span class="callout">14</span>
[BlockStorage]
bs-version=v2 <span id="CO7-15"></span><span class="callout">15</span>
ignore-volume-az=true <span id="CO7-16"></span><span class="callout">16</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>(required) Specifies the URL of the Keystone API used to authenticate the user.
This value can be found in Horizon (the OpenStack control panel).
under Project &gt; Access and Security &gt; API Access &gt; Credentials.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the username of a valid user set in Keystone.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the password of a valid user set in Keystone.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>(required) Used to specify the ID of the project where you want to create your resources.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the name of the domain your user belongs to.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the identifier of the region to use when running on
a multi-region OpenStack cloud. A region is a general division of an OpenStack deployment.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the path to your custom CA file.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to override automatic version detection.
Valid values are <code class="literal">v1</code> or <code class="literal">v2</code>. Where no value is provided, automatic detection
will select the highest supported version exposed by the underlying OpenStack cloud.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the ID of the subnet you want to create your load balancer on.
Can be found at Network &gt; Networks. Click on the respective network to get its subnets.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-10"><span class="callout">10</span></a> </p></td><td valign="top" align="left"><p>(optional) If specified, will create a floating IP for the load balancer.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-11"><span class="callout">11</span></a> </p></td><td valign="top" align="left"><p>(optional) Indicates whether or not to create a health monitor for the Neutron load balancer.
Valid values are true and false. The default is false.
When true is specified then monitor-delay, monitor-timeout, and monitor-max-retries must also be set.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-12"><span class="callout">12</span></a> </p></td><td valign="top" align="left"><p>(optional) The time between sending probes to members of the load balancer.
Ensure that you specify a valid time unit.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-13"><span class="callout">13</span></a> </p></td><td valign="top" align="left"><p>(optional) Maximum time for a monitor to wait for a ping reply before it times out.
The value must be less than the delay value. Ensure that you specify a valid time unit.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-14"><span class="callout">14</span></a> </p></td><td valign="top" align="left"><p>(optional) Number of permissible ping failures before changing the load balancer
member’s status to INACTIVE. Must be a number between 1 and 10.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-15"><span class="callout">15</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to override automatic version detection.
Valid values are v1, v2, v3 and auto. When auto is specified, automatic detection
will select the highest supported version exposed by the underlying OpenStack cloud.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-16"><span class="callout">16</span></a> </p></td><td valign="top" align="left"><p>(optional) Influences availability zone, use when attaching Cinder volumes.
When Nova and Cinder have different availability zones, this should be set to <code class="literal">true</code>.</p></td></tr></table></div><p>After setting options in the <code class="literal">openstack.conf</code> file, please proceed with <a class="xref" href="deployment-bare-metal.html#cluster-bootstrap" title="3.5.2.5. Cluster Bootstrap">Section 3.5.2.5, “Cluster Bootstrap”</a>.</p><div id="id-1.5.8.4.9.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>When cloud provider integration is enabled, it’s very important to bootstrap and join nodes with the same node names that they have inside <code class="literal">Openstack</code>, as
these names will be used by the <code class="literal">Openstack</code> cloud controller manager to reconcile node metadata.</p></div><div class="sect5" id="_amazon_web_services_aws_cpi"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.5.2.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Amazon Web Services (AWS) CPI</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_amazon_web_services_aws_cpi">#</a></h6></div></div></div><p>Define the cluster using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB IP/FQDN&gt; --cloud-provider aws &lt;CLUSTER_NAME&gt;</pre></div><p>Running the above command will create a directory <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/aws</code> with a
<code class="literal">README.md</code> file in it. No further configuration files are needed.</p><p>The supported format and content can be found in the
<a class="link" href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws" target="_blank">official Kubernetes documentation</a>.</p><div id="id-1.5.8.4.9.4.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>When cloud provider integration is enabled, it’s very important to bootstrap and join nodes with the same node names that they have inside <code class="literal">AWS</code>, as
these names will be used by the <code class="literal">AWS</code> cloud controller manager to reconcile node metadata.</p><p>You can use the "private dns" values provided by the Terraform output.</p></div></div><div class="sect5" id="_azure_in_tree_cpi"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.5.2.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Azure In-tree CPI</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_azure_in_tree_cpi">#</a></h6></div></div></div><div id="id-1.5.8.4.9.4.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>This feature is offered as a "tech preview".</p><p>We release this as a tech-preview in order to get early feedback from our customers.
Tech previews are largely untested, unsupported, and thus not ready for production use.</p><p>That said, we strongly believe this technology is useful at this stage in order to make the right improvements based on your feedback.
A fully supported, production-ready release is planned for a later point in time.</p></div><p>Define the cluster using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB_IP/FQDN&gt; --cloud-provider azure my-cluster</pre></div><p>Running the above command will create a directory <code class="literal">my-cluster/cloud/azure</code> with a
<code class="literal">README.md</code> and a <code class="literal">azure.conf.template</code> in it. Copy <code class="literal">azure.conf.template</code>
or create a <code class="literal">azure.conf</code> file inside <code class="literal">my-cluster/cloud/azure</code>, according to the supported format.</p><p>The supported format and content can be found in the <a class="link" href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#azure" target="_blank">official Kubernetes documentation</a>.</p><div id="id-1.5.8.4.9.4.8.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The file <code class="literal">my-cluster/cloud/azure/azure.conf</code> must not be freely accessible.
Please remember to set proper file permissions for it, for example <code class="literal">600</code>.</p></div></div></div><div class="sect4" id="_example_azure_cloud_provider_configuration"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Azure Cloud Provider Configuration</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_example_azure_cloud_provider_configuration">#</a></h5></div></div></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">{
    "cloud": "AzurePublicCloud",
    "tenantId": "&lt;AZ_TENANT_ID&gt;", <span id="CO8-1"></span><span class="callout">1</span>
    "aadClientId": "&lt;AZ_AAD_CLIENT_ID&gt;", <span id="CO8-2"></span><span class="callout">2</span>
    "aadClientSecret": "&lt;AZ_AAD_CLIENT_SECRET&gt;", <span id="CO8-3"></span><span class="callout">3</span>
    "subscriptionId": "&lt;AZ_SUBSCRIPTION_ID&gt;", <span id="CO8-4"></span><span class="callout">4</span>
    "resourceGroup": "&lt;AZ_RESOURCE_GROUP&gt;", <span id="CO8-5"></span><span class="callout">5</span>
    "location": "&lt;AZ_LOCATION&gt;", <span id="CO8-6"></span><span class="callout">6</span>
    "routeTableName": "&lt;AZ_ROUTE_TABLE_NAME&gt;", <span id="CO8-7"></span><span class="callout">7</span>
    "useManagedIdentityExtension": true, <span id="CO8-8"></span><span class="callout">8</span>
    "useInstanceMetadata": true
}</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>(required) The AAD Tenant ID for the Subscription that the cluster is deployed in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>(optional) The ClientID for an AAD application with RBAC access to talk to Azure RM APIs. This is used for service principle authentication.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>(optional) The ClientSecret for an AAD application with RBAC access to talk to Azure RM APIs. This is used for service principle client secret authentication.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>(required) The ID of the Azure Subscription that the cluster is deployed in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>(required) The name of the resource group that the cluster is deployed in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>(required) The location of the resource group that the cluster is deployed in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>(required) The name of the route table attached to the subnet that the cluster is deployed in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>(optional) Use managed service identity for the virtual machine to access Azure ARM APIs. Set to false if not using managed identity authentication.</p></td></tr></table></div><p>After setting options in the <code class="literal">azure.conf</code> file, please proceed with <a class="xref" href="deployment-bare-metal.html#cluster-bootstrap" title="3.5.2.5. Cluster Bootstrap">Section 3.5.2.5, “Cluster Bootstrap”</a>.</p><div id="id-1.5.8.4.9.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Clusters provisioned following <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-deployment/#_using_terraform" target="_blank">Deploying VMs from the Template</a> with <code class="literal">cpi_enable = true</code> automatically configurs Azure virtual machine to use a system-assigned managed identity.</p><p>To use service principle authentication you need to <a class="xref" href="deployment-bare-metal.html#cluster-bootstrap-azure-create-service-principle" title="3.5.2.3.4. Create Service Principle">Section 3.5.2.3.4, “Create Service Principle”</a> and provide <code class="literal">aadClientId</code>, <code class="literal">aadClientSecret</code> and configure <code class="literal">useManagedIdentityExtension</code> to <code class="literal">false</code>.</p></div><div id="id-1.5.8.4.9.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>When cloud provider integration is enabled, it’s very important to bootstrap and join nodes with the node names same as <sub>Azure</sub> virtual machine’s hostnames.
These names will be used by the <code class="literal">Azure</code> cloud controller manager to reconcile node metadata.</p></div></div><div class="sect4" id="cluster-bootstrap-azure-create-service-principle"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Service Principle</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#cluster-bootstrap-azure-create-service-principle">#</a></h5></div></div></div><div id="id-1.5.8.4.9.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Ensure there is sufficient azure account permission. The account needs to be either a member of the <code class="literal">Owner</code> role, or have <code class="literal">Contributor</code> plus <code class="literal">User Access Administrator</code> roles.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install Azure CLI on local machine.</p><p>For installation instructions, refer to <a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank">Install the Azure CLI</a>.</p></li><li class="listitem "><p>Create service principle.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">RESOURCE_GROUP="&lt;RESOURCE_GROUP_NAME&gt;" <span id="CO9-1"></span><span class="callout">1</span>
SUBSCRIPTION="&lt;SUBSCRIPTION_NAME&gt;" <span id="CO9-2"></span><span class="callout">2</span>
SUBSCRIPTION_ID=`az account list --output table | grep "${SUBSCRIPTION_NAME}" | rev | awk '{print $3}' | rev`</pre></div><div class="verbatim-wrap"><pre class="screen">az ad sp create-for-rbac --role="Contributor" --scopes="/subscriptions/${SUBSCRIPTION_ID}"</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The name of the resource group that the cluster is deployed in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The ID of the Azure Subscription that the cluster is deployed in.</p></td></tr></table></div></li><li class="listitem "><p>Record the <code class="literal">appId</code>, <code class="literal">password</code> and <code class="literal">tenant</code> to use in Azure cloud provider configuration.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">Creating a role assignment under the scope of "/subscriptions/00000000-0000-0000-0000-000000000000"
{
  "appId": "00000000-0000-0000-0000-000000000000", <span id="CO10-1"></span><span class="callout">1</span>
  "displayName": "azure-cli-2020-06-12-00-51-10",
  "name": "http://azure-cli-2020-06-12-00-51-10",
  "password": "P`_g:V\\v[cJ6fAu~/2Ak7$F1w1C@[lRi", <span id="CO10-2"></span><span class="callout">2</span>
  "tenant": "000000000000000000-0000-000000000000" <span id="CO10-3"></span><span class="callout">3</span>
}</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The appId applies to aadClientId.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The password applies to aadClientSecret.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Then tenant applies to tenantId.</p></td></tr></table></div></li></ol></div><div class="sect5" id="_vsphere_cpi_vcp"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.5.2.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">vSphere CPI (VCP)</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_vsphere_cpi_vcp">#</a></h6></div></div></div><p>Define the cluster using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB_IP/FQDN&gt; --cloud-provider vsphere &lt;CLUSTER_NAME&gt;</pre></div><p>Running the above command will create a directory <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/vsphere</code> with a
<code class="literal">README.md</code> and a <code class="literal">vsphere.conf.template</code> in it. Copy <code class="literal">vsphere.conf.template</code>
or create a <code class="literal">vsphere.conf</code> file inside <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/vsphere</code>, according to the supported format.</p><p>The supported format and content can be found in the <a class="link" href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#vsphere" target="_blank">official Kubernetes documentation</a>.</p><div id="id-1.5.8.4.9.6.4.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The file <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/vsphere/vsphere.conf</code> must not be freely accessible.
Please remember to set proper file permissions for it, for example <code class="literal">600</code>.</p></div></div></div><div class="sect4" id="vsphere-cloud-provider-configuration"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example vSphere Cloud Provider Configuration</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#vsphere-cloud-provider-configuration">#</a></h5></div></div></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">[Global]
user = "&lt;VC_ADMIN_USERNAME&gt;" <span id="CO11-1"></span><span class="callout">1</span>
password = "&lt;VC_ADMIN_PASSWORD&gt;" <span id="CO11-2"></span><span class="callout">2</span>
port = "443" <span id="CO11-3"></span><span class="callout">3</span>
insecure-flag = "1" <span id="CO11-4"></span><span class="callout">4</span>
[VirtualCenter "&lt;VC_IP_OR_FQDN&gt;"] <span id="CO11-5"></span><span class="callout">5</span>
datacenters = "&lt;VC_DATACENTERS&gt;" <span id="CO11-6"></span><span class="callout">6</span>
[Workspace]
server = "&lt;VC_IP_OR_FQDN&gt;" <span id="CO11-7"></span><span class="callout">7</span>
datacenter = "&lt;VC_DATACENTER&gt;" <span id="CO11-8"></span><span class="callout">8</span>
default-datastore = "&lt;VC_DATASTORE&gt;" <span id="CO11-9"></span><span class="callout">9</span>
resourcepool-path = "&lt;VC_RESOURCEPOOL_PATH&gt;" <span id="CO11-10"></span><span class="callout">10</span>
folder = "&lt;VC_VM_FOLDER&gt;" <span id="CO11-11"></span><span class="callout">11</span>
[Disk]
scsicontrollertype = pvscsi <span id="CO11-12"></span><span class="callout">12</span>
[Network]
public-network = "VM Network" <span id="CO11-13"></span><span class="callout">13</span>
[Labels] <span id="CO11-14"></span><span class="callout">14</span>
region = "&lt;VC_DATACENTER_TAG&gt;" <span id="CO11-15"></span><span class="callout">15</span>
zone = "&lt;VC_CLUSTER_TAG&gt;" <span id="CO11-16"></span><span class="callout">16</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the vCenter username for vSphere cloud provider to authenticate with.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the vCenter password for vCenter user specified with <code class="literal">user</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>(optional) The vCenter Server Port. The default is 443 if not specified.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>(optional) Set to 1 if vCenter used a self-signed certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>(required) The IP address of the vCenter server.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>(required) The datacenter name in vCenter where Kubernetes nodes reside.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>(required) The IP address of the vCenter server for storage provisioning. Usually the same as <code class="literal">VirtualCenter</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>(required) The datacenter to provision temporary VMs for volume provisioning.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>(required) The default datastore to provision temporary VMs for volume provisioning.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-10"><span class="callout">10</span></a> </p></td><td valign="top" align="left"><p>(required) The resource pool to provision temporary VMs for volume provisioning.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-11"><span class="callout">11</span></a> </p></td><td valign="top" align="left"><p>(required) The vCenter VM folder where Kubernetes nodes are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-12"><span class="callout">12</span></a> </p></td><td valign="top" align="left"><p>(required) Defines the SCSI controller in use on the VMs. Almost always set to <code class="literal">pvscsi</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-13"><span class="callout">13</span></a> </p></td><td valign="top" align="left"><p>(optional) The network in vCenter where Kubernetes nodes should join. The default is "VM Network" if not specified.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-14"><span class="callout">14</span></a> </p></td><td valign="top" align="left"><p>(optional) The feature flag for zone and region support.</p><div id="id-1.5.8.4.9.7.3.14.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The zone and region tags must exist and assigned to datacenter and cluster before bootstrap.
Instruction to tag zones and regions, refer to: <a class="link" href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/zones.html#tag-zones-and-regions-in-vcenter" target="_blank">https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/zones.html#tag-zones-and-regions-in-vcenter</a>.</p></div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-15"><span class="callout">15</span></a> </p></td><td valign="top" align="left"><p>(optional) The category name of the tag assigned to the vCenter datacenter.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-16"><span class="callout">16</span></a> </p></td><td valign="top" align="left"><p>(optional) The category name of the tag assigned to the vCenter cluster.</p></td></tr></table></div><p>After setting options in the <code class="literal">vsphere.conf</code> file, please proceed with <a class="xref" href="deployment-bare-metal.html#cluster-bootstrap" title="3.5.2.5. Cluster Bootstrap">Section 3.5.2.5, “Cluster Bootstrap”</a>.</p><div id="id-1.5.8.4.9.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Set <code class="literal">vSphere</code> virtual machine hostnames</h6><p>When cloud provider integration is enabled, it’s very important to bootstrap and join nodes with the node names same as <code class="literal">vSphere</code> virtual machine’s hostnames.
These names will be used by the <code class="literal">vSphere</code> cloud controller manager to reconcile node metadata.</p></div><div id="id-1.5.8.4.9.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Enable <code class="literal">disk.EnableUUID</code>.</h6><p>Each virtual machine requires to have <code class="literal">disk.EnableUUID</code> enabled to successfully mount the virtual disks.</p><p>Clusters provisioned following <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-deployment/_deployment_instructions.html#_using_terraform" target="_blank">Deploying VMs from the Template</a> with <code class="literal">cpi_enable = true</code> automatically enables <code class="literal">disk.EnableUUID</code>.</p><p>For clusters provisioned by any other method, ensure virtual machines are set to use <code class="literal">disk.EnableUUID</code>.</p><p>For more information, refer to: <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.7/Cloud-Native-Storage/GUID-3501C3F2-7D7C-45E9-B20A-F3F70D1E4679.html" target="_blank">Configure Kubernetes Cluster Virtual Machines</a> .</p></div><div id="id-1.5.8.4.9.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Create a Folder For Your Virtual Machines.</h6><p>All virtual machines must exist in a folder and provide the name of that folder as the <code class="literal">folder</code> variable in the <code class="literal">vsphere.conf</code> before bootstrap.</p><p>Clusters provisioned following <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-deployment/_deployment_instructions.html#_using_terraform" target="_blank">Deploying VMs from the Template</a> with <code class="literal">cpi_enable = true</code> automatically create and place all cluster node virtual machines inside a <code class="literal">*-cluster</code> folder.</p><p>For clusters provisioned by any other method, make sure to create and move all cluster node virtual machines to a folder.</p></div></div><div class="sect4" id="_enable_vsphere_cloud_provider"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable vSphere Cloud Provider</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_enable_vsphere_cloud_provider">#</a></h5></div></div></div><p>For an existing cluster without cloud provider enabled at bootstrap, you can enable it later.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>In vCenter, create a folder and move all cluster virtual machines into the folder.
You can use <code class="literal">govc</code> to automate the task.</p><p>For installation instructions, refer to: <a class="link" href="https://github.com/vmware/govmomi/tree/master/govc" target="_blank">https://github.com/vmware/govmomi/tree/master/govc</a>.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">DATACENTER="&lt;VC_DATACENTER&gt;" <span id="CO12-1"></span><span class="callout">1</span>
CLUSTER_PREFIX="&lt;VC_CLUSTER_PREFIX&gt;" <span id="CO12-2"></span><span class="callout">2</span>
govc folder.create /$DATACENTER/vm/$CLUSTER_PREFIX-cluster
govc object.mv /$DATACENTER/vm/$CLUSTER_PREFIX-\* /$DATACENTER/vm/$CLUSTER_PREFIX-cluster</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The datacenter where cluster virtual machines are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Prefix for all machines of the cluster.</p></td></tr></table></div></li><li class="listitem "><p>In vCenter, enable <code class="literal">disk.UUID</code> for all cluster virtual machines.
You can use <code class="literal">govc</code> to automate the task.</p><div id="id-1.5.8.4.9.8.3.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Setup <code class="literal">disk.enabledUUID</code> requires virtual machine to be powered off. The following script
will setup all virtul machine in parallel, hense resulting some cluster downtimes while
all machines are powered off. Modify the script or simply DO NOT use the script if minimal
downtime is in consideration.</p></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">DATACENTER="PROVO" <span id="CO13-1"></span><span class="callout">1</span>
VMS=("caasp-master-0" "caasp-master-1" "caasp-master-2" "caasp-worker-0" "caasp-worker-1") <span id="CO13-2"></span><span class="callout">2</span></pre></div><div class="verbatim-wrap"><pre class="screen">function setup {
  NAME=$1
  echo "[$NAME]"
  govc vm.power -dc=$DATACENTER -off $NAME
  govc vm.change -dc=$DATACENTER -vm=$NAME -e="disk.enableUUID=1" &amp;&amp;\
    echo "Configured disk.enabledUUID: 1"
  govc vm.power -dc=$DATACENTER -on $NAME
}</pre></div><div class="verbatim-wrap"><pre class="screen">for vm in ${VMS[@]}
do
  setup $vm &amp;
done
wait</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO13-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The datacenter where cluster virtual machines are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO13-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The cluster virtual machine names.</p></td></tr></table></div></li><li class="listitem "><p>Update the provider ID for all Kuberentes nodes.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">DATACENTER="&lt;VC_DATACENTER&gt;" <span id="CO14-1"></span><span class="callout">1</span>
CLUSTER_PREFIX="&lt;VC_CLUSTER_PREFIX&gt;" <span id="CO14-2"></span><span class="callout">2</span>
for vm in $(govc ls "/$DATACENTER/vm/$CLUSTER_PREFIX-cluster")
do
  VM_INFO=$(govc vm.info -json -dc=$DATACENTER -vm.ipath="/$vm" -e=true)
  VM_NAME=$(jq -r ' .VirtualMachines[] | .Name' &lt;&lt;&lt; $VM_INFO)
  [[ $VM_NAME == *"-lb-"* ]] &amp;&amp; continue
  VM_UUID=$( jq -r ' .VirtualMachines[] | .Config.Uuid' &lt;&lt;&lt; $VM_INFO )
  echo "Patching $VM_NAME with UUID:$VM_UUID"
  kubectl patch node $VM_NAME -p "{\"spec\":{\"providerID\":\"vsphere://$VM_UUID\"}}"
done</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The datacenter where cluster virtual machines are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Prefix for all machines of the cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create /etc/kubernetes/vsphere.config in every master and worker nodes. Refer to <a class="xref" href="deployment-bare-metal.html#vsphere-cloud-provider-configuration" title="3.5.2.3.5. Example vSphere Cloud Provider Configuration">Section 3.5.2.3.5, “Example vSphere Cloud Provider Configuration”</a> for details.</p></li><li class="listitem "><p>On local machine, save kubeadm-config as <code class="literal">kubeadm-config.conf</code>.</p><div class="informalexample"><p>kubectl -n kube-system get cm/kubeadm-config -o yaml &gt; kubeadm-config.conf</p></div></li><li class="listitem "><p>Edit the <code class="literal">kubeadm-config.conf</code> to add cloud-provider and relate configurations.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">data:
  ClusterConfiguration: |
    apiServer:
      extraArgs:
        cloud-config: /etc/kubernetes/vsphere.conf
        cloud-provider: vsphere
      extraVolumes:
      - hostPath: /etc/kubernetes/vsphere.conf
        mountPath: /etc/kubernetes/vsphere.conf
        name: cloud-config
        pathType: FileOrCreate
        readOnly: true
    controllerManager:
      extraArgs:
        cloud-config: /etc/kubernetes/vsphere.conf
        cloud-provider: vsphere
      extraVolumes:
      - hostPath: /etc/kubernetes/vsphere.conf
        mountPath: /etc/kubernetes/vsphere.conf
        name: cloud-config
        pathType: FileOrCreate
        readOnly: true</pre></div></div></li><li class="listitem "><p>Apply the kubeadm-config to the cluster.</p><div class="informalexample"><p>kubectl apply -f kubeadm-config.conf</p></div></li><li class="listitem "><p>On every master node, update kubelet.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop kubelet
source /var/lib/kubelet/kubeadm-flags.env
echo KUBELET_KUBEADM_ARGS='"'--cloud-config=/etc/kubernetes/vsphere.conf --cloud-provider=vsphere $KUBELET_KUBEADM_ARGS'"' &gt; /tmp/kubeadm-flags.env
sudo mv /tmp/kubeadm-flags.env /var/lib/kubelet/kubeadm-flags.env
sudo systemctl start kubelet</pre></div></div></li><li class="listitem "><p>On every master node, update control-plane components.</p><div class="informalexample"><p>sudo kubeadm upgrade node phase control-plane --etcd-upgrade=false</p></div></li><li class="listitem "><p>On every worker node, update kubelet.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop kubelet
source /var/lib/kubelet/kubeadm-flags.env
echo KUBELET_KUBEADM_ARGS='"'--cloud-config=/etc/kubernetes/vsphere.conf --cloud-provider=vsphere $KUBELET_KUBEADM_ARGS'"' &gt; /tmp/kubeadm-flags.env
sudo mv /tmp/kubeadm-flags.env /var/lib/kubelet/kubeadm-flags.env
sudo systemctl start kubelet</pre></div></div></li></ol></div><p>After the setup you can proceed to use <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_storage.html#_vsphere_storage" target="_blank">vSphere Storage</a> in cluster.</p></div><div class="sect4" id="_integrate_external_ldap_tls"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrate External LDAP TLS</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_integrate_external_ldap_tls">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Based on the manifest in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/base/dex.yaml</code>, provide a kustomize patch to <code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/patches/custom.yaml</code> of the form of strategic merge patch or a JSON 6902 patch.</p></li><li class="listitem "><p>Adapt the <code class="literal">ConfigMap</code> by adding LDAP configuration to the connector section of the <code class="literal">custom.yaml</code> file. For detailed configurations for the LDAP connector, refer to <a class="link" href="https://github.com/dexidp/dex/blob/v2.23.0/Documentation/connectors/ldap.md" target="_blank">https://github.com/dexidp/dex/blob/v2.23.0/Documentation/connectors/ldap.md</a>.</p></li></ol></div><p>Read <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge</a> and <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902</a> to get more information.</p><div class="informalexample"><p># Example LDAP connector</p><div class="verbatim-wrap"><pre class="screen">connectors:
- type: ldap
  id: 389ds
  name: 389ds
  config:
    host: ldap.example.org:636 <span id="CO15-1"></span><span class="callout">1</span> <span id="CO15-2"></span><span class="callout">2</span>
    rootCAData: &lt;BASE64_ENCODED_PEM_FILE&gt; <span id="CO15-3"></span><span class="callout">3</span>
    bindDN: cn=user-admin,ou=Users,dc=example,dc=org <span id="CO15-4"></span><span class="callout">4</span>
    bindPW: &lt;BIND_DN_PASSWORD&gt; <span id="CO15-5"></span><span class="callout">5</span>
    usernamePrompt: Email Address <span id="CO15-6"></span><span class="callout">6</span>
    userSearch:
      baseDN: ou=Users,dc=example,dc=org <span id="CO15-7"></span><span class="callout">7</span>
      filter: "(objectClass=person)" <span id="CO15-8"></span><span class="callout">8</span>
      username: mail <span id="CO15-9"></span><span class="callout">9</span>
      idAttr: DN <span id="CO15-10"></span><span class="callout">10</span>
      emailAttr: mail <span id="CO15-11"></span><span class="callout">11</span>
      nameAttr: cn <span id="CO15-12"></span><span class="callout">12</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Host name of LDAP server reachable from the cluster.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The port on which to connect to the host (for example StartTLS: <code class="literal">389</code>, TLS: <code class="literal">636</code>).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>LDAP server base64 encoded root CA certificate file (for example <code class="literal">cat &lt;root-ca-pem-file&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</code>)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Bind DN of user that can do user searches.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Password of the user.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Label of LDAP attribute users will enter to identify themselves (for example <code class="literal">username</code>).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>BaseDN where users are located (for example <code class="literal">ou=Users,dc=example,dc=org</code>).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Filter to specify type of user objects (for example "(objectClass=person)").</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>Attribute users will enter to identify themselves (for example mail).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-10"><span class="callout">10</span></a> </p></td><td valign="top" align="left"><p>Attribute used to identify user within the system (for example DN).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-11"><span class="callout">11</span></a> </p></td><td valign="top" align="left"><p>Attribute containing the user’s email.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-12"><span class="callout">12</span></a> </p></td><td valign="top" align="left"><p>Attribute used as username within OIDC tokens.</p></td></tr></table></div><p>Besides the LDAP connector you can also set up other connectors.
For additional connectors, refer to the available connector configurations
in the Dex repository: <a class="link" href="https://github.com/dexidp/dex/tree/v2.23.0/Documentation/connectors" target="_blank">https://github.com/dexidp/dex/tree/v2.23.0/Documentation/connectors</a>.</p></div><div class="sect4" id="_prevent_nodes_running_special_workloads_from_being_rebooted"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prevent Nodes Running Special Workloads from Being Rebooted</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_prevent_nodes_running_special_workloads_from_being_rebooted">#</a></h5></div></div></div><p>Some nodes might run specially treated workloads (pods).</p><p>To prevent downtime of those workloads and the respective node,
it is possible to flag the pod with <code class="literal">--blocking-pod-selector=&lt;POD_NAME&gt;</code>.
Any node running this workload will not be rebooted via <code class="literal">kured</code> and needs to
be rebooted manually.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Based on the manifest in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/base/kured.yaml</code>, provide a kustomize patch to <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> of the form of strategic merge patch or a JSON 6902 patch.
Read <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge</a> and <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902</a> to get more information.</p></li><li class="listitem "><p>Adapt the <code class="literal">DaemonSet</code> by adding one of the following flags to the <code class="literal">command</code>
section of the <code class="literal">kured</code> container:</p><div class="verbatim-wrap"><pre class="screen">---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --blocking-pod-selector=name=&lt;POD_NAME&gt;</pre></div></li></ol></div><p>You can add any key/value labels to this selector:</p><div class="verbatim-wrap highlight bash"><pre class="screen">--blocking-pod-selector=&lt;LABEL_KEY_1&gt;=&lt;LABEL_VALUE_1&gt;,&lt;LABEL_KEY_2&gt;=&lt;LABEL_VALUE_2&gt;</pre></div><p>Alternatively, you can adapt the <code class="literal">kured</code> DaemonSet also later during runtime (after bootstrap) by editing <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> and executing:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -k &lt;CLUSTER_NAME&gt;/addons/kured/</pre></div><p>This will restart all <code class="literal">kured</code> pods with the additional configuration flags.</p></div></div><div class="sect3" id="_prevent_nodes_with_any_prometheus_alerts_from_being_rebooted"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prevent Nodes with Any Prometheus Alerts from Being Rebooted</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_prevent_nodes_with_any_prometheus_alerts_from_being_rebooted">#</a></h4></div></div></div><div id="id-1.5.8.4.10.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>By default, <span class="strong"><strong>any</strong></span> prometheus alert blocks a node from reboot.
However you can filter specific alerts to be ignored via the <code class="literal">--alert-filter-regexp</code> flag.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Based on the manifest in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/base/kured.yaml</code>, provide a kustomize patch to <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> of the form of strategic merge patch or a JSON 6902 patch.
Read <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge</a> and <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902</a> to get more information.</p></li><li class="listitem "><p>Adapt the <code class="literal">DaemonSet</code> by adding one of the following flags to the <code class="literal">command</code> section of the <code class="literal">kured</code> container:</p><div class="verbatim-wrap"><pre class="screen">---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --prometheus-url=&lt;PROMETHEUS_SERVER_URL&gt;
            - --alert-filter-regexp=^(RebootRequired|AnotherBenignAlert|...$</pre></div></li></ol></div><div id="id-1.5.8.4.10.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The &lt;PROMETHEUS_SERVER_URL&gt; needs to contain the protocol (<code class="literal">http://</code> or <code class="literal">https://</code>)</p></div><p>Alternatively you can adapt the <code class="literal">kured</code> DaemonSet also later during runtime (after bootstrap) by editing <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> and executing:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -k &lt;CLUSTER_NAME&gt;/addons/kured/</pre></div><p>This will restart all <code class="literal">kured</code> pods with the additional configuration flags.</p></div><div class="sect3" id="cluster-bootstrap"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Bootstrap</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#cluster-bootstrap">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Switch to the new directory.</p></li><li class="listitem "><p>Now bootstrap a master node.
For <code class="literal">--target</code> enter the FQDN of your first master node.
Replace <code class="literal">&lt;NODE_NAME&gt;</code> with a unique identifier, for example, "master-one".</p><div id="id-1.5.8.4.11.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Log retention</h6><p>By default skuba will only display the events of the bootstrap process in the terminal during execution.
The examples in the following sections will use the <code class="literal">tee</code> tool to store a copy of the outputs in a file of your choosing.</p><p>For more information on the different logging approaches utilized by SUSE CaaS Platform components please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_logging.html" target="_blank">SUSE CaaS Platform - Admin Guide: Logging</a>.</p></div><div id="id-1.5.8.4.11.2.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Custom Trusted CA Certificate</h6><p>During cluster bootstrap, <code class="literal">skuba</code> automatically generates CA certificates.
You can however also deploy the Kubernetes cluster with your custom trusted CA certificate.</p><p>Please refer to the <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_certificates.html" target="_blank">SUSE CaaS Platform Administration Guide</a> for more information on how to deploy the Kubernetes cluster with a custom trusted CA certificate.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">cd &lt;CLUSTER_NAME&gt;
skuba node bootstrap --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;</pre></div><p>This will bootstrap the specified node as the first master in the cluster.
The process will generate authentication certificates and the <code class="literal">admin.conf</code>
file that is used for authentication against the cluster.
The files will be stored in the <code class="literal">&lt;CLUSTER_NAME&gt;</code> directory specified in step one.</p></li><li class="listitem "><p>Add additional master nodes to the cluster.</p><p>Replace the <code class="literal">&lt;IP/FQDN&gt;</code> with the IP for the machine.
Replace <code class="literal">&lt;NODE_NAME&gt;</code> with a unique identifier, for example, "master-two".</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role master --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;| tee &lt;NODE_NAME&gt;-skuba-node-join.log</pre></div></li><li class="listitem "><p>Add a worker to the cluster:</p><p>Replace the <code class="literal">&lt;IP/FQDN&gt;</code> with the IP for the machine.
Replace <code class="literal">&lt;NODE_NAME&gt;</code> with a unique identifier, for example, "worker-one".</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role worker --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;| tee &lt;NODE_NAME&gt;-skuba-node-join.log</pre></div></li><li class="listitem "><p>Verify that the nodes have been added:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status</pre></div><p>The output should look like this:</p><div class="verbatim-wrap"><pre class="screen">NAME      STATUS    ROLE     OS-IMAGE                              KERNEL-VERSION           KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master0   Ready     master   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
master1   Ready     master   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
master2   Ready     master   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
worker0   Ready     worker   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
worker1   Ready     worker   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
worker2   Ready     worker   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0</pre></div></li></ol></div><div id="id-1.5.8.4.11.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost cannot be used.</p></div></div></div><div class="sect2" id="_using_kubectl"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using kubectl</span> <a title="Permalink" class="permalink" href="deployment-bare-metal.html#_using_kubectl">#</a></h3></div></div></div><p>You can install and use <code class="literal">kubectl</code> by installing the <code class="literal">kubernetes-client</code> package from the SUSE CaaS Platform extension.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo zypper in kubernetes-client</pre></div><div id="id-1.5.8.5.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Alternatively you can install from upstream: <a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/tools/install-kubectl/</a>.</p></div><p>To talk to your cluster, you must be in the <code class="literal">&lt;CLUSTER_NAME&gt;</code> directory when running commands so it can find the <code class="literal">admin.conf</code> file.</p><div id="id-1.5.8.5.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Setting up <code class="literal">kubeconfig</code></h6><p>To make usage of Kubernetes tools easier, you can store a copy of the <code class="literal">admin.conf</code> file as <a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/" target="_blank">kubeconfig</a>.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p ~/.kube
cp admin.conf ~/.kube/config</pre></div><div id="id-1.5.8.5.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The configuration file contains sensitive information and must be handled in a secure fashion. Copying it to a shared user directory might grant access to unwanted users.</p></div><p>You can run commands against your cluster like usual. For example:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">kubectl get nodes -o wide</code></p><p>or</p></li><li class="listitem "><p><code class="literal">kubectl get pods --all-namespaces</code></p><div class="verbatim-wrap highlight bash"><pre class="screen"># kubectl get pods --all-namespaces

NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5zftb                1/1       Running   0          2m
kube-system   coredns-86c58d9df4-fct4m                1/1       Running   0          2m
kube-system   etcd-my-master                          1/1       Running   0          1m
kube-system   kube-apiserver-my-master                1/1       Running   0          1m
kube-system   kube-controller-manager-my-master       1/1       Running   0          1m
kube-system   cilium-operator-7d6ddddbf5-dmbhv        1/1       Running   0          51s
kube-system   cilium-qjt9h                            1/1       Running   0          53s
kube-system   cilium-szkqc                            1/1       Running   0          2m
kube-system   kube-proxy-5qxnt                        1/1       Running   0          2m
kube-system   kube-proxy-746ws                        1/1       Running   0          53s
kube-system   kube-scheduler-my-master                1/1       Running   0          1m
kube-system   kured-ztnfj                             1/1       Running   0          2m
kube-system   kured-zv696                             1/1       Running   0          2m
kube-system   oidc-dex-55fc689dc-b9bxw                1/1       Running   0          2m
kube-system   oidc-gangway-7b7fbbdbdf-ll6l8           1/1       Running   0          2m</pre></div></li></ul></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="_glossary.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 4 </span>Glossary</span></a><a class="nav-link" href="deployment-preparations.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 2 </span>Deployment Preparations</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>