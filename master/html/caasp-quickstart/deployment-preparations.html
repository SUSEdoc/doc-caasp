<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Deployment Preparations | QuickStart Guide | SUSE CaaS Platform 4.5.2</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.2" /><meta name="book-title" content="QuickStart Guide" /><meta name="chapter-title" content="Chapter 2. Deployment Preparations" /><meta name="description" content="In order to deploy SUSE CaaS Platform you need a workstation running SUSE Linux Enterprise Server 15 SP2 or similar openSUSE equivalent. This workstation is called the Management machine . Important files are generated and must be maintained on this machine, but it is not a member of the SUSE CaaS P…" /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="QuickstartGuide" /><link rel="home" href="index.html" title="QuickStart Guide" /><link rel="up" href="index.html" title="QuickStart Guide" /><link rel="prev" href="deployment-system-requirements.html" title="Chapter 1. Requirements" /><link rel="next" href="deployment-bare-metal.html" title="Chapter 3. Deployment on Bare Metal or KVM" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="QuickStart Guide"><span class="book-icon">QuickStart Guide</span></a><span> › </span><a class="crumb" href="deployment-preparations.html">Deployment Preparations</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>QuickStart Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pr01.html"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="deployment-system-requirements.html"><span class="number">1 </span><span class="name">Requirements</span></a></li><li class="inactive"><a href="deployment-preparations.html"><span class="number">2 </span><span class="name">Deployment Preparations</span></a></li><li class="inactive"><a href="deployment-bare-metal.html"><span class="number">3 </span><span class="name">Deployment on Bare Metal or KVM</span></a></li><li class="inactive"><a href="_glossary.html"><span class="number">4 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="_contributors.html"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="_gnu_licenses.html"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 1. Requirements" href="deployment-system-requirements.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 3. Deployment on Bare Metal or KVM" href="deployment-bare-metal.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="QuickStart Guide"><span class="book-icon">QuickStart Guide</span></a><span> › </span><a class="crumb" href="deployment-preparations.html">Deployment Preparations</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 1. Requirements" href="deployment-system-requirements.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 3. Deployment on Bare Metal or KVM" href="deployment-bare-metal.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="deployment-preparations"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.2</span></div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Preparations</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="deployment-preparations.html#ssh-configuration"><span class="number">2.1 </span><span class="name">Basic SSH Key Configuration</span></a></span></dt><dt><span class="section"><a href="deployment-preparations.html#registration-code"><span class="number">2.2 </span><span class="name">Registration Code</span></a></span></dt><dt><span class="section"><a href="deployment-preparations.html#machine-id"><span class="number">2.3 </span><span class="name">Unique Machine IDs</span></a></span></dt><dt><span class="section"><a href="deployment-preparations.html#_installation_tools"><span class="number">2.4 </span><span class="name">Installation Tools</span></a></span></dt><dt><span class="section"><a href="deployment-preparations.html#loadbalancer"><span class="number">2.5 </span><span class="name">Load Balancer</span></a></span></dt></dl></div></div><p>In order to deploy SUSE CaaS Platform you need a workstation running SUSE Linux Enterprise Server 15 SP2 or similar openSUSE equivalent.
This workstation is called the "Management machine". Important files are generated
and must be maintained on this machine, but it is not a member of the SUSE CaaS Platform cluster.</p><div class="sect1" id="ssh-configuration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Basic SSH Key Configuration</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#ssh-configuration">#</a></h2></div></div></div><p><span class="strong"><strong>In order to successfully deploy SUSE CaaS Platform, you need to have SSH keys loaded into an SSH agent.</strong></span> This is important, because it is required in order to use the installation tools <code class="literal">skuba</code> and <code class="literal">terraform</code>.</p><div id="id-1.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The use of <code class="literal">ssh-agent</code> comes with some implications for security that you should take into consideration.</p><p><a class="link" href="http://rabexc.org/posts/pitfalls-of-ssh-agents" target="_blank">The pitfalls of using ssh-agent</a></p><p>To avoid these risks please make sure to either use <code class="literal">ssh-agent -t &lt;TIMEOUT&gt;</code> and specify a time
after which the agent will self-terminate, or terminate the agent yourself before logging out by running <code class="literal">ssh-agent -k</code>.</p></div><p>To log in to the created cluster nodes from the Management machine, you need to configure an SSH key pair.
This key pair needs to be trusted by the user account you will log in with into each cluster node; that user is called "sles" by default.
In order to use the installation tools <code class="literal">terraform</code> and <code class="literal">skuba</code>, this trusted keypair must be loaded into the SSH agent.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>If you do not have an existing ssh keypair to use, run:</p><div class="verbatim-wrap"><pre class="screen">ssh-keygen -t ecdsa</pre></div></li><li class="listitem "><p>The <code class="literal">ssh-agent</code> or a compatible program is sometimes started automatically by graphical desktop
environments. If that is not your situation, run:</p><div class="verbatim-wrap"><pre class="screen">eval "$(ssh-agent)"</pre></div><p>This will start the agent and set environment variables used for agent
communication within the current session. This has to be the same terminal session
that you run the <code class="literal">skuba</code> commands in. A new terminal usually requires a new ssh-agent.
In some desktop environments the ssh-agent will also automatically load the SSH keys.
To add an SSH key manually, use the <code class="literal">ssh-add</code> command:</p><div class="verbatim-wrap"><pre class="screen">ssh-add &lt;PATH_TO_KEY&gt;</pre></div><div id="id-1.4.3.5.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you are adding the SSH key manually, specify the full path.
For example: <code class="literal">/home/sles/.ssh/id_rsa</code></p></div></li></ol></div><p>You can load multiple keys into your agent using the <code class="literal">ssh-add &lt;PATH_TO_KEY&gt;</code> command.
Keys should be password protected as a security measure. The
ssh-add command will prompt for your password, then the agent caches the
decrypted key material for a configurable lifetime. The <code class="literal">-t lifetime</code> option to
ssh-add specifies a maximum time to cache the specific key. See <code class="literal">man ssh-add</code> for
more information.</p><div id="id-1.4.3.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Specify a key expiration time</h6><p>The ssh key is decrypted when loaded into the key agent.  Though the key itself is not
accesible from the agent, anyone with access to the agent’s control socket file can use
the private key contents to impersonate the key owner.  By default, socket access is
limited to the user who launched the agent.  None the less, it is good security practice
to specify an expiration time for the decrypted key using the <code class="literal">-t</code> option.
For example: <code class="literal">ssh-add -t 1h30m $HOME/.ssh/id.ecdsa</code> would expire the decrypted key in
1.5 hours.
.
Alternatively, <code class="literal">ssh-agent</code> can also be launched with <code class="literal">-t</code> to specify a default timeout.
For example: <code class="literal">eval $( ssh-agent -t 120s )</code> would default to a two minute (120 second)
timeout for keys added.  If timeouts are specified for both programs, the timeout from
<code class="literal">ssh-add</code> is used.
See <code class="literal">man ssh-agent</code> and <code class="literal">man ssh-add</code> for more information.</p></div><div id="id-1.4.3.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Usage of multiple identities with <code class="literal">ssh-agent</code></h6><p>Skuba will try all the identities loaded into the <code class="literal">ssh-agent</code> until one of
them grants access to the node, or until the SSH server’s maximum authentication attempts are exhausted.
This could lead to undesired messages in SSH or other security/authentication logs on your local machine.</p></div><div class="sect2" id="_forwarding_the_authentication_agent_connection"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forwarding the Authentication Agent Connection</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#_forwarding_the_authentication_agent_connection">#</a></h3></div></div></div><p>It is also possible to <span class="strong"><strong>forward the authentication agent connection</strong></span> from a
host to another, which can be useful if you intend to run skuba on
a "jump host" and don’t want to copy your private key to this node.
This can be achieved using the <code class="literal">ssh -A</code> command. Please refer to the man page
of <code class="literal">ssh</code> to learn about the security implications of using this feature.</p></div></div><div class="sect1" id="registration-code"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registration Code</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#registration-code">#</a></h2></div></div></div><div id="id-1.4.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The registration code for SUSE CaaS Platform.4 also contains the activation
permissions for the underlying SUSE Linux Enterprise operating system. You can use your SUSE CaaS Platform
registration code to activate the SUSE Linux Enterprise Server 15 SP2 subscription during installation.</p></div><p>You need a subscription registration code to use SUSE CaaS Platform. You can retrieve your
registration code from SUSE Customer Center.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Login to <a class="link" href="https://scc.suse.com" target="_blank">https://scc.suse.com</a></p></li><li class="listitem "><p>Navigate to <span class="guimenu ">MY ORGANIZATIONS → &lt;YOUR ORG&gt;</span></p></li><li class="listitem "><p>Select the <span class="guimenu ">Subscriptions</span> tab from the menu bar at the top</p></li><li class="listitem "><p>Search for "CaaS Platform"</p></li><li class="listitem "><p>Select the version you wish to deploy (should be the highest available version)</p></li><li class="listitem "><p>Click on the Link in the <span class="guimenu ">Name</span> column</p></li><li class="listitem "><p>The registration code should be displayed as the first line under "Subscription Information"</p></li></ul></div><div id="id-1.4.4.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you can not find SUSE CaaS Platform in the list of subscriptions please contact
your local administrator responsible for software subscriptions or SUSE support.</p></div></div><div class="sect1" id="machine-id"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unique Machine IDs</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#machine-id">#</a></h2></div></div></div><p>During deployment of the cluster nodes, each machine will be assigned a unique ID in the <code class="literal">/etc/machine-id</code> file by Terraform or AutoYaST.
If you are using any (semi-)manual methods of deployments that involve cloning of machines and deploying from templates,
you must make sure to delete this file before creating the template.</p><p>If two nodes are deployed with the same <code class="literal">machine-id</code>, they will not be correctly recognized by <code class="literal">skuba</code>.</p><div id="machine-id-regen" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Regenerating Machine ID</h6><p>In case you are not using Terraform or AutoYaST you must regenerate machine IDs manually.</p><p>During the template preparation you will have removed the machine ID from the template image.
This ID is required for proper functionality in the cluster and must be (re-)generated on each machine.</p><p>Log in to each virtual machine created from the template and run:</p><div class="verbatim-wrap"><pre class="screen">rm /etc/machine-id
dbus-uuidgen --ensure
systemd-machine-id-setup
systemctl restart systemd-journald</pre></div><p>This will regenerate the <code class="literal">machine id</code> values for <code class="literal">DBUS</code> (<code class="literal">/var/lib/dbus/machine-id</code>) and <code class="literal">systemd</code> (<code class="literal">/etc/machine-id</code>) and restart the logging service to make use of the new IDs.</p></div></div><div class="sect1" id="_installation_tools"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation Tools</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#_installation_tools">#</a></h2></div></div></div><p>For any deployment type you will need <code class="literal">skuba</code> and <code class="literal">Terraform</code>. These packages are
available from the SUSE CaaS Platform package sources. They are provided as an installation
"pattern" that will install dependencies and other required packages in one simple step.</p><p>Access to the packages requires the <code class="literal">SUSE CaaS Platform</code>, <code class="literal">Containers</code> and <code class="literal">Public Cloud</code> extension modules.
Enable the modules during the operating system installation or activate them using SUSE Connect.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo SUSEConnect -r  &lt;CAASP_REGISTRATION_CODE&gt; <span id="CO3-1"></span><span class="callout">1</span>
sudo SUSEConnect -p sle-module-containers/15.2/x86_64 <span id="CO3-2"></span><span class="callout">2</span>
sudo SUSEConnect -p sle-module-public-cloud/15.2/x86_64 <span id="CO3-3"></span><span class="callout">3</span>
sudo SUSEConnect -p caasp/4.5/x86_64 -r &lt;CAASP_REGISTRATION_CODE&gt; <span id="CO3-4"></span><span class="callout">4</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Activate SUSE Linux Enterprise</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Add the free <code class="literal">Containers</code> module</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Add the free <code class="literal">Public Cloud</code> module</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Add the SUSE CaaS Platform extension with your registration code</p></td></tr></table></div><p>Install the required tools:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern SUSE-CaaSP-Management</pre></div><p>This will install the <code class="literal">skuba</code> command line tool and <code class="literal">Terraform</code>; as well
as various default configurations and examples.</p><div id="id-1.4.6.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Using a Proxy Server</h6><p>Sometimes you need a proxy server to be able to connect to the SUSE Customer Center.
If you have not already configured a system-wide proxy, you can temporarily do
so for the duration of the current shell session like this:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Expose the environmental variable <code class="literal">http_proxy</code>:</p><div class="verbatim-wrap"><pre class="screen">export http_proxy=http://PROXY_IP_FQDN:PROXY_PORT</pre></div></li><li class="listitem "><p>Replace <code class="literal">&lt;PROXY_IP_FQDN&gt;</code> by the IP address or a fully qualified domain name (FQDN) of the
proxy server and <code class="literal">&lt;PROXY_PORT&gt;</code> by its port.</p></li><li class="listitem "><p>If you use a proxy server with basic authentication, create the file <code class="literal">$HOME/.curlrc</code>
with the following content:</p><div class="verbatim-wrap"><pre class="screen">--proxy-user "&lt;USER&gt;:&lt;PASSWORD&gt;"</pre></div><p>Replace <code class="literal">&lt;USER&gt;</code> and <code class="literal">&lt;PASSWORD&gt;</code> with the credentials of an allowed user for the proxy server, and consider limiting access to the file (<code class="literal">chmod 0600</code>).</p></li></ol></div></div></div><div class="sect1" id="loadbalancer"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#loadbalancer">#</a></h2></div></div></div><div id="id-1.4.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Setting up a load balancer is mandatory in any production environment.</p></div><p>SUSE CaaS Platform requires a load balancer to distribute workload between the deployed
master nodes of the cluster. A failure-tolerant SUSE CaaS Platform cluster will always
use more than one control plane node as well as more than one load balancer,
so there isn’t a single point of failure.</p><p>There are many ways to configure a load balancer. This documentation cannot
describe all possible combinations of load balancer configurations and thus
does not aim to do so. Please apply your organization’s load balancing best
practices.</p><p>For SUSE OpenStack Cloud, the Terraform configurations shipped with this version will automatically deploy
a suitable load balancer for the cluster.</p><p>For bare metal, KVM, or VMware, you must configure a load balancer manually and
allow it access to all master nodes created during <a class="xref" href="deployment-bare-metal.html#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a>.</p><p>The load balancer should be configured before the actual deployment. It is needed
during the cluster bootstrap, and also during upgrades. To simplify configuration,
you can reserve the IPs needed for the cluster nodes and pre-configure these in
the load balancer.</p><p>The load balancer needs access to port <code class="literal">6443</code> on the <code class="literal">apiserver</code> (all master nodes)
in the cluster. It also needs access to Gangway port <code class="literal">32001</code> and Dex port <code class="literal">32000</code>
on all master and worker nodes in the cluster for RBAC authentication.</p><p>We recommend performing regular HTTPS health checks on each master node <code class="literal">/healthz</code>
endpoint to verify that the node is responsive. This is particularly important during
upgrades, when a master node restarts the apiserver. During this rather short time
window, all requests have to go to another master node’s apiserver. The master node
that is being upgraded will have to be marked INACTIVE on the load balancer pool
at least during the restart of the apiserver. We provide reasonable defaults
for that on our default openstack load balancer Terraform configuration.</p><p>The following contains examples for possible load balancer configurations based on SUSE Linux Enterprise Server 15 SP2 and <code class="literal">nginx</code> or <code class="literal">HAProxy</code>.</p><div class="sect2" id="loadbalancer-nginx"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nginx TCP Load Balancer with Passive Checks</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#loadbalancer-nginx">#</a></h3></div></div></div><p>For TCP load balancing, we can use the <code class="literal">ngx_stream_module</code> module (available since version 1.9.0). In this mode, <code class="literal">nginx</code> will just forward the TCP packets to the master nodes.</p><p>The default mechanism is <span class="strong"><strong>round-robin</strong></span> so each request will be distributed to a different server.</p><div id="id-1.4.7.11.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The open source version of Nginx referred to in this guide only allows the use of
passive health checks. <code class="literal">nginx</code> will mark a node as unresponsive only after
a failed request. The original request is lost and not forwarded to an available
alternative server.</p><p>This load balancer configuration is therefore only suitable for testing and proof-of-concept (POC) environments.</p><p>For production environments, we recommend the use of <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/" target="_blank">SUSE Linux Enterprise High Availability Extension 15</a></p></div><div class="sect3" id="_configuring_the_load_balancer"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Load Balancer</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#_configuring_the_load_balancer">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Register SLES and enable the "Server Applications" module:</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.2/x86_64</pre></div></li><li class="listitem "><p>Install Nginx:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in nginx</pre></div></li><li class="listitem "><p>Write the configuration in <code class="literal">/etc/nginx/nginx.conf</code>:</p><div class="verbatim-wrap"><pre class="screen">user  nginx;
worker_processes  auto;

load_module /usr/lib64/nginx/modules/ngx_stream_module.so;

error_log  /var/log/nginx/error.log;
error_log  /var/log/nginx/error.log  notice;
error_log  /var/log/nginx/error.log  info;

events {
    worker_connections  1024;
    use epoll;
}

stream {
    log_format proxy '$remote_addr [$time_local] '
                     '$protocol $status $bytes_sent $bytes_received '
                     '$session_time "$upstream_addr"';

    error_log  /var/log/nginx/k8s-masters-lb-error.log;
    access_log /var/log/nginx/k8s-masters-lb-access.log proxy;

    upstream k8s-masters {
        #hash $remote_addr consistent; <span id="CO4-1"></span><span class="callout">1</span>
        server master00:6443 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-2"></span><span class="callout">2</span>
        server master01:6443 weight=1 max_fails=2 fail_timeout=5s;
        server master02:6443 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 6443;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass k8s-masters;
    }

    upstream dex-backends {
        #hash $remote_addr consistent; <span id="CO4-3"></span><span class="callout">3</span>
        server master00:32000 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-4"></span><span class="callout">4</span>
        server master01:32000 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32000 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32000;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass dex-backends; <span id="CO4-5"></span><span class="callout">5</span>
    }

    upstream gangway-backends {
        #hash $remote_addr consistent; <span id="CO4-6"></span><span class="callout">6</span>
        server master00:32001 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-7"></span><span class="callout">7</span>
        server master01:32001 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32001 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32001;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass gangway-backends; <span id="CO4-8"></span><span class="callout">8</span>
    }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-1"><span class="callout">1</span></a> <a href="#CO4-3"><span class="callout">3</span></a> <a href="#CO4-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p><span class="strong"><strong>Note:</strong></span> To enable session persistence, uncomment the <code class="literal">hash</code> option
so the same client will always be redirected to the same server except if this
server is unavailable.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-2"><span class="callout">2</span></a> <a href="#CO4-4"><span class="callout">4</span></a> <a href="#CO4-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Replace the individual <code class="literal">masterXX</code> with the IP/FQDN of your actual master nodes (one entry each) in the <code class="literal">upstream k8s-masters</code> section.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-5"><span class="callout">5</span></a> <a href="#CO4-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Dex port <code class="literal">32000</code> and Gangway port <code class="literal">32001</code> must be accessible through the load balancer for RBAC authentication.</p></td></tr></table></div></li><li class="listitem "><p>Configure <code class="literal">firewalld</code> to open up port <code class="literal">6443</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload</pre></div></li><li class="listitem "><p>Start and enable Nginx. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now nginx</pre></div></li></ol></div></div><div class="sect3" id="_verifying_the_load_balancer"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the Load Balancer</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#_verifying_the_load_balancer">#</a></h4></div></div></div><div id="id-1.4.7.11.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The SUSE CaaS Platform cluster must be up and running for this to produce any useful
results. This step can only be performed after <a class="xref" href="deployment-bare-metal.html#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a> is completed
successfully.</p></div><p>To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.</p><p>From your workstation, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">while true; do skuba cluster status; sleep 1; done;</pre></div><p>There should be no interruption in the  <span class="strong"><strong>skuba cluster status</strong></span> running command.</p><p>On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># tail -f /var/log/nginx/k8s-masters-lb-access.log
10.0.0.47 [17/May/2019:13:49:06 +0000] TCP 200 2553 1613 1.136 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:08 +0000] TCP 200 2553 1613 0.981 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:10 +0000] TCP 200 2553 1613 0.891 "10.0.0.7:6443"
10.0.0.47 [17/May/2019:13:49:12 +0000] TCP 200 2553 1613 0.895 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:15 +0000] TCP 200 2553 1613 1.157 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:17 +0000] TCP 200 2553 1613 0.897 "10.0.0.7:6443"</pre></div></div></div><div class="sect2" id="loadbalancer-haproxy"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HAProxy TCP Load Balancer with Active Checks</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#loadbalancer-haproxy">#</a></h3></div></div></div><div id="id-1.4.7.12.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Package Support</h6><p><code class="literal">HAProxy</code> is available as a supported package with a <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/" target="_blank">SUSE Linux Enterprise High Availability Extension 15</a> subscription.</p><p>Alternatively, you can install <code class="literal">HAProxy</code> from <a class="link" href="https://packagehub.suse.com/packages/haproxy/" target="_blank">SUSE Package Hub</a> but you will not receive product support for this component.</p></div><p><code class="literal">HAProxy</code> is a very powerful load balancer application which is suitable for production environments.
Unlike the open source version of <code class="literal">nginx</code> mentioned in the example above, <code class="literal">HAProxy</code> supports active health checking which is a vital function for reliable cluster health monitoring.</p><p>The version used at this date is the <code class="literal">1.8.7</code>.</p><div id="id-1.4.7.12.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The configuration of an HA cluster is out of the scope of this document.</p></div><p>The default mechanism is <span class="strong"><strong>round-robin</strong></span> so each request will be distributed to a different server.</p><p>The health-checks are executed every two seconds. If a connection fails, the check will be retried two times with a timeout of five seconds for each request.
If no connection succeeds within this interval (2x5s), the node will be marked as <code class="literal">DOWN</code> and no traffic will be sent until the checks succeed again.</p><div class="sect3" id="_configuring_the_load_balancer_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Load Balancer</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#_configuring_the_load_balancer_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Register SLES and enable the "Server Applications" module:</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.2/x86_64</pre></div></li><li class="listitem "><p>Enable the source for the <code class="literal">haproxy</code> package:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>If you are using the SUSE Linux Enterprise High Availability Extension</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect --product sle-ha/15.2/x86_64 -r ADDITIONAL_REGCODE</pre></div></li><li class="listitem "><p>If you want the free (unsupported) package:</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect --product PackageHub/15.2/x86_64</pre></div></li></ul></div></li><li class="listitem "><p>Configure <code class="literal">/dev/log</code> for HAProxy chroot (optional)</p><p>This step is only required when <code class="literal">HAProxy</code> is configured to run in a jail directory (chroot). This is highly recommended since it increases the security of <code class="literal">HAProxy</code>.</p><p>Since <code class="literal">HAProxy</code> is chrooted, it’s necessary to make the log socket available inside the jail directory so <code class="literal">HAProxy</code> can send logs to the socket.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p /var/lib/haproxy/dev/ &amp;&amp; touch /var/lib/haproxy/dev/log</pre></div><p>This systemd service will take care of mounting the socket in the jail directory.</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &gt; /etc/systemd/system/bindmount-dev-log-haproxy-chroot.service &lt;&lt;EOF
[Unit]
Description=Mount /dev/log in HAProxy chroot
After=systemd-journald-dev-log.socket
Before=haproxy.service

[Service]
Type=oneshot
ExecStart=/bin/mount --bind /dev/log /var/lib/haproxy/dev/log

[Install]
WantedBy=multi-user.target
EOF</pre></div><p>Enabling the service will make the changes persistent after a reboot.</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now bindmount-dev-log-haproxy-chroot.service</pre></div></li><li class="listitem "><p>Install HAProxy:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in haproxy</pre></div></li><li class="listitem "><p>Write the configuration in <code class="literal">/etc/haproxy/haproxy.cfg</code>:</p><div id="id-1.4.7.12.8.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Replace the individual <code class="literal">&lt;MASTER_XX_IP_ADDRESS&gt;</code> with the IP of your actual master nodes (one entry each) in the <code class="literal">server</code> lines.
Feel free to leave the name argument in the <code class="literal">server</code> lines (<code class="literal">master00</code> and etc.) as is - it only serves as a label that will show up in the haproxy logs.</p></div><div class="verbatim-wrap"><pre class="screen">global
  log /dev/log local0 info <span id="CO5-1"></span><span class="callout">1</span>
  chroot /var/lib/haproxy <span id="CO5-2"></span><span class="callout">2</span>
  user haproxy
  group haproxy
  daemon

defaults
  mode       tcp
  log        global
  option     tcplog
  option     redispatch
  option     tcpka
  retries    2
  http-check     expect status 200 <span id="CO5-3"></span><span class="callout">3</span>
  default-server check check-ssl verify none
  timeout connect 5s
  timeout client 5s
  timeout server 5s
  timeout tunnel 86400s <span id="CO5-4"></span><span class="callout">4</span>

listen stats <span id="CO5-5"></span><span class="callout">5</span>
  bind    *:9000
  mode    http
  stats   hide-version
  stats   uri       /stats

listen apiserver <span id="CO5-6"></span><span class="callout">6</span>
  bind   *:6443
  option httpchk GET /healthz
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:6443
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:6443
  server master02 &lt;MASTER_02_IP_ADDRESS&gt;:6443

listen dex <span id="CO5-7"></span><span class="callout">7</span>
  bind   *:32000
  option httpchk GET /healthz
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:32000
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:32000
  server masetr02 &lt;MASTER_02_IP_ADDRESS&gt;:32000

listen gangway <span id="CO5-8"></span><span class="callout">8</span>
  bind   *:32001
  option httpchk GET /
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:32001
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:32001
  server master02 &lt;MASTER_02_IP_ADDRESS&gt;:32001</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Forward the logs to systemd journald, the log level can be set to <code class="literal">debug</code> to increase verbosity.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Define if it will run in a <code class="literal">chroot</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>This timeout is set to <code class="literal">24h</code> in order to allow long connections when accessing pod logs or port forwarding.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>URL to expose <code class="literal">HAProxy</code> stats on port <code class="literal">9000</code>, it is accessible at <code class="literal">http://loadbalancer:9000/stats</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The performed health checks will expect a <code class="literal">200</code> return code</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Kubernetes apiserver listening on port <code class="literal">6443</code>, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:6443/healthz</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Dex listening on port <code class="literal">32000</code>, it must be accessible through the load balancer for RBAC authentication, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:32000/healthz</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Gangway listening on port <code class="literal">32001</code>, it must be accessible through the load balancer for RBAC authentication, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:32001/</code></p></td></tr></table></div></li><li class="listitem "><p>Configure <code class="literal">firewalld</code> to open up port <code class="literal">6443</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload</pre></div></li><li class="listitem "><p>Start and enable <code class="literal">HAProxy</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now haproxy</pre></div></li></ol></div></div><div class="sect3" id="_verifying_the_load_balancer_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the Load Balancer</span> <a title="Permalink" class="permalink" href="deployment-preparations.html#_verifying_the_load_balancer_2">#</a></h4></div></div></div><div id="id-1.4.7.12.9.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The SUSE CaaS Platform cluster must be up and running for this to produce any useful
results. This step can only be performed after <a class="xref" href="deployment-bare-metal.html#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a> is completed
successfully.</p></div><p>To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.</p><p>From your workstation, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">while true; do skuba cluster status; sleep 1; done;</pre></div><p>There should be no interruption in the <span class="strong"><strong>skuba cluster status</strong></span> running command.</p><p>On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># journalctl -flu haproxy
haproxy[2525]: 10.0.0.47:59664 [30/Sep/2019:13:33:20.578] apiserver apiserver/master00 1/0/578 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59666 [30/Sep/2019:13:33:22.476] apiserver apiserver/master01 1/0/747 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59668 [30/Sep/2019:13:33:24.522] apiserver apiserver/master02 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59670 [30/Sep/2019:13:33:26.386] apiserver apiserver/master00 1/0/567 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59678 [30/Sep/2019:13:33:28.279] apiserver apiserver/master01 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59682 [30/Sep/2019:13:33:30.174] apiserver apiserver/master02 1/0/571 9727 -- 18/18/17/7/0 0/0</pre></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="deployment-bare-metal.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 3 </span>Deployment on Bare Metal or KVM</span></a><a class="nav-link" href="deployment-system-requirements.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 1 </span>Requirements</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>