<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>QuickStart Guide | SUSE CaaS Platform 4.5.1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.1" /><meta name="book-title" content="QuickStart Guide" /><meta name="description" content="Copyright © 2006 — 2020 SUSE LLC and contributors. All rights reserved." /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="QuickstartGuide" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>QuickStart Guide</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>Show Contents: QuickStart Guide</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#id-1.2"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="#deployment-system-requirements"><span class="number">1 </span><span class="name">Requirements</span></a></li><li class="inactive"><a href="#deployment-preparations"><span class="number">2 </span><span class="name">Deployment Preparations</span></a></li><li class="inactive"><a href="#deployment-bare-metal"><span class="number">3 </span><span class="name">Deployment on Bare Metal or KVM</span></a></li><li class="inactive"><a href="#_glossary"><span class="number">4 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="#_contributors"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="#_gnu_licenses"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="book" id="id-1" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.1</span></h6><div><h1 class="title">QuickStart Guide</h1></div><div><h2 class="subtitle">This guide describes quick deployment for SUSE CaaS Platform 4.5.1.</h2></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname ">Markus</span> <span class="surname ">Napp</span> and <span class="firstname ">Nora</span> <span class="surname ">Kořánová</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>2020-10-12</div></div></div><div class="toc"><dl><dt><span class="preface"><a href="#id-1.2"><span class="name"></span></a></span></dt><dt><span class="chapter"><a href="#deployment-system-requirements"><span class="number">1 </span><span class="name">Requirements</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_platform"><span class="number">1.1 </span><span class="name">Platform</span></a></span></dt><dt><span class="section"><a href="#_nodes"><span class="number">1.2 </span><span class="name">Nodes</span></a></span></dt><dt><span class="section"><a href="#_hardware"><span class="number">1.3 </span><span class="name">Hardware</span></a></span></dt><dt><span class="section"><a href="#sysreq-networking"><span class="number">1.4 </span><span class="name">Networking</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#deployment-preparations"><span class="number">2 </span><span class="name">Deployment Preparations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#ssh-configuration"><span class="number">2.1 </span><span class="name">Basic SSH Key Configuration</span></a></span></dt><dt><span class="section"><a href="#registration-code"><span class="number">2.2 </span><span class="name">Registration Code</span></a></span></dt><dt><span class="section"><a href="#machine-id"><span class="number">2.3 </span><span class="name">Unique Machine IDs</span></a></span></dt><dt><span class="section"><a href="#_installation_tools"><span class="number">2.4 </span><span class="name">Installation Tools</span></a></span></dt><dt><span class="section"><a href="#loadbalancer"><span class="number">2.5 </span><span class="name">Load Balancer</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#deployment-bare-metal"><span class="number">3 </span><span class="name">Deployment on Bare Metal or KVM</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_environment_description"><span class="number">3.1 </span><span class="name">Environment Description</span></a></span></dt><dt><span class="section"><a href="#_autoyast_preparation"><span class="number">3.2 </span><span class="name">AutoYaST Preparation</span></a></span></dt><dt><span class="section"><a href="#_provisioning_the_cluster_nodes"><span class="number">3.3 </span><span class="name">Provisioning the Cluster Nodes</span></a></span></dt><dt><span class="section"><a href="#_container_runtime_proxy"><span class="number">3.4 </span><span class="name">Container Runtime Proxy</span></a></span></dt><dt><span class="section"><a href="#bootstrap"><span class="number">3.5 </span><span class="name">Bootstrapping the Cluster</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_glossary"><span class="number">4 </span><span class="name">Glossary</span></a></span></dt><dt><span class="appendix"><a href="#_contributors"><span class="number">A </span><span class="name">Contributors</span></a></span></dt><dt><span class="appendix"><a href="#_gnu_licenses"><span class="number">B </span><span class="name">GNU Licenses</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_gnu_free_documentation_license"><span class="number">B.1 </span><span class="name">GNU Free Documentation License</span></a></span></dt></dl></dd></dl></div><div class="preface " id="id-1.2"><div class="titlepage"><div><div><h1 class="title"><span class="number"> </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"></span> <a title="Permalink" class="permalink" href="#id-1.2">#</a></h1></div></div></div><div class="line"></div><div id="id-1.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>This document is a work in progress.</p><p>The content in this document is subject to change without notice.</p></div><div id="id-1.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>This guide assumes a configured SUSE Linux Enterprise Server 15 SP2 environment.</p></div><p>Copyright ©
2006 — 2020
SUSE LLC and contributors.
All rights reserved.</p><p>Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the Invariant Section being this copyright notice and license.
A copy of the license version 1.2 is included in the section entitled <span class="quote">“<span class="quote ">GNU Free Documentation License</span>”</span>.</p><p>For SUSE
trademarks, see <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>.
All other third-party trademarks are the property of their respective owners.
Trademark symbols (®, ™, etc.) denote trademarks of SUSE and its affiliates.
Asterisks (*) denote third-party trademarks.</p><p>All information found in this book has been compiled with utmost attention to detail.
However, this does not guarantee complete accuracy.
Neither SUSE LLC, its affiliates, the authors, nor the translators shall be held liable for possible errors or the consequences thereof.</p></div><div class="chapter " id="deployment-system-requirements"><div class="titlepage"><div><div><h1 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="#deployment-system-requirements">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_platform"><span class="number">1.1 </span><span class="name">Platform</span></a></span></dt><dt><span class="section"><a href="#_nodes"><span class="number">1.2 </span><span class="name">Nodes</span></a></span></dt><dt><span class="section"><a href="#_hardware"><span class="number">1.3 </span><span class="name">Hardware</span></a></span></dt><dt><span class="section"><a href="#sysreq-networking"><span class="number">1.4 </span><span class="name">Networking</span></a></span></dt></dl></div></div><div class="sect1" id="_platform"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Platform</span> <a title="Permalink" class="permalink" href="#_platform">#</a></h2></div></div></div><p>Currently we support the following platforms to deploy on:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SUSE OpenStack Cloud 8</p></li><li class="listitem "><p>VMware ESXi 6.7</p></li><li class="listitem "><p>KVM</p></li><li class="listitem "><p>Bare Metal x86_64</p></li><li class="listitem "><p>Amazon Web Services (technological preview)</p></li></ul></div><p>SUSE CaaS Platform itself is based on <span class="strong"><strong>SLE 15 SP2</strong></span>.</p><p>The steps for obtaining the correct installation image for each platform type
are detailed in the respective platform deployment instructions.</p></div><div class="sect1" id="_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nodes</span> <a title="Permalink" class="permalink" href="#_nodes">#</a></h2></div></div></div><p>SUSE CaaS Platform consists of a number of (virtual) machines that run as a cluster.</p><p>You will need at least two machines:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 master node</p></li><li class="listitem "><p>1 worker node</p></li></ul></div><p>SUSE CaaS Platform 4.5.1 supports deployments with a single or multiple master nodes.
Production environments must be deployed with multiple master nodes for resilience.</p><p>All communication to the cluster is done through a load balancer talking to the respective nodes.
For that reason any failure tolerant environment must provide at least two load balancers for incoming communication.</p><p>The minimal viable failure tolerant production environment configuration consists of:</p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Cluster nodes: </span><a title="Permalink" class="permalink" href="#id-1.3.3.8">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>3 master nodes</p></li><li class="listitem "><p>2 worker nodes</p></li></ul></div><div id="id-1.3.3.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Dedicated Cluster Nodes</h6><p>All cluster nodes must be dedicated (virtual) machines reserved for the purpose of running SUSE CaaS Platform.</p></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Additional systems: </span><a title="Permalink" class="permalink" href="#id-1.3.3.10">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Fault tolerant load balancing solution</p><p>(for example SUSE Linux Enterprise High Availability Extension with <code class="literal">pacemaker</code> and <code class="literal">haproxy</code>)</p></li><li class="listitem "><p>1 management workstation</p></li></ul></div></div><div class="sect1" id="_hardware"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware</span> <a title="Permalink" class="permalink" href="#_hardware">#</a></h2></div></div></div><div class="sect2" id="_management_workstation"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Management Workstation</span> <a title="Permalink" class="permalink" href="#_management_workstation">#</a></h3></div></div></div><p>In order to deploy and control a SUSE CaaS Platform cluster you will need at least one
machine capable of running <code class="literal">skuba</code>. This typically is a regular desktop workstation or laptop
running SLE 15 SP2 or later.</p><p>The <code class="literal">skuba</code> CLI package is available from the SUSE CaaS Platform module.
You will need a valid SUSE Linux Enterprise and SUSE CaaS Platform subscription to install this tool on the workstation.</p><div id="id-1.3.4.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Time Synchronization</h6><p>It is vital that the management workstation runs an NTP client and that time synchronization is configured to the same NTP servers, which you will use later to synchronize the cluster nodes.</p></div></div><div class="sect2" id="_storage_sizing"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Sizing</span> <a title="Permalink" class="permalink" href="#_storage_sizing">#</a></h3></div></div></div><p>The storage sizes in the following lists are absolute minimum configurations.</p><p>Sizing of the storage for worker nodes depends largely on the expected amount of container images, their size and change rate.
The basic operating system for all nodes might also include snapshots (when using <code class="literal">btrfs</code>) that can quickly fill up existing space.</p><p>We recommend provisioning a separate storage partition for container images on each (worker) node that can be adjusted in size when needed.
Storage for <code class="literal">/var/lib/containers</code> on the worker nodes should be approximately 50GB in addition to the base OS storage.</p><div class="sect3" id="_master_nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Master Nodes</span> <a title="Permalink" class="permalink" href="#_master_nodes">#</a></h4></div></div></div><p>Up to 5 worker nodes <span class="strong"><strong>(minimum)</strong></span>:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 2</p></li><li class="listitem "><p>RAM: 4 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Up to 10 worker nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 2</p></li><li class="listitem "><p>RAM: 8 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Up to 100 worker nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 4</p></li><li class="listitem "><p>RAM: 16 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Up to 250 worker nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: 50 GB+</p></li><li class="listitem "><p>(v)CPU: 8</p></li><li class="listitem "><p>RAM: 16 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><div id="id-1.3.4.3.5.10" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Using a minimum of 2 (v)CPUs is a hard requirement, deploying a cluster with less processing units is not possible.</p></div></div><div class="sect3" id="_worker_nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Worker nodes</span> <a title="Permalink" class="permalink" href="#_worker_nodes">#</a></h4></div></div></div><div id="id-1.3.4.3.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The worker nodes must have sufficient memory, CPU and disk space for the Pods/containers/applications that are planned to be hosted on these workers.</p></div><p>A worker node requires the following resources:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>CPU cores: 1.250</p></li><li class="listitem "><p>RAM: 1.2 GB</p></li></ul></div><p>Based on these values, the <span class="strong"><strong>minimal</strong></span> configuration of a worker node is:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Storage: Depending on workloads, minimum 20-30 GB to hold the base OS and required packages. Mount additional storage volumes as needed.</p></li><li class="listitem "><p>(v)CPU: 2</p></li><li class="listitem "><p>RAM: 2 GB</p></li><li class="listitem "><p>Network: Minimum 1Gb/s (faster is preferred)</p></li></ul></div><p>Calculate the size of the required (v)CPU by adding up the base requirements, the estimated additional essential cluster components (logging agent, monitoring agent, configuration management, etc.) and the estimated CPU workloads:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1.250 (base requirements) + 0.250 (estimated additional cluster components) + estimated workload CPU requirements</p></li></ul></div><p>Calculate the size of the RAM using a similar formula:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1.2 GB (base requirements) + 500 MB (estimated additional cluster components) + estimated workload RAM requirements</p></li></ul></div><div id="id-1.3.4.3.6.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>These values are provided as a guide to work in most cases. They may vary based on the type of the running workloads.</p></div></div></div><div class="sect2" id="_storage_performance"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Performance</span> <a title="Permalink" class="permalink" href="#_storage_performance">#</a></h3></div></div></div><p>For master nodes you must ensure storage performance of at least 50 to 500 sequential IOPS with disk bandwidth depending on your cluster size. It is highly recommended to use SSD.</p><div class="verbatim-wrap"><pre class="screen">"Typically 50 sequential IOPS (for example, a 7200 RPM disk) is required.
For heavily loaded clusters, 500 sequential IOPS (for example, a typical local SSD
or a high performance virtualized block device) is recommended."</pre></div><div class="verbatim-wrap"><pre class="screen">"Typically 10MB/s will recover 100MB data within 15 seconds.
For large clusters, 100MB/s or higher is suggested for recovering 1GB data
within 15 seconds."</pre></div><p><a class="link" href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks" target="_blank">https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks</a></p><p>This is extremely important to ensure a proper functioning of the critical component <code class="literal">etcd</code>.</p><p>It is possible to preliminary validate these requirements by using <code class="literal">fio</code>. This tool allows us to simulate <code class="literal">etcd</code> I/O (input/output) and to find out from the output statistics whether or not the storage is suitable.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install the tool:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in -y fio</pre></div></li><li class="listitem "><p>Run the testing:</p><div class="verbatim-wrap highlight bash"><pre class="screen">fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-etcd-dir --size=22m --bs=2300 --name=test-etcd-io</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Replace <code class="literal">test-etcd-dir</code> with a directory located on the same disk as the incoming etcd data under <code class="literal">/var/lib/etcd</code></p></li></ul></div></li></ol></div><p>From the outputs, the interesting part is <code class="literal">fsync/fdatasync/sync_file_range</code> where the values are expressed in microseconds (usec). A disk is considered sufficient when the value of the <code class="literal">99.00th</code> percentile is below 10000usec (10ms).</p><p>Be careful though, this benchmark is for etcd only and does not take into consideration external disk usage. This means that a value slightly under 10ms should be taken with precaution as other workloads will have an impact on the disks.</p><div id="id-1.3.4.4.11" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>If the storage is very slow, the values can be expressed directly in milliseconds.</p></div><p>Let’s see two different examples:</p><div class="verbatim-wrap"><pre class="screen">[...]
  fsync/fdatasync/sync_file_range:
    sync (usec): min=251, max=1894, avg=377.78, stdev=69.89
    sync percentiles (usec):
     |  1.00th=[  273],  5.00th=[  285], 10.00th=[  297], 20.00th=[  330],
     | 30.00th=[  343], 40.00th=[  355], 50.00th=[  367], 60.00th=[  379],
     | 70.00th=[  400], 80.00th=[  424], 90.00th=[  465], 95.00th=[  506],
     | 99.00th=[  594], 99.50th=[  635], 99.90th=[  725], 99.95th=[  742], <span id="CO1-1"></span><span class="callout">1</span>
     | 99.99th=[ 1188]
[...]</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Here we get a value of 594usec (0.5ms) so the storage meets the requirements.</p></td></tr></table></div><div class="verbatim-wrap"><pre class="screen">[...]
  fsync/fdatasync/sync_file_range:
    sync (msec): min=10, max=124, avg=17.62, stdev= 3.38
    sync percentiles (usec):
     |  1.00th=[11731],  5.00th=[11994], 10.00th=[12911], 20.00th=[16712],
     | 30.00th=[17695], 40.00th=[17695], 50.00th=[17695], 60.00th=[17957],
     | 70.00th=[17957], 80.00th=[17957], 90.00th=[19530], 95.00th=[22676],
     | 99.00th=[28705], 99.50th=[30016], 99.90th=[41681], 99.95th=[59507], <span id="CO2-1"></span><span class="callout">1</span>
     | 99.99th=[89654]
[...]</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO2-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Here we get a value of 28705usec (28ms) so the storage clearly does not meet the requirements.</p></td></tr></table></div></div></div><div class="sect1" id="sysreq-networking"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking</span> <a title="Permalink" class="permalink" href="#sysreq-networking">#</a></h2></div></div></div><p>The management workstation needs at least the following networking permissions:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SSH access to all machines in the cluster</p></li><li class="listitem "><p>Access to the <code class="literal">apiserver</code> (the load balancer should expose it, port <code class="literal">6443</code>), that will in turn talk to any master in the cluster</p></li><li class="listitem "><p>Access to Dex on the configured <code class="literal">NodePort</code> (the load balancer should expose it, port <code class="literal">32000</code>) so when the OIDC token has expired, <code class="literal">kubectl</code> can request a new token using the refresh token</p></li></ul></div><div id="id-1.3.5.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>It is good security practice not to expose the kubernetes API server on the public internet.
Use network firewalls that only allow access from trusted subnets.</p></div><div class="sect2" id="_sub_network_sizing"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sub-Network Sizing</span> <a title="Permalink" class="permalink" href="#_sub_network_sizing">#</a></h3></div></div></div><div id="id-1.3.5.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The service subnet and pod subnet must not overlap.</p><p>Please plan generously for workload and the expected size of the networks before bootstrapping.</p></div><p>The default pod subnet is <code class="literal">10.244.0.0/16</code>. It allows for 65536 IP addresses overall.
Assignment of CIDR’s is by default <code class="literal">/24</code> (254 usable IP addresses per node).</p><p>The default node allocation of <code class="literal">/24</code> means a hard cluster node limit of 256 since this is the number of <code class="literal">/24</code> ranges that fit in a <code class="literal">/16</code> range.</p><p>Depending on the size of the nodes that you are planning to use (in terms of resources), or on the number of nodes you are planning to have,
the CIDR can be adjusted to be bigger on a per node basis but the cluster would accommodate less nodes overall.</p><p>If you are planning to use more or less pods per node or have a higher number of nodes, you can adjust these settings to match your requirements.
Please make sure that the networks are suitably sized to adjust to future changes in the cluster.</p><p>You can also adjust the service subnet size, this subnet must not overlap with the pod CIDR, and it should be big enough to accommodate all services.</p><p>For more advanced network requirements please refer to: <a class="link" href="https://docs.cilium.io/en/v1.6/concepts/ipam/#address-management" target="_blank">https://docs.cilium.io/en/v1.6/concepts/ipam/#address-management</a></p></div><div class="sect2" id="_ports"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ports</span> <a title="Permalink" class="permalink" href="#_ports">#</a></h3></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="middle">Node</th><th align="left" valign="middle">Port</th><th align="left" valign="middle">Protocol</th><th align="left" valign="middle">Accessibility</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td rowspan="8" align="left" valign="middle"><p>All nodes</p></td><td align="left" valign="middle"><p>22</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="bottom"><p>SSH (required in public clouds)</p></td></tr><tr><td align="left" valign="middle"><p>4240</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Cilium health check</p></td></tr><tr><td align="left" valign="middle"><p>8472</p></td><td align="left" valign="middle"><p>UDP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Cilium VXLAN</p></td></tr><tr><td align="left" valign="middle"><p>10250</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Kubelet (API server → kubelet communication)</p></td></tr><tr><td align="left" valign="middle"><p>10256</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>kube-proxy health check</p></td></tr><tr><td align="left" valign="middle"><p>30000 - 32767</p></td><td align="left" valign="middle"><p>TCP + UDP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>Range of ports used by Kubernetes when allocating services of type <code class="literal">NodePort</code></p></td></tr><tr><td align="left" valign="middle"><p>32000</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>External</p></td><td align="left" valign="middle"><p>Dex (OIDC Connect)</p></td></tr><tr><td align="left" valign="middle"><p>32001</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>External</p></td><td align="left" valign="middle"><p>Gangway (RBAC Authenticate)</p></td></tr><tr><td rowspan="3" align="left" valign="middle"><p>Masters</p></td><td align="left" valign="middle"><p>2379</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="bottom"><p>etcd (client communication)</p></td></tr><tr><td align="left" valign="middle"><p>2380</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal</p></td><td align="left" valign="middle"><p>etcd (server-to-server traffic)</p></td></tr><tr><td align="left" valign="middle"><p>6443</p></td><td align="left" valign="middle"><p>TCP</p></td><td align="left" valign="middle"><p>Internal / External</p></td><td align="left" valign="middle"><p>Kubernetes API server</p></td></tr></tbody></table></div></div><div class="sect2" id="_ip_addresses"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">IP Addresses</span> <a title="Permalink" class="permalink" href="#_ip_addresses">#</a></h3></div></div></div><div id="id-1.3.5.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Using IPv6 addresses is currently not supported.</p></div><p>All nodes must be assigned static IPv4 addresses, which must not be changed manually afterwards.</p><div id="id-1.3.5.7.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Plan carefully for required IP ranges and future scenarios as
it is not possible to reconfigure the IP ranges once the deployment is complete.</p></div></div><div class="sect2" id="_ip_forwarding"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">IP Forwarding</span> <a title="Permalink" class="permalink" href="#_ip_forwarding">#</a></h3></div></div></div><p>The <a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/cluster-administration/networking/" target="_blank">Kubernetes networking model</a> requires that your nodes have IP forwarding enabled in the kernel.
<code class="literal">skuba</code> checks this value when installing your cluster and installs a rule in <code class="literal">/etc/sysctl.d/90-skuba-net-ipv4-ip-forward.conf</code> to make it persistent.</p><p>Other software can potentially install rules with higher priority overriding this value and causing machines to not behave as expected after rebooting.</p><p>You can manually check if this is enabled using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># sysctl net.ipv4.ip_forward

net.ipv4.ip_forward = 1</pre></div><p><code class="literal">net.ipv4.ip_forward</code> must be set to <code class="literal">1</code>. Additionally, you can check in what order persisted rules are processed by running <code class="literal">sysctl --system -a</code>.</p></div><div class="sect2" id="_networking_whitelist"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Whitelist</span> <a title="Permalink" class="permalink" href="#_networking_whitelist">#</a></h3></div></div></div><p>Besides the SUSE provided packages and containers, SUSE CaaS Platform is typically used with third party provided containers and charts.</p><p>The following SUSE provided resources must be available:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">URL</th><th align="left" valign="top">Name</th><th align="left" valign="top">Purpose</th></tr></thead><tbody><tr><td align="left" valign="top"><p>scc.suse.com</p></td><td align="left" valign="top"><p>SUSE Customer Center</p></td><td align="left" valign="top"><p>Allow registration and license activation</p></td></tr><tr><td align="left" valign="top"><p>registry.suse.com</p></td><td align="left" valign="top"><p>SUSE container registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr><tr><td align="left" valign="top"><p>*.cloudfront.net</p></td><td align="left" valign="top"><p>Cloudfront</p></td><td align="left" valign="top"><p>CDN/distribution backend for <code class="literal">registry.suse.com</code></p></td></tr><tr><td align="left" valign="top"><p>kubernetes-charts.suse.com</p></td><td align="left" valign="top"><p>SUSE helm charts repository</p></td><td align="left" valign="top"><p>Provide helm charts</p></td></tr><tr><td align="left" valign="top"><p>updates.suse.com</p></td><td align="left" valign="top"><p>SUSE package update channel</p></td><td align="left" valign="top"><p>Provide package updates</p></td></tr></tbody></table></div><p>If you wish to use Upstream / Third-Party resources, please also allow the following:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">URL</th><th align="left" valign="top">Name</th><th align="left" valign="top">Purpose</th></tr></thead><tbody><tr><td align="left" valign="top"><p>k8s.gcr.io</p></td><td align="left" valign="top"><p>Google Container Registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr><tr><td align="left" valign="top"><p>kubernetes-charts.storage.googleapis.com</p></td><td align="left" valign="top"><p>Google Helm charts repository</p></td><td align="left" valign="top"><p>Provide helm charts</p></td></tr><tr><td align="left" valign="top"><p>docker.io</p></td><td align="left" valign="top"><p>Docker Container Registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr><tr><td align="left" valign="top"><p>quay.io</p></td><td align="left" valign="top"><p>Red Hat Container Registry</p></td><td align="left" valign="top"><p>Provide container images</p></td></tr></tbody></table></div><p>Please note that not all installation scenarios will need all of these resources.</p><div id="id-1.3.5.9.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you are deploying into an air gap scenario, you must ensure that the resources required
from these locations are present and available on your internal mirror server.</p></div></div><div class="sect2" id="_communication"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Communication</span> <a title="Permalink" class="permalink" href="#_communication">#</a></h3></div></div></div><p>Please make sure that all your Kubernetes components can communicate with each other.
This might require the configuration of routing when using multiple network adapters per node.</p><p>Refer to: <a class="link" href="https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters" target="_blank">https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters</a>.</p><p>Configure firewall and other network security to allow communication on the default ports required by Kubernetes: <a class="link" href="https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports" target="_blank">https://v1-18.docs.kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports</a></p></div><div class="sect2" id="_performance"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance</span> <a title="Permalink" class="permalink" href="#_performance">#</a></h3></div></div></div><p>All master nodes of the cluster must have a minimum 1Gb/s network connection to fulfill the requirements for etcd.</p><div class="verbatim-wrap"><pre class="screen">"1GbE is sufficient for common etcd deployments. For large etcd clusters,
a 10GbE network will reduce mean time to recovery."</pre></div><p><a class="link" href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#network" target="_blank">https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#network</a></p></div><div class="sect2" id="_security"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Security</span> <a title="Permalink" class="permalink" href="#_security">#</a></h3></div></div></div><p>Do not grant access to the kubeconfig file or any workstation configured with this configuration to unauthorized personnel.
In the current state, full administrative access is granted to the cluster.</p><p>Authentication is done via the kubeconfig file generated during deployment. This file will grant full access to the cluster and all workloads.
Apply best practices for access control to workstations configured to administer the SUSE CaaS Platform cluster.</p><p>The SUSE CaaS Platform leverages Kubernetes role-based access control (RBAC) for authentication and will need to have an external authentication server such as LDAP, Active Directory, or similar to validate the user’s entity and grant different user roles or cluster role permission.</p></div><div class="sect2" id="_replicas"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replicas</span> <a title="Permalink" class="permalink" href="#_replicas">#</a></h3></div></div></div><p>Some addon services are desired to be highly available. These services require enough cluster nodes available to run replicas of their services.</p><p>When the cluster is deployed with enough nodes for replica sizes, those service distributions will be balanced across the cluster.</p><p>For clusters deployed with a node number lower than the default replica sizes, services will still try to find a suitable node to run on.
However it is likely you will see services all running on the same nodes, defeating the purpose of high availability.</p><p>You can check the deployment replica size after node bootstrap. The number of cluster nodes should be equal or greater than the <code class="literal">DESIRED</code> replica size.</p><div class="verbatim-wrap"><pre class="screen">kubectl get rs -n kube-system</pre></div><p>After deployment, if the number of healthy nodes falls below the number required for fulfilling the replica sizing, service replicas will show in <code class="literal">Pending</code> state until either the unhealthy node recovers or a new node is joined to cluster.</p><p>The following describes two methods for replica management if you wish to work with a cluster below the default replica size requirement.</p><div class="sect3" id="_update_replica_number"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.4.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update replica number</span> <a title="Permalink" class="permalink" href="#_update_replica_number">#</a></h4></div></div></div><p>One method is to update the number of overall replicas being created by a service.
Please consult the documentation of your respective service what the replica limits for proper high availability are.
In case the replica number is too high for the cluster, you must increase the cluster size to provide more resources.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Update deployment replica size before node joining.</p><div id="id-1.3.5.13.9.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You can use the same steps to increase the replica size again if more resources become available later on.</p></div><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system scale --replicas=&lt;DESIRED_REPLICAS&gt; deployment &lt;NAME&gt;</pre></div></li><li class="listitem "><p>Join new nodes.</p></li></ol></div></div><div class="sect3" id="_re_distribute_replicas"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.4.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-distribute replicas</span> <a title="Permalink" class="permalink" href="#_re_distribute_replicas">#</a></h4></div></div></div><p>When multiple replicas are running on the same pod you will want to redistribute those manually to ensure proper high availability,</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Find the pod for re-distribution.
Check the <code class="literal">NAME</code> and <code class="literal">NODE</code> column for duplicated pods.</p><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system get pod -o wide</pre></div></li><li class="listitem "><p>Delete duplicated pod. This will trigger another pod creation.</p><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system delete pod &lt;POD_NAME&gt;</pre></div></li></ol></div></div></div></div></div><div class="chapter " id="deployment-preparations"><div class="titlepage"><div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Preparations</span> <a title="Permalink" class="permalink" href="#deployment-preparations">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#ssh-configuration"><span class="number">2.1 </span><span class="name">Basic SSH Key Configuration</span></a></span></dt><dt><span class="section"><a href="#registration-code"><span class="number">2.2 </span><span class="name">Registration Code</span></a></span></dt><dt><span class="section"><a href="#machine-id"><span class="number">2.3 </span><span class="name">Unique Machine IDs</span></a></span></dt><dt><span class="section"><a href="#_installation_tools"><span class="number">2.4 </span><span class="name">Installation Tools</span></a></span></dt><dt><span class="section"><a href="#loadbalancer"><span class="number">2.5 </span><span class="name">Load Balancer</span></a></span></dt></dl></div></div><p>In order to deploy SUSE CaaS Platform you need a workstation running SUSE Linux Enterprise Server 15 SP2 or similar openSUSE equivalent.
This workstation is called the "Management machine". Important files are generated
and must be maintained on this machine, but it is not a member of the SUSE CaaS Platform cluster.</p><div class="sect1" id="ssh-configuration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Basic SSH Key Configuration</span> <a title="Permalink" class="permalink" href="#ssh-configuration">#</a></h2></div></div></div><p><span class="strong"><strong>In order to successfully deploy SUSE CaaS Platform, you need to have SSH keys loaded into an SSH agent.</strong></span> This is important, because it is required in order to use the installation tools <code class="literal">skuba</code> and <code class="literal">terraform</code>.</p><div id="id-1.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The use of <code class="literal">ssh-agent</code> comes with some implications for security that you should take into consideration.</p><p><a class="link" href="http://rabexc.org/posts/pitfalls-of-ssh-agents" target="_blank">The pitfalls of using ssh-agent</a></p><p>To avoid these risks please make sure to either use <code class="literal">ssh-agent -t &lt;TIMEOUT&gt;</code> and specify a time
after which the agent will self-terminate, or terminate the agent yourself before logging out by running <code class="literal">ssh-agent -k</code>.</p></div><p>To log in to the created cluster nodes from the Management machine, you need to configure an SSH key pair.
This key pair needs to be trusted by the user account you will log in with into each cluster node; that user is called "sles" by default.
In order to use the installation tools <code class="literal">terraform</code> and <code class="literal">skuba</code>, this trusted keypair must be loaded into the SSH agent.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>If you do not have an existing ssh keypair to use, run:</p><div class="verbatim-wrap"><pre class="screen">ssh-keygen -t ecdsa</pre></div></li><li class="listitem "><p>The <code class="literal">ssh-agent</code> or a compatible program is sometimes started automatically by graphical desktop
environments. If that is not your situation, run:</p><div class="verbatim-wrap"><pre class="screen">eval "$(ssh-agent)"</pre></div><p>This will start the agent and set environment variables used for agent
communication within the current session. This has to be the same terminal session
that you run the <code class="literal">skuba</code> commands in. A new terminal usually requires a new ssh-agent.
In some desktop environments the ssh-agent will also automatically load the SSH keys.
To add an SSH key manually, use the <code class="literal">ssh-add</code> command:</p><div class="verbatim-wrap"><pre class="screen">ssh-add &lt;PATH_TO_KEY&gt;</pre></div><div id="id-1.4.3.5.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you are adding the SSH key manually, specify the full path.
For example: <code class="literal">/home/sles/.ssh/id_rsa</code></p></div></li></ol></div><p>You can load multiple keys into your agent using the <code class="literal">ssh-add &lt;PATH_TO_KEY&gt;</code> command.
Keys should be password protected as a security measure. The
ssh-add command will prompt for your password, then the agent caches the
decrypted key material for a configurable lifetime. The <code class="literal">-t lifetime</code> option to
ssh-add specifies a maximum time to cache the specific key. See <code class="literal">man ssh-add</code> for
more information.</p><div id="id-1.4.3.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Specify a key expiration time</h6><p>The ssh key is decrypted when loaded into the key agent.  Though the key itself is not
accesible from the agent, anyone with access to the agent’s control socket file can use
the private key contents to impersonate the key owner.  By default, socket access is
limited to the user who launched the agent.  None the less, it is good security practice
to specify an expiration time for the decrypted key using the <code class="literal">-t</code> option.
For example: <code class="literal">ssh-add -t 1h30m $HOME/.ssh/id.ecdsa</code> would expire the decrypted key in
1.5 hours.
.
Alternatively, <code class="literal">ssh-agent</code> can also be launched with <code class="literal">-t</code> to specify a default timeout.
For example: <code class="literal">eval $( ssh-agent -t 120s )</code> would default to a two minute (120 second)
timeout for keys added.  If timeouts are specified for both programs, the timeout from
<code class="literal">ssh-add</code> is used.
See <code class="literal">man ssh-agent</code> and <code class="literal">man ssh-add</code> for more information.</p></div><div id="id-1.4.3.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Usage of multiple identities with <code class="literal">ssh-agent</code></h6><p>Skuba will try all the identities loaded into the <code class="literal">ssh-agent</code> until one of
them grants access to the node, or until the SSH server’s maximum authentication attempts are exhausted.
This could lead to undesired messages in SSH or other security/authentication logs on your local machine.</p></div><div class="sect2" id="_forwarding_the_authentication_agent_connection"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forwarding the Authentication Agent Connection</span> <a title="Permalink" class="permalink" href="#_forwarding_the_authentication_agent_connection">#</a></h3></div></div></div><p>It is also possible to <span class="strong"><strong>forward the authentication agent connection</strong></span> from a
host to another, which can be useful if you intend to run skuba on
a "jump host" and don’t want to copy your private key to this node.
This can be achieved using the <code class="literal">ssh -A</code> command. Please refer to the man page
of <code class="literal">ssh</code> to learn about the security implications of using this feature.</p></div></div><div class="sect1" id="registration-code"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registration Code</span> <a title="Permalink" class="permalink" href="#registration-code">#</a></h2></div></div></div><div id="id-1.4.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The registration code for SUSE CaaS Platform.4 also contains the activation
permissions for the underlying SUSE Linux Enterprise operating system. You can use your SUSE CaaS Platform
registration code to activate the SUSE Linux Enterprise Server 15 SP2 subscription during installation.</p></div><p>You need a subscription registration code to use SUSE CaaS Platform. You can retrieve your
registration code from SUSE Customer Center.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Login to <a class="link" href="https://scc.suse.com" target="_blank">https://scc.suse.com</a></p></li><li class="listitem "><p>Navigate to <span class="guimenu ">MY ORGANIZATIONS → &lt;YOUR ORG&gt;</span></p></li><li class="listitem "><p>Select the <span class="guimenu ">Subscriptions</span> tab from the menu bar at the top</p></li><li class="listitem "><p>Search for "CaaS Platform"</p></li><li class="listitem "><p>Select the version you wish to deploy (should be the highest available version)</p></li><li class="listitem "><p>Click on the Link in the <span class="guimenu ">Name</span> column</p></li><li class="listitem "><p>The registration code should be displayed as the first line under "Subscription Information"</p></li></ul></div><div id="id-1.4.4.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you can not find SUSE CaaS Platform in the list of subscriptions please contact
your local administrator responsible for software subscriptions or SUSE support.</p></div></div><div class="sect1" id="machine-id"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unique Machine IDs</span> <a title="Permalink" class="permalink" href="#machine-id">#</a></h2></div></div></div><p>During deployment of the cluster nodes, each machine will be assigned a unique ID in the <code class="literal">/etc/machine-id</code> file by Terraform or AutoYaST.
If you are using any (semi-)manual methods of deployments that involve cloning of machines and deploying from templates,
you must make sure to delete this file before creating the template.</p><p>If two nodes are deployed with the same <code class="literal">machine-id</code>, they will not be correctly recognized by <code class="literal">skuba</code>.</p><div id="machine-id-regen" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Regenerating Machine ID</h6><p>In case you are not using Terraform or AutoYaST you must regenerate machine IDs manually.</p><p>During the template preparation you will have removed the machine ID from the template image.
This ID is required for proper functionality in the cluster and must be (re-)generated on each machine.</p><p>Log in to each virtual machine created from the template and run:</p><div class="verbatim-wrap"><pre class="screen">rm /etc/machine-id
dbus-uuidgen --ensure
systemd-machine-id-setup
systemctl restart systemd-journald</pre></div><p>This will regenerate the <code class="literal">machine id</code> values for <code class="literal">DBUS</code> (<code class="literal">/var/lib/dbus/machine-id</code>) and <code class="literal">systemd</code> (<code class="literal">/etc/machine-id</code>) and restart the logging service to make use of the new IDs.</p></div></div><div class="sect1" id="_installation_tools"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation Tools</span> <a title="Permalink" class="permalink" href="#_installation_tools">#</a></h2></div></div></div><p>For any deployment type you will need <code class="literal">skuba</code> and <code class="literal">Terraform</code>. These packages are
available from the SUSE CaaS Platform package sources. They are provided as an installation
"pattern" that will install dependencies and other required packages in one simple step.</p><p>Access to the packages requires the <code class="literal">SUSE CaaS Platform</code>, <code class="literal">Containers</code> and <code class="literal">Public Cloud</code> extension modules.
Enable the modules during the operating system installation or activate them using SUSE Connect.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo SUSEConnect -r  &lt;CAASP_REGISTRATION_CODE&gt; <span id="CO3-1"></span><span class="callout">1</span>
sudo SUSEConnect -p sle-module-containers/15.2/x86_64 <span id="CO3-2"></span><span class="callout">2</span>
sudo SUSEConnect -p sle-module-public-cloud/15.2/x86_64 <span id="CO3-3"></span><span class="callout">3</span>
sudo SUSEConnect -p caasp/4.5/x86_64 -r &lt;CAASP_REGISTRATION_CODE&gt; <span id="CO3-4"></span><span class="callout">4</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Activate SUSE Linux Enterprise</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Add the free <code class="literal">Containers</code> module</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Add the free <code class="literal">Public Cloud</code> module</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Add the SUSE CaaS Platform extension with your registration code</p></td></tr></table></div><p>Install the required tools:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern SUSE-CaaSP-Management</pre></div><p>This will install the <code class="literal">skuba</code> command line tool and <code class="literal">Terraform</code>; as well
as various default configurations and examples.</p><div id="id-1.4.6.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Using a Proxy Server</h6><p>Sometimes you need a proxy server to be able to connect to the SUSE Customer Center.
If you have not already configured a system-wide proxy, you can temporarily do
so for the duration of the current shell session like this:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Expose the environmental variable <code class="literal">http_proxy</code>:</p><div class="verbatim-wrap"><pre class="screen">export http_proxy=http://PROXY_IP_FQDN:PROXY_PORT</pre></div></li><li class="listitem "><p>Replace <code class="literal">&lt;PROXY_IP_FQDN&gt;</code> by the IP address or a fully qualified domain name (FQDN) of the
proxy server and <code class="literal">&lt;PROXY_PORT&gt;</code> by its port.</p></li><li class="listitem "><p>If you use a proxy server with basic authentication, create the file <code class="literal">$HOME/.curlrc</code>
with the following content:</p><div class="verbatim-wrap"><pre class="screen">--proxy-user "&lt;USER&gt;:&lt;PASSWORD&gt;"</pre></div><p>Replace <code class="literal">&lt;USER&gt;</code> and <code class="literal">&lt;PASSWORD&gt;</code> with the credentials of an allowed user for the proxy server, and consider limiting access to the file (<code class="literal">chmod 0600</code>).</p></li></ol></div></div></div><div class="sect1" id="loadbalancer"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer</span> <a title="Permalink" class="permalink" href="#loadbalancer">#</a></h2></div></div></div><div id="id-1.4.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Setting up a load balancer is mandatory in any production environment.</p></div><p>SUSE CaaS Platform requires a load balancer to distribute workload between the deployed
master nodes of the cluster. A failure-tolerant SUSE CaaS Platform cluster will always
use more than one control plane node as well as more than one load balancer,
so there isn’t a single point of failure.</p><p>There are many ways to configure a load balancer. This documentation cannot
describe all possible combinations of load balancer configurations and thus
does not aim to do so. Please apply your organization’s load balancing best
practices.</p><p>For SUSE OpenStack Cloud, the Terraform configurations shipped with this version will automatically deploy
a suitable load balancer for the cluster.</p><p>For bare metal, KVM, or VMware, you must configure a load balancer manually and
allow it access to all master nodes created during <a class="xref" href="#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a>.</p><p>The load balancer should be configured before the actual deployment. It is needed
during the cluster bootstrap, and also during upgrades. To simplify configuration,
you can reserve the IPs needed for the cluster nodes and pre-configure these in
the load balancer.</p><p>The load balancer needs access to port <code class="literal">6443</code> on the <code class="literal">apiserver</code> (all master nodes)
in the cluster. It also needs access to Gangway port <code class="literal">32001</code> and Dex port <code class="literal">32000</code>
on all master and worker nodes in the cluster for RBAC authentication.</p><p>We recommend performing regular HTTPS health checks on each master node <code class="literal">/healthz</code>
endpoint to verify that the node is responsive. This is particularly important during
upgrades, when a master node restarts the apiserver. During this rather short time
window, all requests have to go to another master node’s apiserver. The master node
that is being upgraded will have to be marked INACTIVE on the load balancer pool
at least during the restart of the apiserver. We provide reasonable defaults
for that on our default openstack load balancer Terraform configuration.</p><p>The following contains examples for possible load balancer configurations based on SUSE Linux Enterprise Server 15 SP2 and <code class="literal">nginx</code> or <code class="literal">HAProxy</code>.</p><div class="sect2" id="loadbalancer-nginx"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nginx TCP Load Balancer with Passive Checks</span> <a title="Permalink" class="permalink" href="#loadbalancer-nginx">#</a></h3></div></div></div><p>For TCP load balancing, we can use the <code class="literal">ngx_stream_module</code> module (available since version 1.9.0). In this mode, <code class="literal">nginx</code> will just forward the TCP packets to the master nodes.</p><p>The default mechanism is <span class="strong"><strong>round-robin</strong></span> so each request will be distributed to a different server.</p><div id="id-1.4.7.11.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The open source version of Nginx referred to in this guide only allows the use of
passive health checks. <code class="literal">nginx</code> will mark a node as unresponsive only after
a failed request. The original request is lost and not forwarded to an available
alternative server.</p><p>This load balancer configuration is therefore only suitable for testing and proof-of-concept (POC) environments.</p><p>For production environments, we recommend the use of <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/" target="_blank">SUSE Linux Enterprise High Availability Extension 15</a></p></div><div class="sect3" id="_configuring_the_load_balancer"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Load Balancer</span> <a title="Permalink" class="permalink" href="#_configuring_the_load_balancer">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Register SLES and enable the "Server Applications" module:</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.2/x86_64</pre></div></li><li class="listitem "><p>Install Nginx:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in nginx</pre></div></li><li class="listitem "><p>Write the configuration in <code class="literal">/etc/nginx/nginx.conf</code>:</p><div class="verbatim-wrap"><pre class="screen">user  nginx;
worker_processes  auto;

load_module /usr/lib64/nginx/modules/ngx_stream_module.so;

error_log  /var/log/nginx/error.log;
error_log  /var/log/nginx/error.log  notice;
error_log  /var/log/nginx/error.log  info;

events {
    worker_connections  1024;
    use epoll;
}

stream {
    log_format proxy '$remote_addr [$time_local] '
                     '$protocol $status $bytes_sent $bytes_received '
                     '$session_time "$upstream_addr"';

    error_log  /var/log/nginx/k8s-masters-lb-error.log;
    access_log /var/log/nginx/k8s-masters-lb-access.log proxy;

    upstream k8s-masters {
        #hash $remote_addr consistent; <span id="CO4-1"></span><span class="callout">1</span>
        server master00:6443 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-2"></span><span class="callout">2</span>
        server master01:6443 weight=1 max_fails=2 fail_timeout=5s;
        server master02:6443 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 6443;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass k8s-masters;
    }

    upstream dex-backends {
        #hash $remote_addr consistent; <span id="CO4-3"></span><span class="callout">3</span>
        server master00:32000 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-4"></span><span class="callout">4</span>
        server master01:32000 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32000 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32000;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass dex-backends; <span id="CO4-5"></span><span class="callout">5</span>
    }

    upstream gangway-backends {
        #hash $remote_addr consistent; <span id="CO4-6"></span><span class="callout">6</span>
        server master00:32001 weight=1 max_fails=2 fail_timeout=5s; <span id="CO4-7"></span><span class="callout">7</span>
        server master01:32001 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32001 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32001;
        proxy_connect_timeout 5s;
        proxy_timeout 30s;
        proxy_pass gangway-backends; <span id="CO4-8"></span><span class="callout">8</span>
    }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-1"><span class="callout">1</span></a> <a href="#CO4-3"><span class="callout">3</span></a> <a href="#CO4-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p><span class="strong"><strong>Note:</strong></span> To enable session persistence, uncomment the <code class="literal">hash</code> option
so the same client will always be redirected to the same server except if this
server is unavailable.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-2"><span class="callout">2</span></a> <a href="#CO4-4"><span class="callout">4</span></a> <a href="#CO4-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Replace the individual <code class="literal">masterXX</code> with the IP/FQDN of your actual master nodes (one entry each) in the <code class="literal">upstream k8s-masters</code> section.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-5"><span class="callout">5</span></a> <a href="#CO4-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Dex port <code class="literal">32000</code> and Gangway port <code class="literal">32001</code> must be accessible through the load balancer for RBAC authentication.</p></td></tr></table></div></li><li class="listitem "><p>Configure <code class="literal">firewalld</code> to open up port <code class="literal">6443</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload</pre></div></li><li class="listitem "><p>Start and enable Nginx. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now nginx</pre></div></li></ol></div></div><div class="sect3" id="_verifying_the_load_balancer"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the Load Balancer</span> <a title="Permalink" class="permalink" href="#_verifying_the_load_balancer">#</a></h4></div></div></div><div id="id-1.4.7.11.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The SUSE CaaS Platform cluster must be up and running for this to produce any useful
results. This step can only be performed after <a class="xref" href="#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a> is completed
successfully.</p></div><p>To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.</p><p>From your workstation, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">while true; do skuba cluster status; sleep 1; done;</pre></div><p>There should be no interruption in the  <span class="strong"><strong>skuba cluster status</strong></span> running command.</p><p>On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># tail -f /var/log/nginx/k8s-masters-lb-access.log
10.0.0.47 [17/May/2019:13:49:06 +0000] TCP 200 2553 1613 1.136 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:08 +0000] TCP 200 2553 1613 0.981 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:10 +0000] TCP 200 2553 1613 0.891 "10.0.0.7:6443"
10.0.0.47 [17/May/2019:13:49:12 +0000] TCP 200 2553 1613 0.895 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:15 +0000] TCP 200 2553 1613 1.157 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:17 +0000] TCP 200 2553 1613 0.897 "10.0.0.7:6443"</pre></div></div></div><div class="sect2" id="loadbalancer-haproxy"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HAProxy TCP Load Balancer with Active Checks</span> <a title="Permalink" class="permalink" href="#loadbalancer-haproxy">#</a></h3></div></div></div><div id="id-1.4.7.12.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Package Support</h6><p><code class="literal">HAProxy</code> is available as a supported package with a <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/" target="_blank">SUSE Linux Enterprise High Availability Extension 15</a> subscription.</p><p>Alternatively, you can install <code class="literal">HAProxy</code> from <a class="link" href="https://packagehub.suse.com/packages/haproxy/" target="_blank">SUSE Package Hub</a> but you will not receive product support for this component.</p></div><p><code class="literal">HAProxy</code> is a very powerful load balancer application which is suitable for production environments.
Unlike the open source version of <code class="literal">nginx</code> mentioned in the example above, <code class="literal">HAProxy</code> supports active health checking which is a vital function for reliable cluster health monitoring.</p><p>The version used at this date is the <code class="literal">1.8.7</code>.</p><div id="id-1.4.7.12.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The configuration of an HA cluster is out of the scope of this document.</p></div><p>The default mechanism is <span class="strong"><strong>round-robin</strong></span> so each request will be distributed to a different server.</p><p>The health-checks are executed every two seconds. If a connection fails, the check will be retried two times with a timeout of five seconds for each request.
If no connection succeeds within this interval (2x5s), the node will be marked as <code class="literal">DOWN</code> and no traffic will be sent until the checks succeed again.</p><div class="sect3" id="_configuring_the_load_balancer_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Load Balancer</span> <a title="Permalink" class="permalink" href="#_configuring_the_load_balancer_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Register SLES and enable the "Server Applications" module:</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.2/x86_64</pre></div></li><li class="listitem "><p>Enable the source for the <code class="literal">haproxy</code> package:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>If you are using the SUSE Linux Enterprise High Availability Extension</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect --product sle-ha/15.2/x86_64 -r ADDITIONAL_REGCODE</pre></div></li><li class="listitem "><p>If you want the free (unsupported) package:</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect --product PackageHub/15.2/x86_64</pre></div></li></ul></div></li><li class="listitem "><p>Configure <code class="literal">/dev/log</code> for HAProxy chroot (optional)</p><p>This step is only required when <code class="literal">HAProxy</code> is configured to run in a jail directory (chroot). This is highly recommended since it increases the security of <code class="literal">HAProxy</code>.</p><p>Since <code class="literal">HAProxy</code> is chrooted, it’s necessary to make the log socket available inside the jail directory so <code class="literal">HAProxy</code> can send logs to the socket.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p /var/lib/haproxy/dev/ &amp;&amp; touch /var/lib/haproxy/dev/log</pre></div><p>This systemd service will take care of mounting the socket in the jail directory.</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &gt; /etc/systemd/system/bindmount-dev-log-haproxy-chroot.service &lt;&lt;EOF
[Unit]
Description=Mount /dev/log in HAProxy chroot
After=systemd-journald-dev-log.socket
Before=haproxy.service

[Service]
Type=oneshot
ExecStart=/bin/mount --bind /dev/log /var/lib/haproxy/dev/log

[Install]
WantedBy=multi-user.target
EOF</pre></div><p>Enabling the service will make the changes persistent after a reboot.</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now bindmount-dev-log-haproxy-chroot.service</pre></div></li><li class="listitem "><p>Install HAProxy:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in haproxy</pre></div></li><li class="listitem "><p>Write the configuration in <code class="literal">/etc/haproxy/haproxy.cfg</code>:</p><div id="id-1.4.7.12.8.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Replace the individual <code class="literal">&lt;MASTER_XX_IP_ADDRESS&gt;</code> with the IP of your actual master nodes (one entry each) in the <code class="literal">server</code> lines.
Feel free to leave the name argument in the <code class="literal">server</code> lines (<code class="literal">master00</code> and etc.) as is - it only serves as a label that will show up in the haproxy logs.</p></div><div class="verbatim-wrap"><pre class="screen">global
  log /dev/log local0 info <span id="CO5-1"></span><span class="callout">1</span>
  chroot /var/lib/haproxy <span id="CO5-2"></span><span class="callout">2</span>
  user haproxy
  group haproxy
  daemon

defaults
  mode       tcp
  log        global
  option     tcplog
  option     redispatch
  option     tcpka
  retries    2
  http-check     expect status 200 <span id="CO5-3"></span><span class="callout">3</span>
  default-server check check-ssl verify none
  timeout connect 5s
  timeout client 5s
  timeout server 5s
  timeout tunnel 86400s <span id="CO5-4"></span><span class="callout">4</span>

listen stats <span id="CO5-5"></span><span class="callout">5</span>
  bind    *:9000
  mode    http
  stats   hide-version
  stats   uri       /stats

listen apiserver <span id="CO5-6"></span><span class="callout">6</span>
  bind   *:6443
  option httpchk GET /healthz
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:6443
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:6443
  server master02 &lt;MASTER_02_IP_ADDRESS&gt;:6443

listen dex <span id="CO5-7"></span><span class="callout">7</span>
  bind   *:32000
  option httpchk GET /healthz
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:32000
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:32000
  server masetr02 &lt;MASTER_02_IP_ADDRESS&gt;:32000

listen gangway <span id="CO5-8"></span><span class="callout">8</span>
  bind   *:32001
  option httpchk GET /
  server master00 &lt;MASTER_00_IP_ADDRESS&gt;:32001
  server master01 &lt;MASTER_01_IP_ADDRESS&gt;:32001
  server master02 &lt;MASTER_02_IP_ADDRESS&gt;:32001</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Forward the logs to systemd journald, the log level can be set to <code class="literal">debug</code> to increase verbosity.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Define if it will run in a <code class="literal">chroot</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>This timeout is set to <code class="literal">24h</code> in order to allow long connections when accessing pod logs or port forwarding.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>URL to expose <code class="literal">HAProxy</code> stats on port <code class="literal">9000</code>, it is accessible at <code class="literal">http://loadbalancer:9000/stats</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The performed health checks will expect a <code class="literal">200</code> return code</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Kubernetes apiserver listening on port <code class="literal">6443</code>, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:6443/healthz</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Dex listening on port <code class="literal">32000</code>, it must be accessible through the load balancer for RBAC authentication, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:32000/healthz</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Gangway listening on port <code class="literal">32001</code>, it must be accessible through the load balancer for RBAC authentication, the checks are performed against <code class="literal">https://MASTER_XX_IP_ADDRESS:32001/</code></p></td></tr></table></div></li><li class="listitem "><p>Configure <code class="literal">firewalld</code> to open up port <code class="literal">6443</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload</pre></div></li><li class="listitem "><p>Start and enable <code class="literal">HAProxy</code>. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl enable --now haproxy</pre></div></li></ol></div></div><div class="sect3" id="_verifying_the_load_balancer_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the Load Balancer</span> <a title="Permalink" class="permalink" href="#_verifying_the_load_balancer_2">#</a></h4></div></div></div><div id="id-1.4.7.12.9.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The SUSE CaaS Platform cluster must be up and running for this to produce any useful
results. This step can only be performed after <a class="xref" href="#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a> is completed
successfully.</p></div><p>To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.</p><p>From your workstation, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">while true; do skuba cluster status; sleep 1; done;</pre></div><p>There should be no interruption in the <span class="strong"><strong>skuba cluster status</strong></span> running command.</p><p>On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># journalctl -flu haproxy
haproxy[2525]: 10.0.0.47:59664 [30/Sep/2019:13:33:20.578] apiserver apiserver/master00 1/0/578 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59666 [30/Sep/2019:13:33:22.476] apiserver apiserver/master01 1/0/747 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59668 [30/Sep/2019:13:33:24.522] apiserver apiserver/master02 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59670 [30/Sep/2019:13:33:26.386] apiserver apiserver/master00 1/0/567 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59678 [30/Sep/2019:13:33:28.279] apiserver apiserver/master01 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59682 [30/Sep/2019:13:33:30.174] apiserver apiserver/master02 1/0/571 9727 -- 18/18/17/7/0 0/0</pre></div></div></div></div></div><div class="chapter " id="deployment-bare-metal"><div class="titlepage"><div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment on Bare Metal or KVM</span> <a title="Permalink" class="permalink" href="#deployment-bare-metal">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_environment_description"><span class="number">3.1 </span><span class="name">Environment Description</span></a></span></dt><dt><span class="section"><a href="#_autoyast_preparation"><span class="number">3.2 </span><span class="name">AutoYaST Preparation</span></a></span></dt><dt><span class="section"><a href="#_provisioning_the_cluster_nodes"><span class="number">3.3 </span><span class="name">Provisioning the Cluster Nodes</span></a></span></dt><dt><span class="section"><a href="#_container_runtime_proxy"><span class="number">3.4 </span><span class="name">Container Runtime Proxy</span></a></span></dt><dt><span class="section"><a href="#bootstrap"><span class="number">3.5 </span><span class="name">Bootstrapping the Cluster</span></a></span></dt></dl></div></div><div id="id-1.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Preparation Required</h6><p>You must have completed <a class="xref" href="#deployment-preparations" title="Chapter 2. Deployment Preparations">Chapter 2, <em>Deployment Preparations</em></a> to proceed.</p></div><div id="id-1.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If deploying on KVM virtual machines, you may use a tool such as <code class="literal">virt-manager</code>
to configure the virtual machines and begin the SUSE Linux Enterprise Server 15 SP2 installation.</p></div><div class="sect1" id="_environment_description"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Environment Description</span> <a title="Permalink" class="permalink" href="#_environment_description">#</a></h2></div></div></div><div id="id-1.5.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You must have a load balancer configured as described in <a class="xref" href="#loadbalancer" title="2.5. Load Balancer">Section 2.5, “Load Balancer”</a>.</p></div><div id="id-1.5.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The AutoYaST file found in <code class="literal">skuba</code> is a template. It has the base requirements.
This AutoYaST file should act as a guide and should be updated with your company’s standards.</p></div><div id="id-1.5.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>To account for hardware/platform-specific setup criteria (legacy BIOS vs. (U)EFI, drive partitioning, networking, etc.),
you must adjust the AutoYaST file to your needs according to the requirements.</p><p>Refer to the official AutoYaST documentation for more information: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#book-autoyast" target="_blank">AutoYaST Guide</a>.</p></div><div class="sect2" id="_machine_configuration_prerequisites"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Machine Configuration Prerequisites</span> <a title="Permalink" class="permalink" href="#_machine_configuration_prerequisites">#</a></h3></div></div></div><p>Deployment with AutoYaST will require a minimum <span class="strong"><strong>disk size of 40 GB</strong></span>.
That space is reserved for container images without any workloads (10 GB),
for the root partition (30 GB) and the EFI system partition (200 MB).</p></div></div><div class="sect1" id="_autoyast_preparation"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">AutoYaST Preparation</span> <a title="Permalink" class="permalink" href="#_autoyast_preparation">#</a></h2></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>On the management machine, get an example AutoYaST file from <code class="literal">/usr/share/caasp/autoyast/bare-metal/autoyast.xml</code>,
(which was installed earlier on as part of the management pattern (<code class="literal">sudo zypper in -t pattern SUSE-CaaSP-Management</code>).</p></li><li class="listitem "><p>Copy the file to a suitable location to modify it. Name the file <code class="literal">autoyast.xml</code>.</p></li><li class="listitem "><p>Modify the following places in the AutoYaST file (and any additional places as required by your specific configuration/environment):</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p><code class="literal">&lt;ntp-client&gt;</code></p><p>Change the pre-filled value to your organization’s NTP server. Provide multiple servers if possible by adding new <code class="literal">&lt;ntp_server&gt;</code> subentries.</p></li><li class="listitem "><p><code class="literal">&lt;timezone&gt;</code></p><p>Adjust the timezone your nodes will be set to. Refer to: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#id-1.7.5.13.6" target="_blank">SUSE Linux Enterprise Server AutoYaST Guide: Country Settings</a></p></li><li class="listitem "><p><code class="literal">&lt;username&gt;sles&lt;/username&gt;</code></p><p>Insert your authorized key in the placeholder field.</p></li><li class="listitem "><p><code class="literal">&lt;users&gt;</code></p><p>You can add additional users by creating new blocks in the configuration containing their data.</p><div id="id-1.5.5.2.3.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If the users are configured to not have a password like in the example, ensure the system’s <code class="literal">sudoers</code> file is updated.
Without updating the sudoers file the user will only be able to perform basic operations that will prohibit many administrative tasks.</p><p>The default AutoYaST file provides examples for a disabled <code class="literal">root</code> user and a <code class="literal">sles</code> user with authorized key SSH access.</p><p>The password for root can be enabled by using the <code class="literal">passwd</code> command.</p></div></li><li class="listitem "><p><code class="literal">&lt;suse_register&gt;</code></p><p>Insert the email address and SUSE CaaS Platform registration code in the placeholder fields. This activates SUSE Linux Enterprise Server 15 SP2.</p></li><li class="listitem "><p><code class="literal">&lt;addon&gt;</code></p><p>Insert the SUSE CaaS Platform registration code in the placeholder field. This enables the SUSE CaaS Platform extension module.
Update the AutoYaST file with your registration keys and your company’s best practices and hardware configurations.</p><div id="id-1.5.5.2.3.2.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Your SUSE CaaS Platform registration key can be used to both activate SUSE Linux Enterprise Server 15 SP2 and enable the extension.</p></div></li></ol></div><p>Refer to the official AutoYaST documentation for more information: <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#book-autoyast" target="_blank">AutoYaST Guide</a>.</p></li><li class="listitem "><p>Host the AutoYaST files on a Web server reachable inside the network you are installing the cluster in.</p></li></ol></div><div class="sect2" id="_deploying_with_local_repository_mirroring_tool_rmt_server"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying with local Repository Mirroring Tool (RMT) server</span> <a title="Permalink" class="permalink" href="#_deploying_with_local_repository_mirroring_tool_rmt_server">#</a></h3></div></div></div><p>In order to use a local Repository Mirroring Tool (RMT) server for deployment of packages, you need to specify
the server configuration in your AutoYaST file. To do so add the following section:</p><div class="verbatim-wrap highlight xml"><pre class="screen">&lt;suse_register&gt;
&lt;do_registration config:type="boolean"&gt;true&lt;/do_registration&gt;
&lt;install_updates config:type="boolean"&gt;true&lt;/install_updates&gt;

&lt;reg_server&gt;https://rmt.example.org&lt;/reg_server&gt; <span id="CO6-1"></span><span class="callout">1</span>
&lt;reg_server_cert&gt;https://rmt.example.org/rmt.crt&lt;/reg_server_cert&gt; <span id="CO6-2"></span><span class="callout">2</span>
&lt;reg_server_cert_fingerprint_type&gt;SHA1&lt;/reg_server_cert_fingerprint_type&gt;
&lt;reg_server_cert_fingerprint&gt;0C:A4:A1:06:AD:E2:A2:AA:D0:08:28:95:05:91:4C:07:AD:13:78:FE&lt;/reg_server_cert_fingerprint&gt; <span id="CO6-3"></span><span class="callout">3</span>
&lt;slp_discovery config:type="boolean"&gt;false&lt;/slp_discovery&gt;
&lt;addons config:type="list"&gt;
  &lt;addon&gt;
    &lt;name&gt;sle-module-containers&lt;/name&gt;
    &lt;version&gt;15.2&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
  &lt;addon&gt;
    &lt;name&gt;sle-module-public-cloud&lt;/name&gt;
    &lt;version&gt;15.2&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
  &lt;addon&gt;
    &lt;name&gt;caasp&lt;/name&gt;
    &lt;version&gt;4.5&lt;/version&gt;
    &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;/addon&gt;
&lt;/addons&gt;
&lt;/suse_register&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Provide FQDN of the Repository Mirroring Tool (RMT) server</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Provide the location on the server where the certificate can be found</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Provide the certificate fingerprint for the Repository Mirroring Tool (RMT) server</p></td></tr></table></div></div></div><div class="sect1" id="_provisioning_the_cluster_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning the Cluster Nodes</span> <a title="Permalink" class="permalink" href="#_provisioning_the_cluster_nodes">#</a></h2></div></div></div><p>Once the AutoYaST file is available in the network that the machines will be configured in, you can start deploying machines.</p><p>The default production scenario consists of 6 nodes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 load balancer</p></li><li class="listitem "><p>3 masters</p></li><li class="listitem "><p>2 workers</p></li></ul></div><p>Depending on the type of load balancer you wish to use, you need to deploy at least 5 machines to serve as cluster nodes and provide a load balancer from the environment.</p><p>The load balancer must point at the machines that are assigned to be used as <code class="literal">master</code> nodes in the future cluster.</p><div id="id-1.5.6.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If you do not wish to use infrastructure load balancers, please deploy additional machines and refer to <a class="xref" href="#loadbalancer" title="2.5. Load Balancer">Section 2.5, “Load Balancer”</a>.</p></div><p>Install SUSE Linux Enterprise Server 15 SP2 from your preferred medium and follow the steps for <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#invoking-autoinst" target="_blank">Invoking the Auto-Installation Process</a></p><p>Provide <code class="literal">autoyast=https://[webserver/path/to/autoyast.xml]</code> during the SUSE Linux Enterprise Server 15 SP2 installation.</p><div class="sect2" id="_suse_linux_enterprise_server_installation"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Linux Enterprise Server Installation</span> <a title="Permalink" class="permalink" href="#_suse_linux_enterprise_server_installation">#</a></h3></div></div></div><div id="id-1.5.6.10.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Use AutoYaST and make sure to use a staged frozen patchlevel via RMT/SUSE Manager to ensure a 100% reproducible setup.
<a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-rmt/#cha-rmt-client" target="_blank">RMT Guide</a></p></div><p>Once the machines have been installed using the AutoYaST file, you are now ready proceed with <a class="xref" href="#bootstrap" title="3.5. Bootstrapping the Cluster">Section 3.5, “Bootstrapping the Cluster”</a>.</p></div></div><div class="sect1" id="_container_runtime_proxy"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Container Runtime Proxy</span> <a title="Permalink" class="permalink" href="#_container_runtime_proxy">#</a></h2></div></div></div><div id="id-1.5.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>CRI-O proxy settings must be adjusted on all nodes before joining the cluster!</p><p>Please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o</a></p><p>In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">SUSE CaaS Platform Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O</a></p></div></div><div class="sect1" id="bootstrap"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bootstrapping the Cluster</span> <a title="Permalink" class="permalink" href="#bootstrap">#</a></h2></div></div></div><p>Bootstrapping the cluster is the initial process of starting up the cluster
and defining which of the nodes are masters and which are workers. For maximum automation of this process,
SUSE CaaS Platform uses the <code class="literal">skuba</code> package.</p><div class="sect2" id="_preparation"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation</span> <a title="Permalink" class="permalink" href="#_preparation">#</a></h3></div></div></div><div class="sect3" id="_install_skuba"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install <code class="literal">skuba</code></span> <a title="Permalink" class="permalink" href="#_install_skuba">#</a></h4></div></div></div><p>First you need to install <code class="literal">skuba</code> on a management machine, like your local workstation:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Add the SLE15 SP2 extension containing <code class="literal">skuba</code>. This also requires the "containers" and the "public cloud" module.</p><div class="verbatim-wrap highlight bash"><pre class="screen">SUSEConnect -p sle-module-containers/15.2/x86_64
SUSEConnect -p sle-module-public-cloud/15.2/x86_64
SUSEConnect -p caasp/4.5/x86_64 -r &lt;PRODUCT_KEY&gt;</pre></div></li><li class="listitem "><p>Install the management pattern with:</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in -t pattern SUSE-CaaSP-Management</pre></div></li></ol></div><div id="id-1.5.8.3.2.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Example deployment configuration files for each deployment scenario are installed
under <code class="literal">/usr/share/caasp/terraform/</code>, or in case of the bare metal deployment:
<code class="literal">/usr/share/caasp/autoyast/</code>.</p></div></div><div class="sect3" id="_container_runtime_proxy_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Container Runtime Proxy</span> <a title="Permalink" class="permalink" href="#_container_runtime_proxy_2">#</a></h4></div></div></div><div id="id-1.5.8.3.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>CRI-O proxy settings must be adjusted manually on all nodes before joining the cluster!</p></div><p>In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin_miscellaneous.html#_configuring_httphttps_proxy_for_cri_o" target="_blank">SUSE CaaS Platform Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O</a></p></div></div><div class="sect2" id="_cluster_deployment"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Deployment</span> <a title="Permalink" class="permalink" href="#_cluster_deployment">#</a></h3></div></div></div><p>Make sure you have added the SSH identity (corresponding to the public SSH key distributed above)
to the ssh-agent on your workstation. For instructions on how to add the SSH identity,
refer to <a class="xref" href="#ssh-configuration" title="2.1. Basic SSH Key Configuration">Section 2.1, “Basic SSH Key Configuration”</a>.</p><p>This is a requirement for <code class="literal">skuba</code> (<a class="link" href="https://github.com/SUSE/skuba#prerequisites" target="_blank">https://github.com/SUSE/skuba#prerequisites</a>).</p><p>By default <code class="literal">skuba</code> connects to the nodes as <code class="literal">root</code> user. A different user can
be specified by the following flags:</p><div class="verbatim-wrap highlight bash"><pre class="screen">--sudo --user &lt;USERNAME&gt;</pre></div><div id="id-1.5.8.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>You must configure <code class="literal">sudo</code> for the user to be able to authenticate without password.
Replace <code class="literal">&lt;USERNAME&gt;</code> with the user you created during installation. As root, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">echo "&lt;USERNAME&gt; ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoers</pre></div></div><div class="sect3" id="_initializing_the_cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initializing the Cluster</span> <a title="Permalink" class="permalink" href="#_initializing_the_cluster">#</a></h4></div></div></div><div id="id-1.5.8.4.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Secure configuration files access</h6><p>The directory created during this step contains configuration files
that allow full administrator access to your cluster.
Apply best practices for access control to this folder.</p></div><p>Now you can initialize the cluster on the deployed machines.
As <code class="literal">--control-plane</code> enter the IP/FQDN of your load balancer.
If you do not use a load balancer use your first master node.</p><div id="id-1.5.8.4.7.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If you are deploying on a cloud provider you must enable vendor specific integrations (CPI).
Please refer further below to <a class="xref" href="#enabling-cpi" title="3.5.2.3.1. Enabling Cloud Provider Integration">Section 3.5.2.3.1, “Enabling Cloud Provider Integration”</a>.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB_IP/FQDN&gt; &lt;CLUSTER_NAME&gt;</pre></div><p><code class="literal">cluster init</code> generates the folder named <code class="literal">&lt;CLUSTER_NAME&gt;</code> and initializes the directory that will hold the configuration (<code class="literal">kubeconfig</code>) for the cluster.</p><div id="id-1.5.8.4.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost cannot be used.</p></div><div class="sect4" id="_transitioning_from_docker_to_cri_o"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Transitioning from Docker to CRI-O</span> <a title="Permalink" class="permalink" href="#_transitioning_from_docker_to_cri_o">#</a></h5></div></div></div><p>SUSE CaaS Platform 4.5.1 <span class="strong"><strong>default configuration</strong></span> uses the CRI-O Container Engine in conjunction with Docker Linux capabilities.
This means SUSE CaaS Platform 4.5.1 containers run on top of CRI-O with the following additional
Linux capabilities: <code class="literal">audit_write</code>, <code class="literal">setfcap</code> and <code class="literal">mknod</code>.
This measure ensures a transparent transition and seamless compatibility with workloads running
on the previous SUSE CaaS Platform versions and out-of-the-box Docker compatibility.</p><p>In case you wish to use <span class="strong"><strong>unmodified CRI-O</strong></span>,
use the <code class="literal">--strict-capability-defaults</code> option during the initial setup when you run <code class="literal">skuba cluster init</code>,
which will create the vanilla CRI-O configuration:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --strict-capability-defaults</pre></div><p>Please be aware that this might result in
incompatibility with your previously running workloads,
unless you explicitly define the additional Linux capabilities required
on top of CRI-O defaults.</p><div id="id-1.5.8.4.7.8.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>After the bootstrap of the Kubernetes cluster there will be no easy
way to revert this modification. Please choose wisely.</p></div></div></div><div class="sect3" id="_configuring_kubernetes_services"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Kubernetes Services</span> <a title="Permalink" class="permalink" href="#_configuring_kubernetes_services">#</a></h4></div></div></div><p>Inspect the <code class="literal">kubeadm-init.conf</code> file inside your cluster definition and set extra configuration settings supported by <code class="literal">kubeadm</code>.
The latest supported version is v1beta1.
Later, when you later run <code class="literal">skuba node bootstrap</code>, <code class="literal">kubeadm</code> will read <code class="literal">kubeadm-init.conf</code>
and will forcefully set certain settings to the ones required by SUSE CaaS Platform.</p><div class="sect4" id="_network_settings"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Settings</span> <a title="Permalink" class="permalink" href="#_network_settings">#</a></h5></div></div></div><p>The default network settings inside <code class="literal">kubeadm-init.conf</code> are viable for production clusters and adjusting them is optional.
If you however wish to change the pod and service subnets, it is important that you do so before the bootstrap.
The subnet ranges must be planned carefully,
because the settings cannot be adjusted after deployment is complete.
The default settings are the following:</p><div class="verbatim-wrap"><pre class="screen">networking:
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12</pre></div><p>The <code class="literal">podSubnet</code> IP range must be big enough to contain all IP addresses for all PODs planned for the cluster.
The subnet also mustn’t conflict with services from outside of the cluster - external databases, file services, etc.
This also holds for <code class="literal">serviceSubnet</code> - the IP range must not conflict with external services and needs to be broad enough for all services planned for the cluster.</p></div></div><div class="sect3" id="_cluster_configuration"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Configuration</span> <a title="Permalink" class="permalink" href="#_cluster_configuration">#</a></h4></div></div></div><p>Before bootstrapping the cluster, it is advisable to perform some additional configuration.</p><div class="sect4" id="enabling-cpi"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Cloud Provider Integration</span> <a title="Permalink" class="permalink" href="#enabling-cpi">#</a></h5></div></div></div><p>Enable cloud provider integration to take advantage of the underlying cloud platforms
and automatically manage resources like the Load Balancer, Nodes (Instances), Network Routes
and Storage services.</p><p>If you want to enable cloud provider integration with different cloud platforms,
initialize the cluster with the flag <code class="literal">--cloud-provider &lt;CLOUD PROVIDER&gt;</code>.
The only currently available options are <code class="literal">openstack</code>, <code class="literal">aws</code> and <code class="literal">vsphere</code>,
but more options are planned.</p><div id="id-1.5.8.4.9.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Cleanup</h6><p>By enabling CPI providers your Kubernetes cluster will be able to
provision cloud resources on its own (eg: Load Balancers, Persistent Volumes).
You will have to manually clean these resources before you destroy the cluster
with Terraform.</p><p>Not removing resources like Load Balancers created by the CPI will result in
Terraform timing out during <code class="literal">destroy</code> operations.</p><p>Persistent volumes created with the <code class="literal">retain</code> policy will exist inside of
the external cloud infrastructure even after the cluster is removed.</p></div><div class="sect5" id="_openstack_cpi"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.5.2.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack CPI</span> <a title="Permalink" class="permalink" href="#_openstack_cpi">#</a></h6></div></div></div><p>Define the cluster using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB_IP/FQDN&gt; --cloud-provider openstack &lt;CLUSTER_NAME&gt;</pre></div><p>Running the above command will create a directory <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/openstack</code> with a
<code class="literal">README.md</code> and an <code class="literal">openstack.conf.template</code> in it. Copy <code class="literal">openstack.conf.template</code>
or create an <code class="literal">openstack.conf</code> file inside <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/openstack</code>,
according to the supported format.
The supported format and content can be found in the official Kubernetes documentation:</p><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack</a></p><div id="id-1.5.8.4.9.3.5.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The file <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/openstack/openstack.conf</code> must not be freely accessible.
Please remember to set proper file permissions for it, for example <code class="literal">600</code>.</p></div></div></div><div class="sect4" id="_example_openstack_cloud_provider_configuration"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example OpenStack Cloud Provider Configuration</span> <a title="Permalink" class="permalink" href="#_example_openstack_cloud_provider_configuration">#</a></h5></div></div></div><p>You can find the required parameters in OpenStack RC File v3.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">[Global]
auth-url=&lt;OS_AUTH_URL&gt; <span id="CO7-1"></span><span class="callout">1</span>
username=&lt;OS_USERNAME&gt; <span id="CO7-2"></span><span class="callout">2</span>
password=&lt;OS_PASSWORD&gt; <span id="CO7-3"></span><span class="callout">3</span>
tenant-id=&lt;OS_PROJECT_ID&gt; <span id="CO7-4"></span><span class="callout">4</span>
domain-name=&lt;OS_USER_DOMAIN_NAME&gt; <span id="CO7-5"></span><span class="callout">5</span>
region=&lt;OS_REGION_NAME&gt; <span id="CO7-6"></span><span class="callout">6</span>
ca-file="/etc/pki/trust/anchors/SUSE_Trust_Root.pem" <span id="CO7-7"></span><span class="callout">7</span>
[LoadBalancer]
lb-version=v2 <span id="CO7-8"></span><span class="callout">8</span>
subnet-id=&lt;PRIVATE_SUBNET_ID&gt; <span id="CO7-9"></span><span class="callout">9</span>
floating-network-id=&lt;PUBLIC_NET_ID&gt; <span id="CO7-10"></span><span class="callout">10</span>
create-monitor=yes <span id="CO7-11"></span><span class="callout">11</span>
monitor-delay=1m <span id="CO7-12"></span><span class="callout">12</span>
monitor-timeout=30s <span id="CO7-13"></span><span class="callout">13</span>
monitor-max-retries=3 <span id="CO7-14"></span><span class="callout">14</span>
[BlockStorage]
bs-version=v2 <span id="CO7-15"></span><span class="callout">15</span>
ignore-volume-az=true <span id="CO7-16"></span><span class="callout">16</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>(required) Specifies the URL of the Keystone API used to authenticate the user.
This value can be found in Horizon (the OpenStack control panel).
under Project &gt; Access and Security &gt; API Access &gt; Credentials.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the username of a valid user set in Keystone.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the password of a valid user set in Keystone.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>(required) Used to specify the ID of the project where you want to create your resources.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the name of the domain your user belongs to.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the identifier of the region to use when running on
a multi-region OpenStack cloud. A region is a general division of an OpenStack deployment.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the path to your custom CA file.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to override automatic version detection.
Valid values are <code class="literal">v1</code> or <code class="literal">v2</code>. Where no value is provided, automatic detection
will select the highest supported version exposed by the underlying OpenStack cloud.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to specify the ID of the subnet you want to create your load balancer on.
Can be found at Network &gt; Networks. Click on the respective network to get its subnets.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-10"><span class="callout">10</span></a> </p></td><td valign="top" align="left"><p>(optional) If specified, will create a floating IP for the load balancer.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-11"><span class="callout">11</span></a> </p></td><td valign="top" align="left"><p>(optional) Indicates whether or not to create a health monitor for the Neutron load balancer.
Valid values are true and false. The default is false.
When true is specified then monitor-delay, monitor-timeout, and monitor-max-retries must also be set.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-12"><span class="callout">12</span></a> </p></td><td valign="top" align="left"><p>(optional) The time between sending probes to members of the load balancer.
Ensure that you specify a valid time unit.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-13"><span class="callout">13</span></a> </p></td><td valign="top" align="left"><p>(optional) Maximum time for a monitor to wait for a ping reply before it times out.
The value must be less than the delay value. Ensure that you specify a valid time unit.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-14"><span class="callout">14</span></a> </p></td><td valign="top" align="left"><p>(optional) Number of permissible ping failures before changing the load balancer
member’s status to INACTIVE. Must be a number between 1 and 10.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-15"><span class="callout">15</span></a> </p></td><td valign="top" align="left"><p>(optional) Used to override automatic version detection.
Valid values are v1, v2, v3 and auto. When auto is specified, automatic detection
will select the highest supported version exposed by the underlying OpenStack cloud.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-16"><span class="callout">16</span></a> </p></td><td valign="top" align="left"><p>(optional) Influences availability zone, use when attaching Cinder volumes.
When Nova and Cinder have different availability zones, this should be set to <code class="literal">true</code>.</p></td></tr></table></div><p>After setting options in the <code class="literal">openstack.conf</code> file, please proceed with <a class="xref" href="#cluster-bootstrap" title="3.5.2.5. Cluster Bootstrap">Section 3.5.2.5, “Cluster Bootstrap”</a>.</p><div id="id-1.5.8.4.9.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>When cloud provider integration is enabled, it’s very important to bootstrap and join nodes with the same node names that they have inside <code class="literal">Openstack</code>, as
these names will be used by the <code class="literal">Openstack</code> cloud controller manager to reconcile node metadata.</p></div><div class="sect5" id="_amazon_web_services_aws_cpi"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.5.2.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Amazon Web Services (AWS) CPI</span> <a title="Permalink" class="permalink" href="#_amazon_web_services_aws_cpi">#</a></h6></div></div></div><p>Define the cluster using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB IP/FQDN&gt; --cloud-provider aws &lt;CLUSTER_NAME&gt;</pre></div><p>Running the above command will create a directory <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/aws</code> with a
<code class="literal">README.md</code> file in it. No further configuration files are needed.</p><p>The supported format and content can be found in the
<a class="link" href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws" target="_blank">official Kubernetes documentation</a>.</p><div id="id-1.5.8.4.9.4.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>When cloud provider integration is enabled, it’s very important to bootstrap and join nodes with the same node names that they have inside <code class="literal">AWS</code>, as
these names will be used by the <code class="literal">AWS</code> cloud controller manager to reconcile node metadata.</p><p>You can use the "private dns" values provided by the Terraform output.</p></div></div><div class="sect5" id="cluster-bootstrap-vcp"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.5.2.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">vSphere CPI (VCP)</span> <a title="Permalink" class="permalink" href="#cluster-bootstrap-vcp">#</a></h6></div></div></div><p>Define the cluster using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster init --control-plane &lt;LB_IP/FQDN&gt; --cloud-provider vsphere &lt;CLUSTER_NAME&gt;</pre></div><p>Running the above command will create a directory <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/vsphere</code> with a
<code class="literal">README.md</code> and a <code class="literal">vsphere.conf.template</code> in it. Copy <code class="literal">vsphere.conf.template</code>
or create a <code class="literal">vsphere.conf</code> file inside <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/vsphere</code>, according to the supported format.</p><p>The supported format and content can be found in the <a class="link" href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#vsphere" target="_blank">official Kubernetes documentation</a>.</p><div id="id-1.5.8.4.9.4.8.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The file <code class="literal">&lt;CLUSTER_NAME&gt;/cloud/vsphere/vsphere.conf</code> must not be freely accessible.
Please remember to set proper file permissions for it, for example <code class="literal">600</code>.</p></div></div></div><div class="sect4" id="vsphere-cloud-provider-configuration"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example vSphere Cloud Provider Configuration</span> <a title="Permalink" class="permalink" href="#vsphere-cloud-provider-configuration">#</a></h5></div></div></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">[Global]
user = "&lt;VC_ADMIN_USERNAME&gt;" <span id="CO8-1"></span><span class="callout">1</span>
password = "&lt;VC_ADMIN_PASSWORD&gt;" <span id="CO8-2"></span><span class="callout">2</span>
port = "443" <span id="CO8-3"></span><span class="callout">3</span>
insecure-flag = "1" <span id="CO8-4"></span><span class="callout">4</span>
[VirtualCenter "&lt;VC_IP_OR_FQDN&gt;"] <span id="CO8-5"></span><span class="callout">5</span>
datacenters = "&lt;VC_DATACENTERS&gt;" <span id="CO8-6"></span><span class="callout">6</span>
[Workspace]
server = "&lt;VC_IP_OR_FQDN&gt;" <span id="CO8-7"></span><span class="callout">7</span>
datacenter = "&lt;VC_DATACENTER&gt;" <span id="CO8-8"></span><span class="callout">8</span>
default-datastore = "&lt;VC_DATASTORE&gt;" <span id="CO8-9"></span><span class="callout">9</span>
resourcepool-path = "&lt;VC_RESOURCEPOOL_PATH&gt;" <span id="CO8-10"></span><span class="callout">10</span>
folder = "&lt;VC_VM_FOLDER&gt;" <span id="CO8-11"></span><span class="callout">11</span>
[Disk]
scsicontrollertype = pvscsi <span id="CO8-12"></span><span class="callout">12</span>
[Network]
public-network = "VM Network" <span id="CO8-13"></span><span class="callout">13</span>
[Labels] <span id="CO8-14"></span><span class="callout">14</span>
region = "&lt;VC_DATACENTER_TAG&gt;" <span id="CO8-15"></span><span class="callout">15</span>
zone = "&lt;VC_CLUSTER_TAG&gt;" <span id="CO8-16"></span><span class="callout">16</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the vCenter username for vSphere cloud provider to authenticate with.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>(required) Refers to the vCenter password for vCenter user specified with <code class="literal">user</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>(optional) The vCenter Server Port. The default is 443 if not specified.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>(optional) Set to 1 if vCenter used a self-signed certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>(required) The IP address of the vCenter server.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>(required) The datacenter name in vCenter where Kubernetes nodes reside.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>(required) The IP address of the vCenter server for storage provisioning. Usually the same as <code class="literal">VirtualCenter</code></p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>(required) The datacenter to provision temporary VMs for volume provisioning.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>(required) The default datastore to provision temporary VMs for volume provisioning.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-10"><span class="callout">10</span></a> </p></td><td valign="top" align="left"><p>(required) The resource pool to provision temporary VMs for volume provisioning.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-11"><span class="callout">11</span></a> </p></td><td valign="top" align="left"><p>(required) The vCenter VM folder where Kubernetes nodes are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-12"><span class="callout">12</span></a> </p></td><td valign="top" align="left"><p>(required) Defines the SCSI controller in use on the VMs. Almost always set to <code class="literal">pvscsi</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-13"><span class="callout">13</span></a> </p></td><td valign="top" align="left"><p>(optional) The network in vCenter where Kubernetes nodes should join. The default is "VM Network" if not specified.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-14"><span class="callout">14</span></a> </p></td><td valign="top" align="left"><p>(optional) The feature flag for zone and region support.</p><div id="id-1.5.8.4.9.5.3.14.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The zone and region tags must exist and assigned to datacenter and cluster before bootstrap.
Instruction to tag zones and regions, refer to: <a class="link" href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/zones.html#tag-zones-and-regions-in-vcenter" target="_blank">https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/zones.html#tag-zones-and-regions-in-vcenter</a>.</p></div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-15"><span class="callout">15</span></a> </p></td><td valign="top" align="left"><p>(optional) The category name of the tag assigned to the vCenter datacenter.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-16"><span class="callout">16</span></a> </p></td><td valign="top" align="left"><p>(optional) The category name of the tag assigned to the vCenter cluster.</p></td></tr></table></div><p>After setting options in the <code class="literal">vsphere.conf</code> file, please proceed with <a class="xref" href="#cluster-bootstrap" title="3.5.2.5. Cluster Bootstrap">Section 3.5.2.5, “Cluster Bootstrap”</a>.</p><div id="id-1.5.8.4.9.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Set <code class="literal">vSphere</code> virtual machine hostnames</h6><p>When cloud provider integration is enabled, it’s very important to bootstrap and join nodes with the node names same as <code class="literal">vSphere</code> virtual machine’s hostnames.
These names will be used by the <code class="literal">vSphere</code> cloud controller manager to reconcile node metadata.</p></div><div id="id-1.5.8.4.9.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Enable <code class="literal">disk.EnableUUID</code>.</h6><p>Each virtual machine requires to have <code class="literal">disk.EnableUUID</code> enabled to successfully mount the virtual disks.</p><p>Clusters provisioned following <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-deployment/_deployment_instructions.html#_using_terraform" target="_blank">Deploying VMs from the Template</a> with <code class="literal">cpi_enable = true</code> automatically enables <code class="literal">disk.EnableUUID</code>.</p><p>For clusters provisioned by any other method, ensure virtual machines are set to use <code class="literal">disk.EnableUUID</code>.</p><p>For more information, refer to: <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.7/Cloud-Native-Storage/GUID-3501C3F2-7D7C-45E9-B20A-F3F70D1E4679.html" target="_blank">Configure Kubernetes Cluster Virtual Machines</a> .</p></div><div id="id-1.5.8.4.9.5.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Create a Folder For Your Virtual Machines.</h6><p>All virtual machines must exist in a folder and provide the name of that folder as the <code class="literal">folder</code> variable in the <code class="literal">vsphere.conf</code> before bootstrap.</p><p>Clusters provisioned following <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-deployment/_deployment_instructions.html#_using_terraform" target="_blank">Deploying VMs from the Template</a> with <code class="literal">cpi_enable = true</code> automatically create and place all cluster node virtual machines inside a <code class="literal">*-cluster</code> folder.</p><p>For clusters provisioned by any other method, make sure to create and move all cluster node virtual machines to a folder.</p></div></div><div class="sect4" id="_enable_vsphere_cloud_provider"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable vSphere Cloud Provider</span> <a title="Permalink" class="permalink" href="#_enable_vsphere_cloud_provider">#</a></h5></div></div></div><p>For an existing cluster without cloud provider enabled at bootstrap, you can enable it later.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>In vCenter, create a folder and move all cluster virtual machines into the folder.
You can use <code class="literal">govc</code> to automate the task.</p><p>For installation instructions, refer to: <a class="link" href="https://github.com/vmware/govmomi/tree/master/govc" target="_blank">https://github.com/vmware/govmomi/tree/master/govc</a>.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">DATACENTER="&lt;VC_DATACENTER&gt;" <span id="CO9-1"></span><span class="callout">1</span>
CLUSTER_PREFIX="&lt;VC_CLUSTER_PREFIX&gt;" <span id="CO9-2"></span><span class="callout">2</span>
govc folder.create /$DATACENTER/vm/$CLUSTER_PREFIX-cluster
govc object.mv /$DATACENTER/vm/$CLUSTER_PREFIX-\* /$DATACENTER/vm/$CLUSTER_PREFIX-cluster</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The datacenter where cluster virtual machines are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Prefix for all machines of the cluster.</p></td></tr></table></div></li><li class="listitem "><p>In vCenter, enable <code class="literal">disk.UUID</code> for all cluster virtual machines.
You can use <code class="literal">govc</code> to automate the task.</p><div id="id-1.5.8.4.9.6.3.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Setup <code class="literal">disk.enabledUUID</code> requires virtual machine to be powered off. The following script
will setup all virtul machine in parallel, hense resulting some cluster downtimes while
all machines are powered off. Modify the script or simply DO NOT use the script if minimal
downtime is in consideration.</p></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">DATACENTER="PROVO" <span id="CO10-1"></span><span class="callout">1</span>
VMS=("caasp-master-0" "caasp-master-1" "caasp-master-2" "caasp-worker-0" "caasp-worker-1") <span id="CO10-2"></span><span class="callout">2</span></pre></div><div class="verbatim-wrap"><pre class="screen">function setup {
  NAME=$1
  echo "[$NAME]"
  govc vm.power -dc=$DATACENTER -off $NAME
  govc vm.change -dc=$DATACENTER -vm=$NAME -e="disk.enableUUID=1" &amp;&amp;\
    echo "Configured disk.enabledUUID: 1"
  govc vm.power -dc=$DATACENTER -on $NAME
}</pre></div><div class="verbatim-wrap"><pre class="screen">for vm in ${VMS[@]}
do
  setup $vm &amp;
done
wait</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The datacenter where cluster virtual machines are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The cluster virtual machine names.</p></td></tr></table></div></li><li class="listitem "><p>Update the provider ID for all Kuberentes nodes.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">DATACENTER="&lt;VC_DATACENTER&gt;" <span id="CO11-1"></span><span class="callout">1</span>
CLUSTER_PREFIX="&lt;VC_CLUSTER_PREFIX&gt;" <span id="CO11-2"></span><span class="callout">2</span>
for vm in $(govc ls "/$DATACENTER/vm/$CLUSTER_PREFIX-cluster")
do
  VM_INFO=$(govc vm.info -json -dc=$DATACENTER -vm.ipath="/$vm" -e=true)
  VM_NAME=$(jq -r ' .VirtualMachines[] | .Name' &lt;&lt;&lt; $VM_INFO)
  [[ $VM_NAME == *"-lb-"* ]] &amp;&amp; continue
  VM_UUID=$( jq -r ' .VirtualMachines[] | .Config.Uuid' &lt;&lt;&lt; $VM_INFO )
  echo "Patching $VM_NAME with UUID:$VM_UUID"
  kubectl patch node $VM_NAME -p "{\"spec\":{\"providerID\":\"vsphere://$VM_UUID\"}}"
done</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The datacenter where cluster virtual machines are in.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Prefix for all machines of the cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create /etc/kubernetes/vsphere.config in every master and worker nodes. Refer to <a class="xref" href="#vsphere-cloud-provider-configuration" title="3.5.2.3.3. Example vSphere Cloud Provider Configuration">Section 3.5.2.3.3, “Example vSphere Cloud Provider Configuration”</a> for details.</p></li><li class="listitem "><p>On local machine, save kubeadm-config as <code class="literal">kubeadm-config.conf</code>.</p><div class="informalexample"><p>kubectl -n kube-system get cm/kubeadm-config -o yaml &gt; kubeadm-config.conf</p></div></li><li class="listitem "><p>Edit the <code class="literal">kubeadm-config.conf</code> to add cloud-provider and relate configurations.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">data:
  ClusterConfiguration: |
    apiServer:
      extraArgs:
        cloud-config: /etc/kubernetes/vsphere.conf
        cloud-provider: vsphere
      extraVolumes:
      - hostPath: /etc/kubernetes/vsphere.conf
        mountPath: /etc/kubernetes/vsphere.conf
        name: cloud-config
        pathType: FileOrCreate
        readOnly: true
    controllerManager:
      extraArgs:
        cloud-config: /etc/kubernetes/vsphere.conf
        cloud-provider: vsphere
      extraVolumes:
      - hostPath: /etc/kubernetes/vsphere.conf
        mountPath: /etc/kubernetes/vsphere.conf
        name: cloud-config
        pathType: FileOrCreate
        readOnly: true</pre></div></div></li><li class="listitem "><p>Apply the kubeadm-config to the cluster.</p><div class="informalexample"><p>kubectl apply -f kubeadm-config.conf</p></div></li><li class="listitem "><p>On every master node, update kubelet.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop kubelet
source /var/lib/kubelet/kubeadm-flags.env
echo KUBELET_KUBEADM_ARGS='"'--cloud-config=/etc/kubernetes/vsphere.conf --cloud-provider=vsphere $KUBELET_KUBEADM_ARGS'"' &gt; /tmp/kubeadm-flags.env
sudo mv /tmp/kubeadm-flags.env /var/lib/kubelet/kubeadm-flags.env
sudo systemctl start kubelet</pre></div></div></li><li class="listitem "><p>On every master node, update control-plane components.</p><div class="informalexample"><p>sudo kubeadm upgrade node phase control-plane --etcd-upgrade=false</p></div></li><li class="listitem "><p>On every worker node, update kubelet.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop kubelet
source /var/lib/kubelet/kubeadm-flags.env
echo KUBELET_KUBEADM_ARGS='"'--cloud-config=/etc/kubernetes/vsphere.conf --cloud-provider=vsphere $KUBELET_KUBEADM_ARGS'"' &gt; /tmp/kubeadm-flags.env
sudo mv /tmp/kubeadm-flags.env /var/lib/kubelet/kubeadm-flags.env
sudo systemctl start kubelet</pre></div></div></li></ol></div><p>After the setup you can proceed to use <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_storage.html#_vsphere_storage" target="_blank">vSphere Storage</a> in cluster.</p></div><div class="sect4" id="_integrate_external_ldap_tls"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrate External LDAP TLS</span> <a title="Permalink" class="permalink" href="#_integrate_external_ldap_tls">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Based on the manifest in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/base/dex.yaml</code>, provide a kustomize patch to <code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/patches/custom.yaml</code> of the form of strategic merge patch or a JSON 6902 patch.</p></li><li class="listitem "><p>Adapt the <code class="literal">ConfigMap</code> by adding LDAP configuration to the connector section of the <code class="literal">custom.yaml</code> file. For detailed configurations for the LDAP connector, refer to <a class="link" href="https://github.com/dexidp/dex/blob/v2.23.0/Documentation/connectors/ldap.md" target="_blank">https://github.com/dexidp/dex/blob/v2.23.0/Documentation/connectors/ldap.md</a>.</p></li></ol></div><p>Read <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge</a> and <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902</a> to get more information.</p><div class="informalexample"><p># Example LDAP connector</p><div class="verbatim-wrap"><pre class="screen">connectors:
- type: ldap
  id: 389ds
  name: 389ds
  config:
    host: ldap.example.org:636 <span id="CO12-1"></span><span class="callout">1</span> <span id="CO12-2"></span><span class="callout">2</span>
    rootCAData: &lt;BASE64_ENCODED_PEM_FILE&gt; <span id="CO12-3"></span><span class="callout">3</span>
    bindDN: cn=user-admin,ou=Users,dc=example,dc=org <span id="CO12-4"></span><span class="callout">4</span>
    bindPW: &lt;BIND_DN_PASSWORD&gt; <span id="CO12-5"></span><span class="callout">5</span>
    usernamePrompt: Email Address <span id="CO12-6"></span><span class="callout">6</span>
    userSearch:
      baseDN: ou=Users,dc=example,dc=org <span id="CO12-7"></span><span class="callout">7</span>
      filter: "(objectClass=person)" <span id="CO12-8"></span><span class="callout">8</span>
      username: mail <span id="CO12-9"></span><span class="callout">9</span>
      idAttr: DN <span id="CO12-10"></span><span class="callout">10</span>
      emailAttr: mail <span id="CO12-11"></span><span class="callout">11</span>
      nameAttr: cn <span id="CO12-12"></span><span class="callout">12</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Host name of LDAP server reachable from the cluster.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The port on which to connect to the host (for example StartTLS: <code class="literal">389</code>, TLS: <code class="literal">636</code>).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>LDAP server base64 encoded root CA certificate file (for example <code class="literal">cat &lt;root-ca-pem-file&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</code>)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Bind DN of user that can do user searches.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Password of the user.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Label of LDAP attribute users will enter to identify themselves (for example <code class="literal">username</code>).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>BaseDN where users are located (for example <code class="literal">ou=Users,dc=example,dc=org</code>).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Filter to specify type of user objects (for example "(objectClass=person)").</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>Attribute users will enter to identify themselves (for example mail).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-10"><span class="callout">10</span></a> </p></td><td valign="top" align="left"><p>Attribute used to identify user within the system (for example DN).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-11"><span class="callout">11</span></a> </p></td><td valign="top" align="left"><p>Attribute containing the user’s email.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-12"><span class="callout">12</span></a> </p></td><td valign="top" align="left"><p>Attribute used as username within OIDC tokens.</p></td></tr></table></div><p>Besides the LDAP connector you can also set up other connectors.
For additional connectors, refer to the available connector configurations
in the Dex repository: <a class="link" href="https://github.com/dexidp/dex/tree/v2.23.0/Documentation/connectors" target="_blank">https://github.com/dexidp/dex/tree/v2.23.0/Documentation/connectors</a>.</p></div><div class="sect4" id="_prevent_nodes_running_special_workloads_from_being_rebooted"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.5.2.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prevent Nodes Running Special Workloads from Being Rebooted</span> <a title="Permalink" class="permalink" href="#_prevent_nodes_running_special_workloads_from_being_rebooted">#</a></h5></div></div></div><p>Some nodes might run specially treated workloads (pods).</p><p>To prevent downtime of those workloads and the respective node,
it is possible to flag the pod with <code class="literal">--blocking-pod-selector=&lt;POD_NAME&gt;</code>.
Any node running this workload will not be rebooted via <code class="literal">kured</code> and needs to
be rebooted manually.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Based on the manifest in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/base/kured.yaml</code>, provide a kustomize patch to <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> of the form of strategic merge patch or a JSON 6902 patch.
Read <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge</a> and <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902</a> to get more information.</p></li><li class="listitem "><p>Adapt the <code class="literal">DaemonSet</code> by adding one of the following flags to the <code class="literal">command</code>
section of the <code class="literal">kured</code> container:</p><div class="verbatim-wrap"><pre class="screen">---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --blocking-pod-selector=name=&lt;POD_NAME&gt;</pre></div></li></ol></div><p>You can add any key/value labels to this selector:</p><div class="verbatim-wrap highlight bash"><pre class="screen">--blocking-pod-selector=&lt;LABEL_KEY_1&gt;=&lt;LABEL_VALUE_1&gt;,&lt;LABEL_KEY_2&gt;=&lt;LABEL_VALUE_2&gt;</pre></div><p>Alternatively, you can adapt the <code class="literal">kured</code> DaemonSet also later during runtime (after bootstrap) by editing <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> and executing:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -k &lt;CLUSTER_NAME&gt;/addons/kured/</pre></div><p>This will restart all <code class="literal">kured</code> pods with the additional configuration flags.</p></div></div><div class="sect3" id="_prevent_nodes_with_any_prometheus_alerts_from_being_rebooted"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prevent Nodes with Any Prometheus Alerts from Being Rebooted</span> <a title="Permalink" class="permalink" href="#_prevent_nodes_with_any_prometheus_alerts_from_being_rebooted">#</a></h4></div></div></div><div id="id-1.5.8.4.10.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>By default, <span class="strong"><strong>any</strong></span> prometheus alert blocks a node from reboot.
However you can filter specific alerts to be ignored via the <code class="literal">--alert-filter-regexp</code> flag.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Based on the manifest in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/base/kured.yaml</code>, provide a kustomize patch to <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> of the form of strategic merge patch or a JSON 6902 patch.
Read <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge</a> and <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902</a> to get more information.</p></li><li class="listitem "><p>Adapt the <code class="literal">DaemonSet</code> by adding one of the following flags to the <code class="literal">command</code> section of the <code class="literal">kured</code> container:</p><div class="verbatim-wrap"><pre class="screen">---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --prometheus-url=&lt;PROMETHEUS_SERVER_URL&gt;
            - --alert-filter-regexp=^(RebootRequired|AnotherBenignAlert|...$</pre></div></li></ol></div><div id="id-1.5.8.4.10.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The &lt;PROMETHEUS_SERVER_URL&gt; needs to contain the protocol (<code class="literal">http://</code> or <code class="literal">https://</code>)</p></div><p>Alternatively you can adapt the <code class="literal">kured</code> DaemonSet also later during runtime (after bootstrap) by editing <code class="literal">&lt;CLUSTER_NAME&gt;/addons/kured/patches/custom.yaml</code> and executing:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -k &lt;CLUSTER_NAME&gt;/addons/kured/</pre></div><p>This will restart all <code class="literal">kured</code> pods with the additional configuration flags.</p></div><div class="sect3" id="cluster-bootstrap"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.5.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Bootstrap</span> <a title="Permalink" class="permalink" href="#cluster-bootstrap">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Switch to the new directory.</p></li><li class="listitem "><p>Now bootstrap a master node.
For <code class="literal">--target</code> enter the FQDN of your first master node.
Replace <code class="literal">&lt;NODE_NAME&gt;</code> with a unique identifier, for example, "master-one".</p><div id="id-1.5.8.4.11.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Log retention</h6><p>By default skuba will only display the events of the bootstrap process in the terminal during execution.
The examples in the following sections will use the <code class="literal">tee</code> tool to store a copy of the outputs in a file of your choosing.</p><p>For more information on the different logging approaches utilized by SUSE CaaS Platform components please refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_logging.html" target="_blank">SUSE CaaS Platform - Admin Guide: Logging</a>.</p></div><div id="id-1.5.8.4.11.2.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Custom Trusted CA Certificate</h6><p>During cluster bootstrap, <code class="literal">skuba</code> automatically generates CA certificates.
You can however also deploy the Kubernetes cluster with your custom trusted CA certificate.</p><p>Please refer to the <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_certificates.html" target="_blank">SUSE CaaS Platform Administration Guide</a> for more information on how to deploy the Kubernetes cluster with a custom trusted CA certificate.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">cd &lt;CLUSTER_NAME&gt;
skuba node bootstrap --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;</pre></div><p>This will bootstrap the specified node as the first master in the cluster.
The process will generate authentication certificates and the <code class="literal">admin.conf</code>
file that is used for authentication against the cluster.
The files will be stored in the <code class="literal">&lt;CLUSTER_NAME&gt;</code> directory specified in step one.</p></li><li class="listitem "><p>Add additional master nodes to the cluster.</p><p>Replace the <code class="literal">&lt;IP/FQDN&gt;</code> with the IP for the machine.
Replace <code class="literal">&lt;NODE_NAME&gt;</code> with a unique identifier, for example, "master-two".</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role master --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;| tee &lt;NODE_NAME&gt;-skuba-node-join.log</pre></div></li><li class="listitem "><p>Add a worker to the cluster:</p><p>Replace the <code class="literal">&lt;IP/FQDN&gt;</code> with the IP for the machine.
Replace <code class="literal">&lt;NODE_NAME&gt;</code> with a unique identifier, for example, "worker-one".</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role worker --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;| tee &lt;NODE_NAME&gt;-skuba-node-join.log</pre></div></li><li class="listitem "><p>Verify that the nodes have been added:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status</pre></div><p>The output should look like this:</p><div class="verbatim-wrap"><pre class="screen">NAME      STATUS    ROLE     OS-IMAGE                              KERNEL-VERSION           KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master0   Ready     master   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
master1   Ready     master   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
master2   Ready     master   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
worker0   Ready     worker   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
worker1   Ready     worker   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0
worker2   Ready     worker   SUSE Linux Enterprise Server 15 SP2   4.12.14-197.29-default   v1.18.6           cri-o://1.18.2      no            no                       4.5.0</pre></div></li></ol></div><div id="id-1.5.8.4.11.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost cannot be used.</p></div></div></div><div class="sect2" id="_using_kubectl"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using kubectl</span> <a title="Permalink" class="permalink" href="#_using_kubectl">#</a></h3></div></div></div><p>You can install and use <code class="literal">kubectl</code> by installing the <code class="literal">kubernetes-client</code> package from the SUSE CaaS Platform extension.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo zypper in kubernetes-client</pre></div><div id="id-1.5.8.5.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Alternatively you can install from upstream: <a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/tools/install-kubectl/</a>.</p></div><p>To talk to your cluster, you must be in the <code class="literal">&lt;CLUSTER_NAME&gt;</code> directory when running commands so it can find the <code class="literal">admin.conf</code> file.</p><div id="id-1.5.8.5.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Setting up <code class="literal">kubeconfig</code></h6><p>To make usage of Kubernetes tools easier, you can store a copy of the <code class="literal">admin.conf</code> file as <a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/" target="_blank">kubeconfig</a>.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p ~/.kube
cp admin.conf ~/.kube/config</pre></div><div id="id-1.5.8.5.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The configuration file contains sensitive information and must be handled in a secure fashion. Copying it to a shared user directory might grant access to unwanted users.</p></div><p>You can run commands against your cluster like usual. For example:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">kubectl get nodes -o wide</code></p><p>or</p></li><li class="listitem "><p><code class="literal">kubectl get pods --all-namespaces</code></p><div class="verbatim-wrap highlight bash"><pre class="screen"># kubectl get pods --all-namespaces

NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5zftb                1/1       Running   0          2m
kube-system   coredns-86c58d9df4-fct4m                1/1       Running   0          2m
kube-system   etcd-my-master                          1/1       Running   0          1m
kube-system   kube-apiserver-my-master                1/1       Running   0          1m
kube-system   kube-controller-manager-my-master       1/1       Running   0          1m
kube-system   cilium-operator-7d6ddddbf5-dmbhv        1/1       Running   0          51s
kube-system   cilium-qjt9h                            1/1       Running   0          53s
kube-system   cilium-szkqc                            1/1       Running   0          2m
kube-system   kube-proxy-5qxnt                        1/1       Running   0          2m
kube-system   kube-proxy-746ws                        1/1       Running   0          53s
kube-system   kube-scheduler-my-master                1/1       Running   0          1m
kube-system   kured-ztnfj                             1/1       Running   0          2m
kube-system   kured-zv696                             1/1       Running   0          2m
kube-system   oidc-dex-55fc689dc-b9bxw                1/1       Running   0          2m
kube-system   oidc-gangway-7b7fbbdbdf-ll6l8           1/1       Running   0          2m</pre></div></li></ul></div></div></div></div><div class="chapter " id="_glossary"><div class="titlepage"><div><div><h1 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Glossary</span> <a title="Permalink" class="permalink" href="#_glossary">#</a></h1></div></div></div><div class="line"></div><div class="horizontal"><table class="horizontal" border="0"><colgroup><col /><col /></colgroup><tbody valign="top"><tr><td valign="top">
<p>AWS</p>
</td><td valign="top">
<p>Amazon Web Services. A broadly adopted cloud platform run by Amazon.</p>
</td></tr><tr><td valign="top">
<p>BPF</p>
</td><td valign="top">
<p>Berkeley Packet Filter. Technology used by Cilium to filter network traffic at the level of packet processing in the kernel.</p>
</td></tr><tr><td valign="top">
<p>CA</p>
</td><td valign="top">
<p>Certificate or Certification Authority. An entity that issues digital certificates.</p>
</td></tr><tr><td valign="top">
<p>CIDR</p>
</td><td valign="top">
<p>Classless Inter-Domain Routing. Method for allocating IP addresses and IP routing.</p>
</td></tr><tr><td valign="top">
<p>CNI</p>
</td><td valign="top">
<p>Container Networking Interface. Creates a generic plugin-based networking solution for containers based on spec files in JSON format.</p>
</td></tr><tr><td valign="top">
<p>CRD</p>
</td><td valign="top">
<p>Custom Resource Definition. Functionality to define non-default resources for Kubernetes pods.</p>
</td></tr><tr><td valign="top">
<p>FQDN</p>
</td><td valign="top">
<p>Fully Qualified Domain Name. The complete domain name for a specific computer, or host, on the internet, consisting of two parts: the hostname and the domain name.</p>
</td></tr><tr><td valign="top">
<p>GKE</p>
</td><td valign="top">
<p>Google Kubernetes Engine. Manager for container orchestration built on Kubernetes by Google. Similar for example to Amazon Elastic Kubernetes Service (Amazon EKS) and Azure Kubernetes Service (AKS).</p>
</td></tr><tr><td valign="top">
<p>HPA</p>
</td><td valign="top">
<p>Horizontal Pod Autoscaler. Based on CPU usage, HPA controls the number of pods in a deployment/replica or stateful set or a replication controller.</p>
</td></tr><tr><td valign="top">
<p>KVM</p>
</td><td valign="top">
<p>Kernel-based Virtual Machine. Linux native virtualization tool that allows the kernel to function as a hypervisor.</p>
</td></tr><tr><td valign="top">
<p>LDAP</p>
</td><td valign="top">
<p>Lightweight Directory Access Protocol. A client/server protocol used to access and manage directory information. It reads and edits directories over IP networks and runs directly over TCP/IP using simple string formats for data transfer.</p>
</td></tr><tr><td valign="top">
<p>OCI</p>
</td><td valign="top">
<p>Open Containers Initiative. A project under the Linux Foundation with the goal of creating open industry standards around container formats and runtime.</p>
</td></tr><tr><td valign="top">
<p>OIDC</p>
</td><td valign="top">
<p>OpenID Connect. Identity layer on top of the OAuth 2.0 protocol.</p>
</td></tr><tr><td valign="top">
<p>OLM</p>
</td><td valign="top">
<p>Operator Lifecycle Manager. Open Source tool for managing operators in a Kubernetes cluster.</p>
</td></tr><tr><td valign="top">
<p>POC</p>
</td><td valign="top">
<p>Proof of Concept. Pioneering project directed at proving the feasibility of a design concept.</p>
</td></tr><tr><td valign="top">
<p>PSP</p>
</td><td valign="top">
<p>Pod Security Policy. PSPs are cluster-level resources that control security-sensitive aspects of pod specification.</p>
</td></tr><tr><td valign="top">
<p>PVC</p>
</td><td valign="top">
<p>Persistent Volume Claim. A request for storage by a user.</p>
</td></tr><tr><td valign="top">
<p>RBAC</p>
</td><td valign="top">
<p>Role-based Access Control. An approach to restrict authorized user access based on defined roles.</p>
</td></tr><tr><td valign="top">
<p>RMT</p>
</td><td valign="top">
<p>Repository Mirroring Tool. Successor of the SMT. Helps optimize the management of SUSE Linux Enterprise software updates and subscription entitlements.</p>
</td></tr><tr><td valign="top">
<p>RPO</p>
</td><td valign="top">
<p>Recovery Point Objective. Defines the interval of time that can occur between to backup points before normal business can no longer be resumed.</p>
</td></tr><tr><td valign="top">
<p>RTO</p>
</td><td valign="top">
<p>Recovery Time Objective. This defines the time (and typically service level from SLA) with which backup relevant incidents must be handled within.</p>
</td></tr><tr><td valign="top">
<p>RSA</p>
</td><td valign="top">
<p>Rivest-Shamir-Adleman. Asymmetric encryption technique that uses two different keys as public and private keys to perform the encryption and decryption.</p>
</td></tr><tr><td valign="top">
<p>SLA</p>
</td><td valign="top">
<p>Service Level Agreement. A contractual clause or set of clauses that determines the guaranteed handling of support or incidents by a software vendor or supplier.</p>
</td></tr><tr><td valign="top">
<p>SMT</p>
</td><td valign="top">
<p>SUSE Subscription Management Tool. Helps to manage software updates, maintain corporate firewall policy and meet regulatory compliance requirements in SUSE Linux Enterprise 11 and 12. Has been replaced by the RMT and SUSE Manager in newer SUSE Linux Enterprise versions.</p>
</td></tr><tr><td valign="top">
<p>STS</p>
</td><td valign="top">
<p>StatefulSet. Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods for a "stateful" application.</p>
</td></tr><tr><td valign="top">
<p>SMTP</p>
</td><td valign="top">
<p>Simple Mail Transfer Protocol. A communication protocol for electronic mail transmission.</p>
</td></tr><tr><td valign="top">
<p>TOML</p>
</td><td valign="top">
<p>Tom’s Obvious, Minimal Language. Configuration file format used for configuring container registries for CRI-O.</p>
</td></tr><tr><td valign="top">
<p>VPA</p>
</td><td valign="top">
<p>Vertical Pod Autoscaler. VPA automatically sets the values for resource requests and container limits based on usage.</p>
</td></tr><tr><td valign="top">
<p>VPC</p>
</td><td valign="top">
<p>Virtual Private Cloud. Division of a public cloud, which supports private cloud computing and thus offers more control over virtual networks and an isolated environment for sensitive workloads.</p>
</td></tr></tbody></table></div></div><div class="appendix " id="_contributors"><div class="titlepage"><div><div><h1 class="title"><span class="number">A </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Contributors</span> <a title="Permalink" class="permalink" href="#_contributors">#</a></h1></div></div></div><div class="line"></div><p>The contents of these documents are edited by the technical writers for
SUSE CaaS Platform and original works created by its <a class="link" href="https://github.com/SUSE/doc-caasp/graphs/contributors" target="_blank">contributors</a>.</p></div><div class="appendix " id="_gnu_licenses"><div class="titlepage"><div><div><h1 class="title"><span class="number">B </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">GNU Licenses</span> <a title="Permalink" class="permalink" href="#_gnu_licenses">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_gnu_free_documentation_license"><span class="number">B.1 </span><span class="name">GNU Free Documentation License</span></a></span></dt></dl></div></div><p>This appendix contains the GNU Free Documentation License version 1.2.</p><div class="sect1" id="_gnu_free_documentation_license"><div class="titlepage"><div><div><h2 class="title"><span class="number">B.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">GNU Free Documentation License</span> <a title="Permalink" class="permalink" href="#_gnu_free_documentation_license">#</a></h2></div></div></div><p>Copyright © 2000, 2001, 2002 Free Software Foundation, Inc.
51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p><div class="sect2" id="gfdl-preamble"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">0. PREAMBLE</span> <a title="Permalink" class="permalink" href="#gfdl-preamble">#</a></h3></div></div></div><p>The purpose of this License is to make a manual, textbook, or other functional and useful document "free" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or non-commercially.
Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.</p><p>This License is a kind of "copyleft", which means that derivative works of the document must themselves be free in the same sense.
It complements the GNU General Public License, which is a copyleft license designed for free software.</p><p>We have designed this License to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does.
But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book.
We recommend this License principally for works whose purpose is instruction or reference.</p></div><div class="sect2" id="gfdl-applicabilty"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">1. APPLICABILITY AND DEFINITIONS</span> <a title="Permalink" class="permalink" href="#gfdl-applicabilty">#</a></h3></div></div></div><p>This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License.
Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein.
The "Document", below, refers to any such manual or work.
Any member of the public is a licensee, and is addressed as "you". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.</p><p>A "Modified Version" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.</p><p>A "Secondary Section" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document’s overall subject (or to related matters) and contains nothing that could fall directly within that overall subject.
(Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.</p><p>The "Invariant Sections" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License.
If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant.
The Document may contain zero Invariant Sections.
If the Document does not identify any Invariant Sections then there are none.</p><p>The "Cover Texts" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License.
A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.</p><p>A "Transparent" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters.
A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent.
An image format is not Transparent if used for any substantial amount of text.
A copy that is not "Transparent" is called "Opaque".</p><p>Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification.
Examples of transparent image formats include PNG, XCF and JPG.
Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.</p><p>The "Title Page" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page.
For works in formats which do not have any title page as such, "Title Page" means the text near the most prominent appearance of the work’s title, preceding the beginning of the body of the text.</p><p>A section "Entitled XYZ" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language.
(Here XYZ stands for a specific section name mentioned below, such as "Acknowledgements", "Dedications", "Endorsements", or "History".) To "Preserve the Title" of such a section when you modify the Document means that it remains a section "Entitled XYZ" according to this definition.</p><p>The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document.
These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.</p></div><div class="sect2" id="gfdl-verbatim-copying"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">2. VERBATIM COPYING</span> <a title="Permalink" class="permalink" href="#gfdl-verbatim-copying">#</a></h3></div></div></div><p>You may copy and distribute the Document in any medium, either commercially or non-commercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License.
You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute.
However, you may accept compensation in exchange for copies.
If you distribute a large enough number of copies you must also follow the conditions in section 3.</p><p>You may also lend copies, under the same conditions stated above, and you may publicly display copies.</p></div><div class="sect2" id="gfdl-quantity-copying"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">3. COPYING IN QUANTITY</span> <a title="Permalink" class="permalink" href="#gfdl-quantity-copying">#</a></h3></div></div></div><p>If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document’s license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover.
Both covers must also clearly and legibly identify you as the publisher of these copies.
The front cover must present the full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition.
Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.</p><p>If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.</p><p>If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material.
If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.</p><p>It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.</p></div><div class="sect2" id="gfdl-modifications"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">4. MODIFICATIONS</span> <a title="Permalink" class="permalink" href="#gfdl-modifications">#</a></h3></div></div></div><p>You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it.
In addition, you must do these things in the Modified Version:</p><div class="orderedlist "><ol class="orderedlist" type="A"><li class="listitem "><p>Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.</p></li><li class="listitem "><p>List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.</p></li><li class="listitem "><p>State on the Title page the name of the publisher of the Modified Version, as the publisher.</p></li><li class="listitem "><p>Preserve all the copyright notices of the Document.</p></li><li class="listitem "><p>Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.</p></li><li class="listitem "><p>Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.</p></li><li class="listitem "><p>Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document’s license notice.</p></li><li class="listitem "><p>Include an unaltered copy of this License.</p></li><li class="listitem "><p>Preserve the section Entitled "History", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled "History" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.</p></li><li class="listitem "><p>Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the "History" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.</p></li><li class="listitem "><p>For any section Entitled "Acknowledgements" or "Dedications", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.</p></li><li class="listitem "><p>Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.</p></li><li class="listitem "><p>Delete any section Entitled "Endorsements". Such a section may not be included in the Modified Version.</p></li><li class="listitem "><p>Do not retitle any existing section to be Entitled "Endorsements" or to conflict in title with any Invariant Section.</p></li><li class="listitem "><p>Preserve any Warranty Disclaimers.</p></li></ol></div><p>If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant.
To do this, add their titles to the list of Invariant Sections in the Modified Version’s license notice.
These titles must be distinct from any other section titles.</p><p>You may add a section Entitled "Endorsements", provided it contains nothing but endorsements of your Modified Version by various parties—​for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.</p><p>You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version.
Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity.
If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.</p><p>The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.</p></div><div class="sect2" id="gfdl-combining"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">5. COMBINING DOCUMENTS</span> <a title="Permalink" class="permalink" href="#gfdl-combining">#</a></h3></div></div></div><p>You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.</p><p>The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy.
If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.</p><p>In the combination, you must combine any sections Entitled "History" in the various original documents, forming one section Entitled "History"; likewise combine any sections Entitled "Acknowledgements", and any sections Entitled "Dedications". You must delete all sections Entitled "Endorsements".</p></div><div class="sect2" id="gfdl-collections"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">6. COLLECTIONS OF DOCUMENTS</span> <a title="Permalink" class="permalink" href="#gfdl-collections">#</a></h3></div></div></div><p>You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.</p><p>You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.</p></div><div class="sect2" id="gfdl-aggregation"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">7. AGGREGATION WITH INDEPENDENT WORKS</span> <a title="Permalink" class="permalink" href="#gfdl-aggregation">#</a></h3></div></div></div><p>A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an "aggregate" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation’s users beyond what the individual works permit.
When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.</p><p>If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document’s Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form.
Otherwise they must appear on printed covers that bracket the whole aggregate.</p></div><div class="sect2" id="gfdl-translation"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">8. TRANSLATION</span> <a title="Permalink" class="permalink" href="#gfdl-translation">#</a></h3></div></div></div><p>Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4.
Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections.
You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers.
In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.</p><p>If a section in the Document is Entitled "Acknowledgements", "Dedications", or "History", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.</p></div><div class="sect2" id="gfdl-termination"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">9. TERMINATION</span> <a title="Permalink" class="permalink" href="#gfdl-termination">#</a></h3></div></div></div><p>You may not copy, modify, sublicense, or distribute the Document except as expressly provided for under this License.
Any other attempt to copy, modify, sublicense or distribute the Document is void, and will automatically terminate your rights under this License.
However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.</p><div class="sect3" id="gfdl-future"><div class="titlepage"><div><div><h4 class="title"><span class="number">B.1.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">10. FUTURE REVISIONS OF THIS LICENSE</span> <a title="Permalink" class="permalink" href="#gfdl-future">#</a></h4></div></div></div><p>The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time.
Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.
See <a class="link" href="http://www.gnu.org/copyleft/" target="_blank">http://www.gnu.org/copyleft/</a>.</p><p>Each version of the License is given a distinguishing version number.
If the Document specifies that a particular numbered version of this License "or any later version" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation.
If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation.</p></div><div class="sect3" id="gfdl-addendum"><div class="titlepage"><div><div><h4 class="title"><span class="number">B.1.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ADDENDUM: How to use this License for your documents</span> <a title="Permalink" class="permalink" href="#gfdl-addendum">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled "GNU
Free Documentation License".</pre></div><p>If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the “
with…​Texts.”
line with this:</p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>If you have Invariant Sections without Cover Texts, or some other combination of the three, merge those two alternatives to suit the situation.</p><p>If your document contains nontrivial examples of program code, we recommend releasing these examples in parallel under your choice of free software license, such as the GNU General Public License, to permit their use in free software.</p></div></div></div></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>