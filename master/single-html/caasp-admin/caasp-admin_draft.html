<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Administration Guide | SUSE CaaS Platform 4.5.1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="product-name" content="SUSE CaaS Platform" /><meta name="product-number" content="4.5.1" /><meta name="book-title" content="Administration Guide" /><meta name="description" content="Copyright © 2006 — 2020 SUSE LLC and contributors. All rights reserved." /><meta name="tracker-url" content="https://github.com/SUSE/doc-caasp/issues/new" /><meta name="tracker-type" content="gh" /><meta name="tracker-gh-labels" content="AdminGuide" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>Administration Guide</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>Show Contents: Administration Guide</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#id-1.2"><span class="number"> </span><span class="name"></span></a></li><li class="inactive"><a href="#_about_this_guide"><span class="number">1 </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="#_cluster_management"><span class="number">2 </span><span class="name">Cluster Management</span></a></li><li class="inactive"><a href="#_software_management"><span class="number">3 </span><span class="name">Software Management</span></a></li><li class="inactive"><a href="#_cluster_updates"><span class="number">4 </span><span class="name">Cluster Updates</span></a></li><li class="inactive"><a href="#_upgrading_suse_caas_platform"><span class="number">5 </span><span class="name">Upgrading SUSE CaaS Platform</span></a></li><li class="inactive"><a href="#_security"><span class="number">6 </span><span class="name">Security</span></a></li><li class="inactive"><a href="#_logging"><span class="number">7 </span><span class="name">Logging</span></a></li><li class="inactive"><a href="#_monitoring"><span class="number">8 </span><span class="name">Monitoring</span></a></li><li class="inactive"><a href="#_storage"><span class="number">9 </span><span class="name">Storage</span></a></li><li class="inactive"><a href="#_integration"><span class="number">10 </span><span class="name">Integration</span></a></li><li class="inactive"><a href="#_gpu_dependent_workloads"><span class="number">11 </span><span class="name">GPU-Dependent Workloads</span></a></li><li class="inactive"><a href="#_cluster_disaster_recovery"><span class="number">12 </span><span class="name">Cluster Disaster Recovery</span></a></li><li class="inactive"><a href="#backup-and-restore-with-velero"><span class="number">13 </span><span class="name">Backup and Restore with Velero</span></a></li><li class="inactive"><a href="#_miscellaneous"><span class="number">14 </span><span class="name">Miscellaneous</span></a></li><li class="inactive"><a href="#_troubleshooting_3"><span class="number">15 </span><span class="name">Troubleshooting</span></a></li><li class="inactive"><a href="#_glossary"><span class="number">16 </span><span class="name">Glossary</span></a></li><li class="inactive"><a href="#_contributors"><span class="number">A </span><span class="name">Contributors</span></a></li><li class="inactive"><a href="#_gnu_licenses"><span class="number">B </span><span class="name">GNU Licenses</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="book" id="id-1" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname ">SUSE CaaS Platform</span> <span class="productnumber ">4.5.1</span></h6><div><h1 class="title">Administration Guide</h1></div><div><h2 class="subtitle">This guide describes general and specialized administrative tasks for SUSE CaaS Platform 4.5.1.</h2></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname ">Markus</span> <span class="surname ">Napp</span> and <span class="firstname ">Nora</span> <span class="surname ">Kořánová</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>2020-11-11</div></div></div><div class="toc"><dl><dt><span class="preface"><a href="#id-1.2"><span class="name"></span></a></span></dt><dt><span class="chapter"><a href="#_about_this_guide"><span class="number">1 </span><span class="name">About This Guide</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_required_background"><span class="number">1.1 </span><span class="name">Required Background</span></a></span></dt><dt><span class="section"><a href="#_available_documentation"><span class="number">1.2 </span><span class="name">Available Documentation</span></a></span></dt><dt><span class="section"><a href="#_feedback"><span class="number">1.3 </span><span class="name">Feedback</span></a></span></dt><dt><span class="section"><a href="#_documentation_conventions"><span class="number">1.4 </span><span class="name">Documentation Conventions</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_cluster_management"><span class="number">2 </span><span class="name">Cluster Management</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_prerequisites"><span class="number">2.1 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#_bootstrap_and_initial_configuration"><span class="number">2.2 </span><span class="name">Bootstrap and Initial Configuration</span></a></span></dt><dt><span class="section"><a href="#adding-nodes"><span class="number">2.3 </span><span class="name">Adding Nodes</span></a></span></dt><dt><span class="section"><a href="#removing-nodes"><span class="number">2.4 </span><span class="name">Removing Nodes</span></a></span></dt><dt><span class="section"><a href="#_reconfiguring_nodes"><span class="number">2.5 </span><span class="name">Reconfiguring Nodes</span></a></span></dt><dt><span class="section"><a href="#node-operations"><span class="number">2.6 </span><span class="name">Node Operations</span></a></span></dt><dt><span class="section"><a href="#shutdown-startup"><span class="number">2.7 </span><span class="name">Graceful Cluster Shutdown &amp; Startup</span></a></span></dt><dt><span class="section"><a href="#_post_startup_activities"><span class="number">2.8 </span><span class="name">Post Startup Activities</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_software_management"><span class="number">3 </span><span class="name">Software Management</span></a></span></dt><dd><dl><dt><span class="section"><a href="#software-installation"><span class="number">3.1 </span><span class="name">Software Installation</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_cluster_updates"><span class="number">4 </span><span class="name">Cluster Updates</span></a></span></dt><dd><dl><dt><span class="section"><a href="#handling-updates"><span class="number">4.1 </span><span class="name">Update Requirements</span></a></span></dt><dt><span class="section"><a href="#_updating_kubernetes_components"><span class="number">4.2 </span><span class="name">Updating Kubernetes Components</span></a></span></dt><dt><span class="section"><a href="#_updating_nodes"><span class="number">4.3 </span><span class="name">Updating Nodes</span></a></span></dt><dt><span class="section"><a href="#base-os-updates"><span class="number">4.4 </span><span class="name">Base OS Updates</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_upgrading_suse_caas_platform"><span class="number">5 </span><span class="name">Upgrading SUSE CaaS Platform</span></a></span></dt><dd><dl><dt><span class="section"><a href="#caasp-migration"><span class="number">5.1 </span><span class="name">Migration to SUSE CaaS Platform 4.5</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_security"><span class="number">6 </span><span class="name">Security</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_network_access_considerations"><span class="number">6.1 </span><span class="name">Network Access Considerations</span></a></span></dt><dt><span class="section"><a href="#_access_control"><span class="number">6.2 </span><span class="name">Access Control</span></a></span></dt><dt><span class="section"><a href="#authentication"><span class="number">6.3 </span><span class="name">Authentication</span></a></span></dt><dt><span class="section"><a href="#sec-admin-security-users"><span class="number">6.4 </span><span class="name">Managing LDAP Users and Groups</span></a></span></dt><dt><span class="section"><a href="#rbac"><span class="number">6.5 </span><span class="name">Role-Based Access Control (RBAC)</span></a></span></dt><dt><span class="section"><a href="#admission"><span class="number">6.6 </span><span class="name">Admission Controllers</span></a></span></dt><dt><span class="section"><a href="#_pod_security_policies"><span class="number">6.7 </span><span class="name">Pod Security Policies</span></a></span></dt><dt><span class="section"><a href="#nginx-ingress"><span class="number">6.8 </span><span class="name">NGINX Ingress Controller</span></a></span></dt><dt><span class="section"><a href="#_certificates"><span class="number">6.9 </span><span class="name">Certificates</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_logging"><span class="number">7 </span><span class="name">Logging</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_introduction_2"><span class="number">7.1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="section"><a href="#tee-logging"><span class="number">7.2 </span><span class="name">Logging in skuba</span></a></span></dt><dt><span class="section"><a href="#_audit_log"><span class="number">7.3 </span><span class="name">Audit Log</span></a></span></dt><dt><span class="section"><a href="#centralized-logging"><span class="number">7.4 </span><span class="name">Centralized Logging</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_monitoring"><span class="number">8 </span><span class="name">Monitoring</span></a></span></dt><dd><dl><dt><span class="section"><a href="#monitoring-stack"><span class="number">8.1 </span><span class="name">Monitoring Stack</span></a></span></dt><dt><span class="section"><a href="#_health_checks"><span class="number">8.2 </span><span class="name">Health Checks</span></a></span></dt><dt><span class="section"><a href="#horizontal-pod-autoscaler"><span class="number">8.3 </span><span class="name">Horizontal Pod Autoscaler</span></a></span></dt><dt><span class="section"><a href="#_stratos_web_console"><span class="number">8.4 </span><span class="name">Stratos Web Console</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_storage"><span class="number">9 </span><span class="name">Storage</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_azure_storage"><span class="number">9.1 </span><span class="name">Azure Storage</span></a></span></dt><dt><span class="section"><a href="#_vsphere_storage"><span class="number">9.2 </span><span class="name">vSphere Storage</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_integration"><span class="number">10 </span><span class="name">Integration</span></a></span></dt><dd><dl><dt><span class="section"><a href="#ses-integration"><span class="number">10.1 </span><span class="name">SUSE Enterprise Storage Integration</span></a></span></dt><dt><span class="section"><a href="#_suse_cloud_application_platform_integration"><span class="number">10.2 </span><span class="name">SUSE Cloud Application Platform Integration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_gpu_dependent_workloads"><span class="number">11 </span><span class="name">GPU-Dependent Workloads</span></a></span></dt><dd><dl><dt><span class="section"><a href="#gpus"><span class="number">11.1 </span><span class="name">NVIDIA GPUs</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_cluster_disaster_recovery"><span class="number">12 </span><span class="name">Cluster Disaster Recovery</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_backing_up_etcd_cluster_data"><span class="number">12.1 </span><span class="name">Backing Up etcd Cluster Data</span></a></span></dt><dt><span class="section"><a href="#_recovering_master_nodes"><span class="number">12.2 </span><span class="name">Recovering Master Nodes</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#backup-and-restore-with-velero"><span class="number">13 </span><span class="name">Backup and Restore with Velero</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_limitations_2"><span class="number">13.1 </span><span class="name">Limitations</span></a></span></dt><dt><span class="section"><a href="#_prerequisites_8"><span class="number">13.2 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#_known_issues"><span class="number">13.3 </span><span class="name">Known Issues</span></a></span></dt><dt><span class="section"><a href="#_deployment_2"><span class="number">13.4 </span><span class="name">Deployment</span></a></span></dt><dt><span class="section"><a href="#_operations"><span class="number">13.5 </span><span class="name">Operations</span></a></span></dt><dt><span class="section"><a href="#_backup"><span class="number">13.6 </span><span class="name">Backup</span></a></span></dt><dt><span class="section"><a href="#_restore"><span class="number">13.7 </span><span class="name">Restore</span></a></span></dt><dt><span class="section"><a href="#_use_cases"><span class="number">13.8 </span><span class="name">Use Cases</span></a></span></dt><dt><span class="section"><a href="#_uninstall"><span class="number">13.9 </span><span class="name">Uninstall</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_miscellaneous"><span class="number">14 </span><span class="name">Miscellaneous</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_configuring_httphttps_proxy_for_cri_o"><span class="number">14.1 </span><span class="name">Configuring HTTP/HTTPS Proxy for CRI-O</span></a></span></dt><dt><span class="section"><a href="#config-crio-container-registry"><span class="number">14.2 </span><span class="name">Configuring Container Registries for CRI-O</span></a></span></dt><dt><span class="section"><a href="#_flexvolume_configuration"><span class="number">14.3 </span><span class="name">FlexVolume Configuration</span></a></span></dt><dt><span class="section"><a href="#_configuring_kubelet"><span class="number">14.4 </span><span class="name">Configuring kubelet</span></a></span></dt><dt><span class="section"><a href="#k8s-changes-117-118"><span class="number">14.5 </span><span class="name">Changes from Kubernetes 1.17 to 1.18</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_troubleshooting_3"><span class="number">15 </span><span class="name">Troubleshooting</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_the_supportconfig_tool"><span class="number">15.1 </span><span class="name">The <code class="literal">supportconfig</code> Tool</span></a></span></dt><dt><span class="section"><a href="#_cluster_definition_directory"><span class="number">15.2 </span><span class="name">Cluster definition directory</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-logs"><span class="number">15.3 </span><span class="name">Log collection</span></a></span></dt><dt><span class="section"><a href="#_debugging_sles_nodes_provision"><span class="number">15.4 </span><span class="name">Debugging SLES Nodes provision</span></a></span></dt><dt><span class="section"><a href="#_debugging_cluster_deployment"><span class="number">15.5 </span><span class="name">Debugging Cluster Deployment</span></a></span></dt><dt><span class="section"><a href="#_error_x509_certificate_signed_by_unknown_authority"><span class="number">15.6 </span><span class="name">Error <code class="literal">x509: certificate signed by unknown authority</code></span></a></span></dt><dt><span class="section"><a href="#_error_invalid_client_credentials"><span class="number">15.7 </span><span class="name">Error <code class="literal">Invalid client credentials</code></span></a></span></dt><dt><span class="section"><a href="#_replacing_a_lost_node"><span class="number">15.8 </span><span class="name">Replacing a Lost Node</span></a></span></dt><dt><span class="section"><a href="#_rebooting_an_undrained_node_with_rbd_volumes_mapped"><span class="number">15.9 </span><span class="name">Rebooting an Undrained Node with RBD Volumes Mapped</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-etcd"><span class="number">15.10 </span><span class="name">ETCD Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#_kubernetes_debugging_tips"><span class="number">15.11 </span><span class="name">Kubernetes debugging tips</span></a></span></dt><dt><span class="section"><a href="#_helm_error_context_deadline_exceeded"><span class="number">15.12 </span><span class="name">Helm <code class="literal">Error: context deadline exceeded</code></span></a></span></dt><dt><span class="section"><a href="#_aws_deployment_fails_with_cannot_attach_profile_error"><span class="number">15.13 </span><span class="name">AWS Deployment fails with <code class="literal">cannot attach profile</code> error</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_glossary"><span class="number">16 </span><span class="name">Glossary</span></a></span></dt><dt><span class="appendix"><a href="#_contributors"><span class="number">A </span><span class="name">Contributors</span></a></span></dt><dt><span class="appendix"><a href="#_gnu_licenses"><span class="number">B </span><span class="name">GNU Licenses</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_gnu_free_documentation_license"><span class="number">B.1 </span><span class="name">GNU Free Documentation License</span></a></span></dt></dl></dd></dl></div><div class="preface " id="id-1.2"><div class="titlepage"><div><div><h1 class="title"><span class="number"> </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"></span> <a title="Permalink" class="permalink" href="#id-1.2">#</a></h1></div></div></div><div class="line"></div><div id="id-1.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>This document is a work in progress.</p><p>The content in this document is subject to change without notice.</p></div><div id="id-1.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>This guide assumes a configured SUSE Linux Enterprise Server 15 SP2 environment.</p></div><p>Copyright ©
2006 — 2020
SUSE LLC and contributors.
All rights reserved.</p><p>Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the Invariant Section being this copyright notice and license.
A copy of the license version 1.2 is included in the section entitled <span class="quote">“<span class="quote ">GNU Free Documentation License</span>”</span>.</p><p>For SUSE
trademarks, see <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>.
All other third-party trademarks are the property of their respective owners.
Trademark symbols (®, ™, etc.) denote trademarks of SUSE and its affiliates.
Asterisks (*) denote third-party trademarks.</p><p>All information found in this book has been compiled with utmost attention to detail.
However, this does not guarantee complete accuracy.
Neither SUSE LLC, its affiliates, the authors, nor the translators shall be held liable for possible errors or the consequences thereof.</p></div><div class="chapter " id="_about_this_guide"><div class="titlepage"><div><div><h1 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">About This Guide</span> <a title="Permalink" class="permalink" href="#_about_this_guide">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_required_background"><span class="number">1.1 </span><span class="name">Required Background</span></a></span></dt><dt><span class="section"><a href="#_available_documentation"><span class="number">1.2 </span><span class="name">Available Documentation</span></a></span></dt><dt><span class="section"><a href="#_feedback"><span class="number">1.3 </span><span class="name">Feedback</span></a></span></dt><dt><span class="section"><a href="#_documentation_conventions"><span class="number">1.4 </span><span class="name">Documentation Conventions</span></a></span></dt></dl></div></div><div class="sect1" id="_required_background"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Required Background</span> <a title="Permalink" class="permalink" href="#_required_background">#</a></h2></div></div></div><p>To keep the scope of these guidelines manageable, certain technical assumptions have been made.
These documents are not aimed at beginners in Kubernetes usage and require that:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>You have some computer experience and are familiar with common technical terms.</p></li><li class="listitem "><p>You are familiar with the documentation for your system and the network on which it runs.</p></li><li class="listitem "><p>You have a basic understanding of Linux systems.</p></li><li class="listitem "><p>You have an understanding of how to follow instructions aimed at experienced Linux administrators
and can fill in gaps with your own research.</p></li><li class="listitem "><p>You understand how to plan, deploy and manage Kubernetes applications.</p></li></ul></div></div><div class="sect1" id="_available_documentation"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Available Documentation</span> <a title="Permalink" class="permalink" href="#_available_documentation">#</a></h2></div></div></div><p><a id="id-1.3.3.2.1" class="indexterm"></a>
<a id="id-1.3.3.2.2" class="indexterm"></a></p><p>We provide HTML and PDF versions of our books in different languages.
Documentation for our products is available at <a class="link" href="https://documentation.suse.com/" target="_blank">https://documentation.suse.com/</a>, where you can also find the latest updates and browse or download the documentation in various formats.</p><p>The following documentation is available for this product:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.3.5.1"><span class="term ">Deployment Guide</span></dt><dd><p>The SUSE CaaS Platform Deployment Guide gives you details about installation and configuration of SUSE CaaS Platform
along with a description of architecture and minimum system requirements.</p></dd><dt id="id-1.3.3.5.2"><span class="term ">Quick Start Guide</span></dt><dd><p>The SUSE CaaS Platform
Quick Start guides you through the installation of a minimum cluster in the fastest way possible.</p></dd><dt id="id-1.3.3.5.3"><span class="term ">Admin Guide</span></dt><dd><p>The SUSE CaaS Platform
Admin Guide discusses authorization, updating clusters and individual nodes, monitoring, logging, use of Helm and Tiller, troubleshooting and integration with SUSE Enterprise Storage and SUSE Cloud Application Platform.</p></dd></dl></div></div><div class="sect1" id="_feedback"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Feedback</span> <a title="Permalink" class="permalink" href="#_feedback">#</a></h2></div></div></div><p>Several feedback channels are available:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.3.1"><span class="term ">Bugs and Enhancement Requests</span></dt><dd><p>For services and support options available for your product, refer to <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.</p><p>To report bugs for a product component, go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log in, and click <span class="guimenu ">Create New</span>.</p></dd><dt id="id-1.3.4.3.2"><span class="term ">User Comments</span></dt><dd><p>We want to hear your comments about and suggestions for this manual and the other documentation included with this product.
Use the User Comments feature at the bottom of each page in the online documentation or go to <a class="link" href="https://documentation.suse.com/" target="_blank">https://documentation.suse.com/</a>, click Feedback at the bottom of the page and enter your comments in the Feedback Form.</p></dd><dt id="id-1.3.4.3.3"><span class="term ">Mail</span></dt><dd><p>For feedback on the documentation of this product, you can also send a mail to <code class="literal">doc-team@suse.com</code>.
Make sure to include the document title, the product version and the publication date of the documentation.
To report errors or suggest enhancements, provide a concise description of the problem and refer to the respective section number and page (or URL).</p></dd></dl></div></div><div class="sect1" id="_documentation_conventions"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Documentation Conventions</span> <a title="Permalink" class="permalink" href="#_documentation_conventions">#</a></h2></div></div></div><p>The following notices and typographical conventions are used in this documentation:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">/etc/passwd</code> : directory names and file names</p></li><li class="listitem "><p><code class="literal">&lt;PLACEHOLDER&gt;</code>: replace <code class="literal">&lt;PLACEHOLDER&gt;</code> with the actual value</p></li><li class="listitem "><p><code class="literal">PATH</code>: the environment variable PATH</p></li><li class="listitem "><p><code class="literal">ls</code>, <code class="literal">--help</code>: commands, options, and parameters</p></li><li class="listitem "><p><code class="literal">user</code> : users or groups</p></li><li class="listitem "><p><span class="package">package name</span> : name of a package</p></li><li class="listitem "><p><span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span> : a key to press or a key combination; keys are shown in uppercase as on a keyboard</p></li><li class="listitem "><p><span class="guimenu ">File</span> › <span class="guimenu ">Save As</span> : menu items, buttons</p></li><li class="listitem "><p><span class="emphasis"><em>Dancing Penguins</em></span> (Chapter <span class="emphasis"><em>Penguins</em></span>, ↑Another Manual): This is a reference to a chapter in another manual.</p></li><li class="listitem "><p>Commands that must be run with root privileges. Often you can also prefix these commands with the <code class="literal">sudo</code> command to run them as non-privileged user.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo command</pre></div></li><li class="listitem "><p>Commands that can be run by non-privileged users.</p><div class="verbatim-wrap highlight bash"><pre class="screen">command</pre></div></li><li class="listitem "><p>Notices:</p><div id="id-1.3.5.3.12.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Vital information you must be aware of before proceeding.
Warns you about security issues, potential loss of data, damage to hardware, or physical hazards.</p></div><div id="id-1.3.5.3.12.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Important information you should be aware of before proceeding.</p></div><div id="id-1.3.5.3.12.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Additional information, for example about differences in software versions.</p></div><div id="id-1.3.5.3.12.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Helpful information, like a guideline or a piece of practical advice.</p></div></li></ul></div></div></div><div class="chapter " id="_cluster_management"><div class="titlepage"><div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Management</span> <a title="Permalink" class="permalink" href="#_cluster_management">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_prerequisites"><span class="number">2.1 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#_bootstrap_and_initial_configuration"><span class="number">2.2 </span><span class="name">Bootstrap and Initial Configuration</span></a></span></dt><dt><span class="section"><a href="#adding-nodes"><span class="number">2.3 </span><span class="name">Adding Nodes</span></a></span></dt><dt><span class="section"><a href="#removing-nodes"><span class="number">2.4 </span><span class="name">Removing Nodes</span></a></span></dt><dt><span class="section"><a href="#_reconfiguring_nodes"><span class="number">2.5 </span><span class="name">Reconfiguring Nodes</span></a></span></dt><dt><span class="section"><a href="#node-operations"><span class="number">2.6 </span><span class="name">Node Operations</span></a></span></dt><dt><span class="section"><a href="#shutdown-startup"><span class="number">2.7 </span><span class="name">Graceful Cluster Shutdown &amp; Startup</span></a></span></dt><dt><span class="section"><a href="#_post_startup_activities"><span class="number">2.8 </span><span class="name">Post Startup Activities</span></a></span></dt></dl></div></div><p>Cluster management refers to several processes in the life cycle of a cluster and
its individual nodes: bootstrapping, joining and removing nodes.
For maximum automation and ease SUSE CaaS Platform uses the <code class="literal">skuba</code> tool,
which simplifies Kubernetes cluster creation and reconfiguration.</p><div class="sect1" id="_prerequisites"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites">#</a></h2></div></div></div><p>You must have the proper SSH keys for accessing the nodes set up and allow passwordless <code class="literal">sudo</code>
on the nodes in order to perform many of these steps. If you have followed the standard
deployment procedures this should already be the case.</p><p>Please note: If you are using a different management workstation than the one you have
used during the initial deployment, you might have to transfer the SSH identities
from the original management workstation.</p></div><div class="sect1" id="_bootstrap_and_initial_configuration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bootstrap and Initial Configuration</span> <a title="Permalink" class="permalink" href="#_bootstrap_and_initial_configuration">#</a></h2></div></div></div><p>Bootstrapping the cluster is the initial process of starting up a minimal
viable cluster and joining the first master node. Only the first master node needs to be bootstrapped,
later nodes can simply be joined as described in <a class="xref" href="#adding-nodes" title="2.3. Adding Nodes">Section 2.3, “Adding Nodes”</a>.</p><p>Before bootstrapping any nodes to the cluster,
you need to create an initial cluster definition folder (initialize the cluster).
This is done using <code class="literal">skuba cluster init</code> and its <code class="literal">--control-plane</code> flag.</p><p>For a step by step guide on how to initialize the cluster, configure updates using <code class="literal">kured</code>
and subsequently bootstrap nodes to it, refer to the <span class="emphasis"><em>SUSE CaaS Platform Deployment Guide</em></span>.</p></div><div class="sect1" id="adding-nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Nodes</span> <a title="Permalink" class="permalink" href="#adding-nodes">#</a></h2></div></div></div><p>Once you have added the first master node to the cluster using <code class="literal">skuba node bootstrap</code>,
use the <code class="literal">skuba node join</code> command to add more nodes. Joining master or worker nodes to
an existing cluster should be done sequentially, meaning the nodes have to be added
one after another and not more of them in parallel.</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role &lt;MASTER/WORKER&gt; --user &lt;USER_NAME&gt; --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;</pre></div><p>The mandatory flags for the join command are <code class="literal">--role</code>, <code class="literal">--user</code>, <code class="literal">--sudo</code> and <code class="literal">--target</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">--role</code> serves to specify if the node is a <span class="strong"><strong>master</strong></span> or <span class="strong"><strong>worker</strong></span>.</p></li><li class="listitem "><p><code class="literal">--sudo</code> is for running the command with superuser privileges,
which is necessary for all node operations.</p></li><li class="listitem "><p><code class="literal">&lt;USER_NAME&gt;</code> is the name of the user that exists on your SLES machine (default: <code class="literal">sles</code>).</p></li><li class="listitem "><p><code class="literal">--target &lt;IP/FQDN&gt;</code> is the IP address or FQDN of the relevant machine.</p></li><li class="listitem "><p><code class="literal">&lt;NODE_NAME&gt;</code> is how you decide to name the node you are adding.</p></li></ul></div><div id="id-1.4.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>New master nodes that you didn’t initially include in your Terraform’s configuration have
to be manually added to your load balancer’s configuration.</p></div><p>To add a new <span class="strong"><strong>worker</strong></span> node, you would run something like:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node join --role worker --user sles --sudo --target 10.86.2.164 worker1</pre></div><div class="sect2" id="_adding_nodes_from_template"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Nodes from Template</span> <a title="Permalink" class="permalink" href="#_adding_nodes_from_template">#</a></h3></div></div></div><p>If you are using a virtual machine template for creating new cluster nodes,
you must make sure that <span class="strong"><strong>before</strong></span> joining the cloned machine to the cluster it is updated to the same software versions
than the other nodes in the cluster.</p><p>Refer to <a class="xref" href="#handling-updates" title="4.1. Update Requirements">Section 4.1, “Update Requirements”</a>.</p><p>Nodes with mismatching package or container software
versions might not be fully functional.</p></div></div><div class="sect1" id="removing-nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing Nodes</span> <a title="Permalink" class="permalink" href="#removing-nodes">#</a></h2></div></div></div><div class="sect2" id="_temporary_removal"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Temporary Removal</span> <a title="Permalink" class="permalink" href="#_temporary_removal">#</a></h3></div></div></div><p>If you wish to remove a node temporarily, the recommended approach is to first drain the node.</p><p>When you want to bring the node back, you only have to uncordon it.</p><div id="id-1.4.6.2.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>For instructions on how to perform these operations refer to <a class="xref" href="#node-operations" title="2.6. Node Operations">Section 2.6, “Node Operations”</a>.</p></div></div><div class="sect2" id="_permanent_removal"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Permanent Removal</span> <a title="Permalink" class="permalink" href="#_permanent_removal">#</a></h3></div></div></div><div id="id-1.4.6.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Nodes removed with this method cannot be added back to the cluster or any other
skuba-initiated cluster. You must reinstall the entire node and then join it
again to the cluster.</p></div><p>The <code class="literal">skuba node remove</code> command serves to <span class="strong"><strong>permanently</strong></span> remove nodes.
Running this command will work even if the target virtual machine is down,
so it is the safest way to remove the node.</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node remove &lt;NODE_NAME&gt; [flags]</pre></div><div id="id-1.4.6.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Per default, node removal has an unlimited timeout on waiting for the node to drain.
If the node is unreachable it can not be drained and thus the removal will fail or get stuck indefinitely.
You can specify a time after which removal will be performed without waiting for the node to
drain with the flag <code class="literal">--drain-timeout &lt;DURATION&gt;</code>.</p><p>For example, waiting for the node to drain for 1 minute and 5 seconds:</p><div class="verbatim-wrap"><pre class="screen">skuba node remove caasp-worker1 --drain-timeout 1m5s</pre></div><p>For a list of supported time formats run <code class="literal">skuba node remove -h</code>.</p></div><div id="id-1.4.6.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>After the removal of a master node, you have to manually delete its entries
from your load balancer’s configuration.</p></div></div></div><div class="sect1" id="_reconfiguring_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring Nodes</span> <a title="Permalink" class="permalink" href="#_reconfiguring_nodes">#</a></h2></div></div></div><p>To reconfigure a node, for example to change the node’s role from worker to master,
you will need to use a combination of commands.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Run <code class="literal">skuba node remove &lt;NODE_NAME&gt;</code>.</p></li><li class="listitem "><p>Reinstall the node from scratch.</p></li><li class="listitem "><p>Run <code class="literal">skuba node join --role &lt;DESIRED_ROLE&gt; --user &lt;USER_NAME&gt; --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt;</code>.</p></li></ol></div></div><div class="sect1" id="node-operations"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Operations</span> <a title="Permalink" class="permalink" href="#node-operations">#</a></h2></div></div></div><div class="sect2" id="_uncordon_and_cordon"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Uncordon and Cordon</span> <a title="Permalink" class="permalink" href="#_uncordon_and_cordon">#</a></h3></div></div></div><p>These to commands respectively define if a node is marked as <code class="literal">schedulable</code> or <code class="literal">unschedulable</code>.
This means that a node is allowed to or not allowed to receive any new workloads.
This can be useful when troubleshooting a node.</p><p>To mark a node as <code class="literal">unschedulable</code> run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl cordon &lt;NODE_NAME&gt;</pre></div><p>To mark a node as <code class="literal">schedulable</code> run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl uncordon &lt;NODE_NAME&gt;</pre></div></div><div class="sect2" id="_draining_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Draining Nodes</span> <a title="Permalink" class="permalink" href="#_draining_nodes">#</a></h3></div></div></div><p>Draining a node consists of evicting all the running pods from the current node in order to perform maintenance.
This is a mandatory step in order to ensure a proper functioning of the workloads.
This is achieved using <code class="literal">kubectl</code>.</p><p>To drain a node run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl drain &lt;NODE_NAME&gt;</pre></div><p>This action will also implicitly cordon the node.
Therefore once the maintenance is done, uncordon the node to set it back to schedulable.</p><p>Refer to the official Kubernetes documentation for more information:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service</a></p></div></div><div class="sect1" id="shutdown-startup"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Graceful Cluster Shutdown &amp; Startup</span> <a title="Permalink" class="permalink" href="#shutdown-startup">#</a></h2></div></div></div><p>In some scenarios like maintenance windows in your datacenter or some disaster scenarios,
you will want to shut down the cluster in a controlled fashion and later on bring it back up safely.
Follow the following instructions to safely stop all workloads.</p><div class="sect2" id="_cluster_shutdown"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Shutdown</span> <a title="Permalink" class="permalink" href="#_cluster_shutdown">#</a></h3></div></div></div><div id="id-1.4.9.3.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Document Scope</h6><p>This document is only concerned with shutting down the SUSE CaaS Platform cluster itself.</p></div><div id="id-1.4.9.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Storage Shutdown/Startup</h6><p>Any real time data streaming workloads will lose data if not rerouted to an alternative cluster.</p><p>Any workloads that hold data only in memory will lose this data.
Please check with the provider of your workload/application about proper data persistence in case of shutdown.</p><p>Any external storage services must be stopped/started separately. Please refer to the respective storage solution’s documentation.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a <a class="link" href="https://documentation.suse.com/suse-caasp/4.5//single-html/caasp-admin/#_backup" target="_blank">backup</a> of your cluster.</p></li><li class="listitem "><p>Scale all applications down to zero by using either the manifests or deployment names:</p><div id="id-1.4.9.3.4.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Do not scale down cluster services.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale --replicas=0 -f deployment.yaml</pre></div><p>or</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale deploy my-deployment --replicas=0</pre></div></li><li class="listitem "><p>Drain/cordon all worker nodes.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl drain &lt;node name&gt;</pre></div><div id="id-1.4.9.3.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Wait for the command to finish by itself, if it fails check for <a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/" target="_blank">Help</a></p></div></li><li class="listitem "><p>Run <code class="literal">kubectl get nodes</code> and make sure all your worker nodes have the status <code class="literal">Ready,SchedulingDisabled</code>.</p></li><li class="listitem "><p>Proceed to shutdown all your <code class="literal">worker</code> nodes on the machine level.</p></li><li class="listitem "><p>Now it is necessary to find out where the <code class="literal">etcd</code> leader is running, that is going to be the last node to shut down.
Find out which pods are running <code class="literal">etcd</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl get pods -n kube-system -o wide
NAME                         READY   STATUS    RESTARTS   AGE    IP              NODE                     NOMINATED NODE   READINESS GATES
...
etcd-master-pimp-general-00  1/1     Running   0          23m     10.84.73.114   master-pimp-general-00   &lt;none&gt;           &lt;none&gt;
...</pre></div></li><li class="listitem "><p>Then you need to get the list of active <code class="literal">etcd</code> members, this will also show which <code class="literal">master</code> node is currently the <code class="literal">etcd</code> leader.
Either run a terminal session on one of the <code class="literal">etcd</code> pods:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl exec -ti -n kube-system etcd-master01 -- sh

# Now run this command
etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list</pre></div><p>or directly execute the command on the pod:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl exec -ti -n kube-system etcd-master01 -- etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list</pre></div><p>The output will be the same. Note the boolean values at the end of each line.
The current <code class="literal">etcd</code> leader will have <code class="literal">true</code>.
In this case the node <code class="literal">master02</code> is the current <code class="literal">etcd</code> leader.</p><div class="verbatim-wrap"><pre class="screen">356ebc35f3e8b25, started, master02, https://172.28.0.16:2380, https://172.28.0.16:2379, true
bdef0dced3caa0d4, started, master01, https://172.28.0.15:2380, https://172.28.0.15:2379, false
f9ae57d57b369ede, started, master03, https://172.28.0.21:2380, https://172.28.0.21:2379, false</pre></div></li><li class="listitem "><p>Shutdown all other master nodes, leaving the current <code class="literal">etcd</code> leader for last.</p></li><li class="listitem "><p>Finally, shut down the <code class="literal">etcd</code> leader node.</p><div id="id-1.4.9.3.4.9.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>This is the first node that needs to be started back up.</p></div></li></ol></div></div><div class="sect2" id="_cluster_startup"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Startup</span> <a title="Permalink" class="permalink" href="#_cluster_startup">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To start up your cluster again, first start your <code class="literal">etcd</code> leader and wait until you get status <code class="literal">Ready</code>, like this:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   NotReady   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   NotReady   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Start the rest of the <code class="literal">master</code> nodes, and wait for them to become <code class="literal">Ready</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   NotReady,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Start all the workers, wait until you see them on status <code class="literal">Ready,SchedulingDisabled</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   Ready,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   Ready,SchedulingDisabled   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Run the command <code class="literal">kubectl uncordon &lt;WORKER-NODE&gt;</code>, for each of the worker nodes, your cluster status should now be completely <code class="literal">Ready</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba cluster status

NAME       STATUS     ROLE     OS-IMAGE                              KERNEL-VERSION         KUBELET-VERSION   CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES   CAASP-RELEASE-VERSION
master01   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master02   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
master03   Ready   master   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker01   Ready   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5
worker02   Ready   &lt;none&gt;   SUSE Linux Enterprise Server 15 SP2   5.3.18-24.15-default   v1.18.6           cri-o://1.18.2      yes           yes                      4.5</pre></div></li><li class="listitem "><p>Bring back all your processes by scaling them up again:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale --replicas=N -f deployment.yaml</pre></div><p>or</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl scale deploy my-deployment --replicas=N</pre></div><div id="id-1.4.9.4.2.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Replace N with the number of replicas you want running.</p></div></li></ol></div></div></div><div class="sect1" id="_post_startup_activities"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post Startup Activities</span> <a title="Permalink" class="permalink" href="#_post_startup_activities">#</a></h2></div></div></div><p>Verify that all of your workloads and applications have resumed operation properly.</p></div></div><div class="chapter " id="_software_management"><div class="titlepage"><div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Management</span> <a title="Permalink" class="permalink" href="#_software_management">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#software-installation"><span class="number">3.1 </span><span class="name">Software Installation</span></a></span></dt></dl></div></div><div class="sect1" id="software-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Installation</span> <a title="Permalink" class="permalink" href="#software-installation">#</a></h2></div></div></div><p>Software can be installed in three basic layers</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.2.3.1"><span class="term ">Base OS layer</span></dt><dd><p>Linux RPM packages, Kernel etc.. Installation via AutoYaST,Terraform or {zypper}</p></dd><dt id="id-1.5.2.3.2"><span class="term ">Kubernetes Stack</span></dt><dd><p>Software that helps/controls execution of workloads in Kubernetes</p></dd><dt id="id-1.5.2.3.3"><span class="term ">Container image</span></dt><dd><p>Here it entirely depends on the actual makeup of the container what can be installed and how.
Please refer to your respecitve container image documentation for further details.</p></dd></dl></div><div id="id-1.5.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Installation of software in container images is beyond the scope of this document.</p></div><div class="sect2" id="_base_os"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Base OS</span> <a title="Permalink" class="permalink" href="#_base_os">#</a></h3></div></div></div><p>Applications that will be deployed to Kubernetes will typically contain all the required software to be executed.
In some cases, especially when it comes to the hardware layer abstraction (storage backends, GPU), additional packages
must be installed on the underlying operating system outside of Kubernetes.</p><div id="id-1.5.2.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The following examples show installation of required packages for <code class="literal">Ceph</code>, please adjust the list of
packages and repositories to whichever software you need to install.</p><p>While you can install any software package from the SLES ecosystem this falls outside of the support scope for SUSE CaaS Platform.</p></div><div class="sect3" id="_initial_rollout"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initial Rollout</span> <a title="Permalink" class="permalink" href="#_initial_rollout">#</a></h4></div></div></div><p>During the rollout of nodes you can use either AutoYaST or Terraform (depending on your chosen deployment type)
to automatically install packages to all nodes.</p><p>For example, to install additional packages required by the <code class="literal">Ceph</code> storage backend you can modify
your <code class="literal">autoyast.xml</code> or <code class="literal">tfvars.yml</code> files to include the additional repositories and instructions to
install <code class="literal">xfsprogs</code> and <code class="literal">ceph-common</code>.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><code class="literal">tfvars.yml</code></p><div class="verbatim-wrap highlight yaml"><pre class="screen"># EXAMPLE:
# repositories = {
#   repository1 = "http://example.my.repo.com/repository1/"
#   repository2 = "http://example.my.repo.com/repository2/"
# }
repositories = {
        ....
}

# Minimum required packages. Do not remove them.
# Feel free to add more packages
packages = [
  "kernel-default",
  "-kernel-default-base",
  "xfsprogs",
  "ceph-common"
]</pre></div></li><li class="listitem "><p><code class="literal">autoyast.xml</code></p><div class="verbatim-wrap highlight xml"><pre class="screen">&lt;!-- install required packages --&gt;
&lt;software&gt;
  &lt;image/&gt;
  &lt;products config:type="list"&gt;
    &lt;product&gt;SLES&lt;/product&gt;
  &lt;/products&gt;
  &lt;instsource/&gt;
  &lt;patterns config:type="list"&gt;
    &lt;pattern&gt;base&lt;/pattern&gt;
    &lt;pattern&gt;enhanced_base&lt;/pattern&gt;
    &lt;pattern&gt;minimal_base&lt;/pattern&gt;
    &lt;pattern&gt;basesystem&lt;/pattern&gt;
  &lt;/patterns&gt;
  &lt;packages config:type="list"&gt;
    &lt;package&gt;ceph-common&lt;/package&gt;
    &lt;package&gt;xfsprogs&lt;/package&gt;
  &lt;/packages&gt;
&lt;/software&gt;</pre></div></li></ol></div></div><div class="sect3" id="_existing_cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Existing Cluster</span> <a title="Permalink" class="permalink" href="#_existing_cluster">#</a></h4></div></div></div><p>To install software on existing cluster nodes, you must use <code class="literal">zypper</code> on each node individually.
Simply log in to a node via SSH and run:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper in ceph-common xfsprogs</pre></div></div></div><div class="sect2" id="_kubernetes_stack"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Kubernetes stack</span> <a title="Permalink" class="permalink" href="#_kubernetes_stack">#</a></h3></div></div></div><div class="sect3" id="helm-tiller-install"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Helm</span> <a title="Permalink" class="permalink" href="#helm-tiller-install">#</a></h4></div></div></div><p>As of SUSE CaaS Platform 4.5.1, Helm 2 is part of the SUSE CaaS Platform package repository, so to use this,
you only need to run the following command from the location where you normally run <code class="literal">skuba</code> commands:</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo zypper install helm</pre></div><p>Helm 2 is the default for SUSE CaaS Platform 5. Helm 3 is offered as an alternate tool and may be installed in parallel to aid migration.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo zypper install helm3
sudo update-alternatives --set helm /usr/bin/helm3</pre></div><div id="id-1.5.2.6.2.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Unless you are migrating from SUSE CaaS Platform 4.2 with Helm charts already deployed or have legacy Helm charts that only work with Helm 2, please use Helm 3.</p><p>Helm 2 is planned to end support in November 2020.
Helm 3 is offered as an alternative in SUSE CaaS Platform 4.5.0 and will become the default tool in the following release.
Please see <a class="xref" href="#helm-2to3-migration" title="3.1.2.3. Helm 2 to 3 Migration">Section 3.1.2.3, “Helm 2 to 3 Migration”</a> for upgrade instructions and upgrade as soon as feasible.</p></div></div><div class="sect3" id="_installing_tiller"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Tiller</span> <a title="Permalink" class="permalink" href="#_installing_tiller">#</a></h4></div></div></div><div id="id-1.5.2.6.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Tiller is only a requirement for Helm 2 and has been removed from Helm 3.  If using Helm 3, please skip this section.</p></div><p>As of SUSE CaaS Platform 4.5.1, Tiller is not part of the SUSE CaaS Platform package repository but it is available as a
helm chart from the chart repository. To install the Tiller server, choose either way to deploy the Tiller server:</p><div class="sect4" id="_unsecured_tiller_deployment"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unsecured Tiller Deployment</span> <a title="Permalink" class="permalink" href="#_unsecured_tiller_deployment">#</a></h5></div></div></div><p>This will install Tiller without additional certificate security.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create serviceaccount --namespace kube-system tiller

kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-image registry.suse.com/caasp/v4.5/helm-tiller:2.16.9 \
    --service-account tiller</pre></div></div><div class="sect4" id="_secured_tiller_deployment_with_tls_certificate"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Secured Tiller Deployment with TLS certificate</span> <a title="Permalink" class="permalink" href="#_secured_tiller_deployment_with_tls_certificate">#</a></h5></div></div></div><p>This installs tiller with TLS certificate security.</p><div class="sect5" id="_trusted_certificates"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.1.2.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trusted Certificates</span> <a title="Permalink" class="permalink" href="#_trusted_certificates">#</a></h6></div></div></div><p>Please reference to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> and <a class="xref" href="#trusted-client-certificate" title="6.9.9.1.2. Trusted Client Certificate">Section 6.9.9.1.2, “Trusted Client Certificate”</a> on how to sign the trusted tiller and helm certificate.
The server.conf for IP.1 is <code class="literal">127.0.0.1</code>.</p><p>Then, import trusted certificate to Kubernetes cluster. In this example, trusted certificate are <code class="literal">ca.crt</code>, <code class="literal">tiller.crt</code>, <code class="literal">tiller.key</code>, <code class="literal">helm.crt</code> and <code class="literal">helm.key</code>.</p></div><div class="sect5" id="_self_signed_certificates_optional"><div class="titlepage"><div><div><h6 class="title"><span class="number">3.1.2.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Self-signed Certificates (optional)</span> <a title="Permalink" class="permalink" href="#_self_signed_certificates_optional">#</a></h6></div></div></div><p>Please reference to <a class="xref" href="#self-signed-server-certificate" title="6.9.9.2.2. Self-signed Server Certificate">Section 6.9.9.2.2, “Self-signed Server Certificate”</a> and <a class="xref" href="#self-signed-client-certificate" title="6.9.9.2.3. Self-signed Client Certificate">Section 6.9.9.2.3, “Self-signed Client Certificate”</a> on how to sign the self-signed tiller and helm certificate.
The server.conf for IP.1 is <code class="literal">127.0.0.1</code>.</p><p>Then, import trusted certificate to Kubernetes cluster. In this example, trusted certificate are <code class="literal">ca.crt</code>, <code class="literal">tiller.crt</code>, <code class="literal">tiller.key</code>, <code class="literal">helm.crt</code> and <code class="literal">helm.key</code>.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Deploy Tiller server with TLS certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-tls \
    --tiller-tls-verify \
    --tiller-tls-cert tiller.crt \
    --tiller-tls-key tiller.key \
    --tls-ca-cert ca.crt \
    --tiller-image registry.suse.com/caasp/v4.5/helm-tiller:2.16.9 \
    --service-account tiller</pre></div></li><li class="listitem "><p>Configure Helm client with TLS certificate</p><p>Setup $HELM_HOME environment and copy the CA certificate, helm client certificate and key to the $HELM_HOME path.</p><div class="verbatim-wrap highlight bash"><pre class="screen">export HELM_HOME=&lt;path/to/helm/home&gt;

cp ca.crt $HELM_HOME/ca.pem
cp helm.crt $HELM_HOME/cert.pem
cp helm.key $HELM_HOME/key.pem</pre></div><p>Then, for helm commands, pass flag <code class="literal">--tls</code>. For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm ls --tls [flags]
helm install --tls &lt;CHART&gt; [flags]
helm upgrade --tls &lt;RELEASE_NAME&gt; &lt;CHART&gt; [flags]
helm del --tls &lt;RELEASE_NAME&gt; [flags]</pre></div></li></ol></div></div></div></div><div class="sect3" id="helm-2to3-migration"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Helm 2 to 3 Migration</span> <a title="Permalink" class="permalink" href="#helm-2to3-migration">#</a></h4></div></div></div><div id="id-1.5.2.6.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The process for migrating an installation from Helm 2 to Helm 3 has been documented and tested by the Helm community. Please reference the following links before proceeding.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://v3.helm.sh/docs/topics/v2_v3_migration/" target="_blank">https://v3.helm.sh/docs/topics/v2_v3_migration/</a></p></li><li class="listitem "><p><a class="link" href="https://helm.sh/blog/migrate-from-helm-v2-to-helm-v3/" target="_blank">https://helm.sh/blog/migrate-from-helm-v2-to-helm-v3/</a></p></li><li class="listitem "><p><a class="link" href="https://github.com/helm/helm-2to3" target="_blank">https://github.com/helm/helm-2to3</a></p></li></ul></div></div><div class="sect4" id="_preconditions"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preconditions</span> <a title="Permalink" class="permalink" href="#_preconditions">#</a></h5></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>A healthy SUSE CaaS Platform 4.5 installation with applications deployed using Helm 2 and Tiller.</p></li><li class="listitem "><p>A system, which <code class="literal">skuba</code> and <code class="literal">helm</code> version 2 have run on previously.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The procedure below requires an available internet connection to install the <code class="literal">2to3</code> plugin.  If the installation is in an air gapped environment, the system may need to be moved back out of the air gapped environment.</p></li></ul></div></li><li class="listitem "><p>These instructions are written for a single cluster managed from a single Helm 2 installation. If more than one cluster is being managed by this installation of Helm 2, please reference <a class="link" href="https://github.com/helm/helm-2to3" target="_blank">https://github.com/helm/helm-2to3</a> for further details and do not do the clean-up step until all clusters are migrated.</p></li></ul></div></div><div class="sect4" id="_migration_procedure"><div class="titlepage"><div><div><h5 class="title"><span class="number">3.1.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migration Procedure</span> <a title="Permalink" class="permalink" href="#_migration_procedure">#</a></h5></div></div></div><p>This is a procedure for migrating a SUSE CaaS Platform 4.5 deployment that has used Helm 2 to deploy applications.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install <code class="literal">helm3</code> package in the same location you normally run <code class="literal">skuba</code> commands (alongside the helm2 package):</p><div class="verbatim-wrap"><pre class="screen">sudo zypper in helm3</pre></div></li><li class="listitem "><p>Install the <code class="literal">2to3</code> plugin:</p><div class="verbatim-wrap"><pre class="screen">helm3 plugin install https://github.com/helm/helm-2to3.git</pre></div></li><li class="listitem "><p>Backup Helm 2 data found in the following:</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Helm 2 home folder.</p></li><li class="listitem "><p>Release data from the cluster. Refer to <a class="link" href="http://technosophos.com/2017/03/23/how-helm-uses-configmaps-to-store-data.html" target="_blank">How Helm Uses ConfigMaps to Store Data</a> for details on how Helm 2 stores release data in the cluster. This should apply similarly if Helm 2 is configured for secrets.</p></li></ol></div></li><li class="listitem "><p>Move configuration from 2 to 3:</p><div class="verbatim-wrap"><pre class="screen">helm3 2to3 move config</pre></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>After the move, if you have installed any custom plugins, then check that they work fine with Helm 3. If needed, remove and re-add them as described in <a class="link" href="https://github.com/helm/helm-2to3s" target="_blank">https://github.com/helm/helm-2to3s</a>.</p></li><li class="listitem "><p>If you have configured any local helm chart repositories, you will need to remove and re-add them.  For example:</p><div class="verbatim-wrap"><pre class="screen">helm3 repo remove &lt;my-custom-repo&gt;
helm3 repo add &lt;my-custom-repo&gt; &lt;url-to-custom-repo&gt;
helm3 repo update</pre></div></li></ol></div></li><li class="listitem "><p>Migrate Helm releases (deployed charts) in place:</p><div class="verbatim-wrap"><pre class="screen">helm3 2to3 convert RELEASE</pre></div></li><li class="listitem "><p>Clean up Helm 2 data:</p><div id="id-1.5.2.6.4.4.3.6.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Tiller will be cleaned up, and Helm 2 will not be usable on this cluster after cleanup.</p></div><div class="verbatim-wrap"><pre class="screen">helm3 2to3 cleanup</pre></div></li><li class="listitem "><p>You may now uninstall the <code class="literal">helm2</code> package and use the <code class="literal">helm</code> command line from the <code class="literal">helm3</code> package from now on.</p><div class="verbatim-wrap"><pre class="screen">sudo zypper remove helm2</pre></div></li></ol></div></div></div></div></div></div><div class="chapter " id="_cluster_updates"><div class="titlepage"><div><div><h1 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Updates</span> <a title="Permalink" class="permalink" href="#_cluster_updates">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#handling-updates"><span class="number">4.1 </span><span class="name">Update Requirements</span></a></span></dt><dt><span class="section"><a href="#_updating_kubernetes_components"><span class="number">4.2 </span><span class="name">Updating Kubernetes Components</span></a></span></dt><dt><span class="section"><a href="#_updating_nodes"><span class="number">4.3 </span><span class="name">Updating Nodes</span></a></span></dt><dt><span class="section"><a href="#base-os-updates"><span class="number">4.4 </span><span class="name">Base OS Updates</span></a></span></dt></dl></div></div><div class="sect1" id="handling-updates"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update Requirements</span> <a title="Permalink" class="permalink" href="#handling-updates">#</a></h2></div></div></div><div id="id-1.6.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Attempting a cluster update without updating the installed packages pattern on the management node, can lead to an incomplete or failed update.</p></div><p>Before updating a SUSE CaaS Platform cluster, it’s required to update packages installed by the <code class="literal">SUSE-CaaSP-Management</code> pattern on the management workstation.</p><p>The cluster update depends on updated skuba, but might also require new helm / Terraform or other dependencies which will be updated with the refreshed pattern.</p><p>Run <code class="literal">sudo zypper update</code> on the management workstation before any attempt to update the cluster.</p></div><div class="sect1" id="_updating_kubernetes_components"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating Kubernetes Components</span> <a title="Permalink" class="permalink" href="#_updating_kubernetes_components">#</a></h2></div></div></div><p>Updating of Kubernetes and its components from one minor version to the next (for example from 1.16 to 1.17) is handled by <code class="literal">skuba</code>.
The reason for this is that <span class="strong"><strong>minor updates</strong></span> require special plan and apply procedures.
These procedures differ for <span class="strong"><strong>patch updates</strong></span> (for example 1.16.1 to 1.16.2), which are handled by <code class="literal">skuba-update</code> as described in <a class="xref" href="#base-os-updates" title="4.4. Base OS Updates">Section 4.4, “Base OS Updates”</a>.</p><div id="id-1.6.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Generally speaking: If you have other deployments not installed via Kubernetes or helm, update them last in the upgrade process.</p><p>However, if your applications/deployments in their current versions are incompatible with the Kubernetes version that you are upgrading to,
you must update these applications/deployments to a compatible version before attempting a cluster upgrade.</p><p>Refer to the individual application/deployment for the requirements for Kubernetes version and dependencies.</p></div><p>The general procedure should look like this:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Check if all current versions of applications and deployments in the cluster will work on the new Kubernetes version you plan to install.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>If an application/deployment is incompatible with the new Kubernetes version, update the application/deployment before performing any of the other upgrade steps.</p></li></ul></div></li><li class="listitem "><p>Update the packages and reboot your management workstation to get all the latest changes to skuba, helm and their dependencies.</p></li><li class="listitem "><p>Run the commands on the management workstation.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">skuba addon refresh localconfig</code></p></li><li class="listitem "><p><code class="literal">skuba addon upgrade plan</code></p></li><li class="listitem "><p><code class="literal">skuba addon upgrade apply</code></p></li></ul></div></li><li class="listitem "><p>Apply all the configuration files that you modified for addons, the upgrade will have reset the configurations to defaults.</p></li><li class="listitem "><p>Check if there are addon upgrades available for the current cluster using <code class="literal">skuba addon upgrade plan</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Check if all the deployments in the cluster are compatible with the Kubernetes release that will be installed (refer to your individual deployments' documentation).</p></li><li class="listitem "><p>Check if the kustomize patches manifest is compatible for the current cluster, it will do Kubernetes server-side dry-run validation and displays the error message if present.
If the deployment is not compatible you must update it to ensure it working with the updated Kubernetes.</p></li></ul></div></li><li class="listitem "><p>Upgrade all master nodes by sequentially running <code class="literal">skuba node upgrade plan</code> and <code class="literal">skuba node upgrade apply</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Make sure to wait until all PODs/deployments/DaemonSets are up and running as expected before moving to the next node.</p></li></ul></div></li><li class="listitem "><p>Upgrade all worker nodes by sequentially running <code class="literal">skuba node upgrade plan</code> and <code class="literal">skuba node upgrade apply</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Make sure to wait until all PODs/deployments/DaemonSets are up and running as expected before moving to the next node.</p></li></ul></div></li><li class="listitem "><p>Check if new addons are available for the new version using <code class="literal">skuba addon upgrade plan</code> and do Kubernetes server-side dry-run validation to validates the addon base and patches manifest.</p></li><li class="listitem "><p>Once all nodes are up to date, update helm and tiller (as needed) and subsequently the helm deployments.</p></li></ol></div><div class="sect2" id="_update_management_workstation"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update Management Workstation</span> <a title="Permalink" class="permalink" href="#_update_management_workstation">#</a></h3></div></div></div><p>Run <code class="literal">sudo zypper up</code> on your management workstation to get the latest version of <code class="literal">skuba</code> and its dependencies.
Reboot the machine to make sure that all system changes are correctly applied.</p></div><div class="sect2" id="_generating_an_overview_of_available_platform_updates"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generating an Overview of Available Platform Updates</span> <a title="Permalink" class="permalink" href="#_generating_an_overview_of_available_platform_updates">#</a></h3></div></div></div><p>To get an overview of the addon updates available with validating the addon base and patches manifest before being applied to the current cluster, you can run:</p><div class="verbatim-wrap"><pre class="screen">skuba cluster upgrade plan</pre></div><p>This will show you a list of updates (if available) for different components
installed on the cluster. If the cluster is already running the latest available
versions, the output should look like this:</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.16.2
Latest Kubernetes version: 1.16.2

Congratulations! You are already at the latest version available</pre></div><p>If the cluster has a new patch-level or minor Kubernetes version available, the
output should look like this:</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.15.2
Latest Kubernetes version: 1.16.2

Upgrade path to update from 1.15.2 to 1.16.2:
 - 1.15.2 -&gt; 1.16.2</pre></div><p>Similarly, you can also fetch this information on a per-node basis with the following command:</p><div class="verbatim-wrap"><pre class="screen">skuba node upgrade plan &lt;NODE&gt;</pre></div><p>For example, if the cluster has a node named <code class="literal">worker0</code> which is running the latest available versions, the output should look like this:</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.16.2
Latest Kubernetes version: 1.16.2

Node worker0 is up to date</pre></div><p>On the other hand, if this same node has a new patch-level or minor Kubernetes version available, the output should look like this:</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.15.2
Latest Kubernetes version: 1.16.2

Current Node version: 1.15.2

Component versions in worker0
  - kubelet: 1.15.2 -&gt; 1.16.2
  - cri-o: 1.15.0 -&gt; 1.16.0</pre></div><p>You will get a similar output if there is a version available on a master node
(named <code class="literal">master0</code> in this example):</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.15.2
Latest Kubernetes version: 1.16.2

Current Node version: 1.15.2

Component versions in master0
  - apiserver: 1.15.2 -&gt; 1.16.2
  - controller-manager: 1.15.2 -&gt; 1.16.2
  - scheduler: 1.15.2 -&gt; 1.16.2
  - etcd: 3.3.11 -&gt; 3.3.15
  - kubelet: 1.15.2 -&gt; 1.16.2
  - cri-o: 1.15.0 -&gt; 1.16.0</pre></div><p>It may happen that the Kubernetes version on the control plane is too outdated
for the update to progress. In this case, you would get output similar to the following:</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.15.0
Latest Kubernetes version: 1.15.0

Unable to plan node upgrade: at least one control plane does not tolerate the current cluster version</pre></div><div id="id-1.6.3.7.18" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>The control plane consists of these components:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>apiserver</p></li><li class="listitem "><p>controller-manager</p></li><li class="listitem "><p>scheduler</p></li><li class="listitem "><p>etcd</p></li><li class="listitem "><p>kubelet</p></li><li class="listitem "><p>cri-o</p></li></ul></div></div></div><div class="sect2" id="_generating_an_overview_of_available_addon_updates"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generating an Overview of Available Addon Updates</span> <a title="Permalink" class="permalink" href="#_generating_an_overview_of_available_addon_updates">#</a></h3></div></div></div><div id="id-1.6.3.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Due to changes to the way <code class="literal">skuba</code> handles addons some existing components might be shown as <code class="literal">new addon</code> in the status output.
This is expected and no cause for concern. For any upgrade afterwards the addon will be considered known and only show available upgrades.</p></div><div id="id-1.6.3.8.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>SUSE CaaS Platform 4.2.1 provides the update of Cilium from 1.5.3 to 1.6.6.
The important change in Cilium 1.6 is usage of Kubernetes CRDs instead of etcd.
<code class="literal">skuba</code> performs and automated migration of data from etcd to CRDs.
If that migration is not successful, <code class="literal">skuba</code> shows the following warning:</p><p><span class="emphasis"><em>"Could not migrate data from etcd to CRD. Addons upgrade will be continued without it,
which will result in temporary connection loss for currently existing pods and services."</em></span></p><p>That warning means that Cilium is going to regenerate all internal data on the first run after upgrade.
It can result in temporary connection loss for pods and services which might take few minutes.</p></div><p>Each Kubernetes cluster version comes with different addons base manifests.
To update your local addons cluster folder definition in-sync with
current Kubernetes cluster version, please run:</p><div class="verbatim-wrap"><pre class="screen">skuba addon refresh localconfig</pre></div><p>To get an overview of the addon updates available with validating the addon base and patches manifest before being applied to the current cluster, you can run:</p><div class="verbatim-wrap"><pre class="screen">skuba addon upgrade plan</pre></div><p>This will show you a list of updates (if available) for different addons
installed on the cluster:</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.17.4
Latest Kubernetes version: 1.17.4

Addon upgrades for 1.17.4:
  - cilium: 1.5.3 -&gt; 1.6.6
  - dex: 2.16.0 (manifest version from 5 to 6)
  - gangway: 3.1.0-rev4 (manifest version from 4 to 5)
  - metrics-server: 0.3.6 (new addon)</pre></div><p>If the cluster is already running the latest available
versions, the output should look like this:</p><div class="verbatim-wrap"><pre class="screen">Current Kubernetes cluster version: 1.17.4
Latest Kubernetes version: 1.17.4

Congratulations! Addons are already at the latest version available</pre></div><p>Before updating the nodes you must apply the addon upgrades to your management workstation.
Please run:</p><div class="verbatim-wrap"><pre class="screen">skuba addon upgrade apply</pre></div></div></div><div class="sect1" id="_updating_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating Nodes</span> <a title="Permalink" class="permalink" href="#_updating_nodes">#</a></h2></div></div></div><div id="id-1.6.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>It is recommended to use a load balancer with active health checks and pool management that
will take care of adding/removing nodes to/from the pool during this process.</p></div><p>Updates have to be applied separately to each node, starting with the control
plane all the way down to the worker nodes.</p><p>Note that the upgrade via <code class="literal">skuba node upgrade apply</code> will:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Upgrade the containerized control plane.</p></li><li class="listitem "><p>Upgrade the rest of the Kubernetes system stack (<code class="literal">kubelet</code>, <code class="literal">cri-o</code>).</p></li><li class="listitem "><p>Restart services.</p></li></ul></div><p>During the upgrade to a newer version, the API server will be unavailable.</p><p>During the upgrade all the pods in the worker node will be restarted so it is
recommended to drain the pods if your application requires high availability.
In most cases, the restart is handled by <code class="literal">replicaSet</code>.</p><div class="sect2" id="_how_to_update_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How To Update Nodes</span> <a title="Permalink" class="permalink" href="#_how_to_update_nodes">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Upgrade the master nodes:</p><div class="verbatim-wrap"><pre class="screen">skuba node upgrade apply --target &lt;MASTER_NODE_IP&gt; --user &lt;USER&gt; --sudo</pre></div></li><li class="listitem "><p>When all master nodes are upgraded, upgrade the worker nodes as well:</p><div class="verbatim-wrap"><pre class="screen">skuba node upgrade apply --target &lt;WORKER_NODE_IP&gt; --user &lt;USER&gt; --sudo</pre></div></li><li class="listitem "><p>Verify that your cluster nodes are upgraded by running:</p><div class="verbatim-wrap"><pre class="screen">skuba cluster upgrade plan</pre></div></li></ol></div><div id="id-1.6.4.8.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>The upgrade via <code class="literal">skuba node upgrade apply</code> will:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>upgrade the containerized control plane.</p></li><li class="listitem "><p>upgrade the rest of the Kubernetes system stack (<code class="literal">kubelet</code>, <code class="literal">cri-o</code>).</p></li><li class="listitem "><p>temporarily drain/cordon the node before starting the whole process, and then undrain/uncordon the node after the upgrade has been successfully applied.</p></li><li class="listitem "><p>restart services.</p></li></ul></div></div></div><div class="sect2" id="_check_for_upgrades_to_new_version"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Check for Upgrades to New Version</span> <a title="Permalink" class="permalink" href="#_check_for_upgrades_to_new_version">#</a></h3></div></div></div><p>Once you have upgraded all nodes, please run <code class="literal">skuba cluster upgrade plan</code> again.
This will show if any upgrades are available that required the versions you just installed.
If there are upgrades available please repeat the procedure until no more new upgrades are shown.</p></div></div><div class="sect1" id="base-os-updates"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Base OS Updates</span> <a title="Permalink" class="permalink" href="#base-os-updates">#</a></h2></div></div></div><p>Base operating system updates are handled by <code class="literal">skuba-update</code>, which works together
with the <code class="literal">kured</code> reboot daemon.</p><div class="sect2" id="disabling-automatic-updates"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabling Automatic Updates</span> <a title="Permalink" class="permalink" href="#disabling-automatic-updates">#</a></h3></div></div></div><p>Nodes added to a cluster have the service <code class="literal">skuba-update.timer</code>, which is responsible for running automatic updates, activated by default.</p><p>This service calls the <code class="literal">skuba-update</code> utility and it can be configured with the <code class="literal">/etc/sysconfig/skuba-update</code> file.</p><div id="id-1.6.5.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: How skuba-update non-interactive mode works</h6><p><code class="literal">skuba-update</code> uses the flags <code class="literal">--non-interactive</code> and <code class="literal">--non-interactive-include-reboot-patches</code>. The <code class="literal">--non-interactive</code> flag causes zypper to use default answers to questions rather than prompting a user for answers.  In non-interactive mode, the <code class="literal">--non-interactive-include-reboot-patches</code> flag causes patches with the <code class="literal">rebootSuggested-flag</code> to not be skipped. Zypper does not perform the reboot directly. Instead, <code class="literal">kured</code> will be used to safely schedule reboots as needed.</p></div><p>To disable the automatic updates on a node, simply <code class="literal">ssh</code> to it and then configure the skuba-update service by editing the <code class="literal">/etc/sysconfig/skuba-update</code> file with the following runtime options:</p><div class="verbatim-wrap"><pre class="screen">## Path           : System/Management
## Description    : Extra switches for skuba-update
## Type           : string
## Default        : ""
## ServiceRestart : skuba-update
#
SKUBA_UPDATE_OPTIONS="--annotate-only"</pre></div><div id="id-1.6.5.3.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>It is not required to reload or restart <code class="literal">skuba-update.timer</code>.</p></div><p>The <code class="literal">--annotate-only</code> flag makes the <code class="literal">skuba-update</code> utility only check if updates are available and annotate the node accordingly.
When this flag is activated no updates are installed at all.</p><p>When OS updates are disabled, then you will have to manage OS updates manually. In order to do so, you will have to call <code class="literal">skuba-update</code> manually on each node.</p><div id="id-1.6.5.3.10" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Do not use <code class="literal">zypper up/zypper patch</code> commands as these do not manage the Kubernetes annotations used by <code class="literal">kured</code>.
If you perform a manual update using these commands you might render your cluster unusable.</p></div><p>After that, rebooting the node will depend on whether you have also disabled reboots or not. If you have disabled reboots for this node, then you will have to follow the instructions as given in <a class="xref" href="#_completely_disabling_reboots" title="4.4.2. Completely Disabling Reboots">Section 4.4.2, “Completely Disabling Reboots”</a>. Otherwise, you will have to wait until <code class="literal">kured</code> performs the reboot of the node</p></div><div class="sect2" id="_completely_disabling_reboots"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Completely Disabling Reboots</span> <a title="Permalink" class="permalink" href="#_completely_disabling_reboots">#</a></h3></div></div></div><p>If you would like to take care of reboots manually, either as a temporary measure or permanently, you can disable them by creating a lock:</p><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system annotate ds kured weave.works/kured-node-lock='{"nodeID":"manual"}'</pre></div><p>This command modifies an annotation (<code class="literal">annotate</code>) on the daemonset (<code class="literal">ds</code>) named <code class="literal">kured</code>.</p><p>When automatic reboots are disabled, you will have to manage reboots yourself.
In order to do this, you will have to follow some steps whenever you want to issue a reboot marker for a node.
First of all, you will have to <code class="literal">cordon</code> and <a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/" target="_blank"><code class="literal">drain</code></a> the node:</p><div class="verbatim-wrap"><pre class="screen">kubectl cordon &lt;NODE_ID&gt;
kubectl drain --force=true \
  --ignore-daemonsets=true \ <span id="CO1-1"></span><span class="callout">1</span>
  --delete-local-data=false \ <span id="CO1-2"></span><span class="callout">2</span>
  --grace-period 600 \ <span id="CO1-3"></span><span class="callout">3</span>
  --timeout=900s \ <span id="CO1-4"></span><span class="callout">4</span>
  &lt;NODE_ID&gt;</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Core components like <code class="literal">kured</code> and <code class="literal">cilium</code> are running as <code class="literal">DaemonSet</code> and draining those pods will fail if this is not set to <code class="literal">true</code>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Continues even if there are pods using <code class="literal">emptyDir</code> (local data that will be deleted when the node is drained; e.g: <code class="literal">metrics-server</code>).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Running applications will be notified of termination and given 10 minutes (<code class="literal">600</code> seconds) to safely store data.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Draining of the node will fail after 15 minutes (<code class="literal">900</code> seconds) have elapsed without success.</p></td></tr></table></div><div id="id-1.6.5.4.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Depending on your deployed applications, you must adjust the values for <code class="literal">--grace-period</code> and <code class="literal">--timeout</code> to grant the applications enough time to safely shut down without losing data.
The values here are meant to represent a conservative default for an application like SUSE Cloud Application Platform.</p><p>If you do not set these values, applications might never finish and draining of the pod will hang indefinitely.</p></div><p>Only then you will be able to manually <code class="literal">reboot</code> the node safely.</p><p>Once the node is back, remember to <code class="literal">uncordon</code> it so it is scheduleable again:</p><div class="verbatim-wrap"><pre class="screen">kubectl uncordon &lt;NODE_ID&gt;</pre></div><p>Perform the above steps first on control plane nodes, and afterwards on worker nodes.</p><div id="id-1.6.5.4.13" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>If the node that should be rebooted does not contain any workload you can skip the above steps and simply reboot the node.</p></div></div><div class="sect2" id="_manual_unlock"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manual Unlock</span> <a title="Permalink" class="permalink" href="#_manual_unlock">#</a></h3></div></div></div><p>In exceptional circumstances, such as a node experiencing a permanent failure whilst rebooting, manual intervention may be required to remove the cluster lock:</p><div class="verbatim-wrap"><pre class="screen">kubectl -n kube-system annotate ds kured weave.works/kured-node-lock-</pre></div><p>This command modifies an annotation (<code class="literal">annotate</code>) on the daemonset (<code class="literal">ds</code>) named <code class="literal">kured</code>.
It explicitly performs an "unset" (<code class="literal">-</code>) for the value for the annotation named <code class="literal">weave.works/kured-node-lock</code>.</p></div></div></div><div class="chapter " id="_upgrading_suse_caas_platform"><div class="titlepage"><div><div><h1 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrading SUSE CaaS Platform</span> <a title="Permalink" class="permalink" href="#_upgrading_suse_caas_platform">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#caasp-migration"><span class="number">5.1 </span><span class="name">Migration to SUSE CaaS Platform 4.5</span></a></span></dt></dl></div></div><div class="sect1" id="caasp-migration"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migration to SUSE CaaS Platform 4.5</span> <a title="Permalink" class="permalink" href="#caasp-migration">#</a></h2></div></div></div><div id="id-1.7.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Sequential Upgrade Required</h6><p>For a successful migration, make sure you are at the latest 4.2 version before migrating your cluster and management workstation to SUSE CaaS Platform 4.5.</p><p>For this, please follow the upgrade guide to update all your cluster nodes and management workstation to the latest base OS updates and SUSE CaaS Platform updates.
Refer to: <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_cluster_updates.html" target="_blank">https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_cluster_updates.html</a></p></div><div class="sect2" id="_updating_the_operating_system"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the operating system</span> <a title="Permalink" class="permalink" href="#_updating_the_operating_system">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The node should be able to communicate with the servers for SUSE Customer Center or Repository Mirroring Tool (RMT).
Other migration scenarios are covered in the SLES upgrade guide.</p><div id="id-1.7.2.3.2.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>In order to reconnect your system to the registration server, run:</p><div class="verbatim-wrap"><pre class="screen">SUSEConnect -r &lt;your SCC key&gt; SUSEConnect -p sle-module-containers/15.1/x86_64 -r &lt;your SCC key&gt;</pre></div></div></li><li class="listitem "><p>You also need the new <code class="literal">zypper migration</code> plugin.
This plugin is used to perform the needed tasks in order to migrate the node itself to the latest version of {platform}, such as updating the repositories to the new ones, and calling <code class="literal">zypper dup</code>.
This plugin is provided by the <code class="literal">zypper-migration-plugin</code> package.
Therefore, you need to install the <code class="literal">zypper-migration-plugin</code> package:</p><div class="verbatim-wrap"><pre class="screen">zypper -n in zypper-migration-plugin</pre></div></li><li class="listitem "><p>Then, run the newly installed <code class="literal">zypper-migration</code> plugin (on the management node first, then on the rest of the nodes):</p><div class="verbatim-wrap"><pre class="screen">zypper migration</pre></div><div id="id-1.7.2.3.2.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you want migration to progress non-interactive, you can add the flags: <code class="literal">--non-interactive --auto-agree-with-licenses</code></p></div></li><li class="listitem "><p>Check that all required repositories are enabled again and have the correct version. Run:</p><div class="verbatim-wrap"><pre class="screen">zypper lr -uE</pre></div><p>Verify that all repositories on the following list are present and enabled:</p><div id="id-1.7.2.3.2.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The actual Aliases might be different from the ones shown here if they were configured differently during the initial installation of SUSE Linux Enterprise.</p><p>The URIs will have long UUID strings (<code class="literal">update?&lt;UUID&gt;</code>,<code class="literal">product?&lt;UUID&gt;</code>) attached to them. The UUIDs identify your personal licensed product or update repositories.
These have been omitted from this output example.</p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="left" valign="top">Alias</th><th align="left" valign="top">URI</th></tr></thead><tbody><tr><td align="left" valign="top"><p>Basesystem_Module_15_SP2_x86_64:SLE-Module-Basesystem15-SP2-Pool</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Products/SLE-Module-Basesystem/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Products/SLE-Module-Basesystem/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>Basesystem_Module_15_SP2_x86_64:SLE-Module-Basesystem15-SP2-Updates</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Updates/SLE-Module-Basesystem/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Updates/SLE-Module-Basesystem/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>Containers_Module_15_SP2_x86_64:SLE-Module-Containers15-SP2-Pool</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Products/SLE-Module-Containers/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Products/SLE-Module-Containers/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>Containers_Module_15_SP2_x86_64:SLE-Module-Containers15-SP2-Updates</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Updates/SLE-Module-Containers/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Updates/SLE-Module-Containers/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>Python_2_Module_15_SP2_x86_64:SLE-Module-Python2-15-SP2-Pool</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Products/SLE-Module-Python2/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Products/SLE-Module-Python2/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>Python_2_Module_15_SP2_x86_64:SLE-Module-Python2-15-SP2-Updates</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Updates/SLE-Module-Python2/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Updates/SLE-Module-Python2/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>SUSE_CaaS_Platform_4.5_x86_64:SUSE-CAASP-4.5-Pool</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Products/SUSE-CAASP/4.5/x86_64/" target="_blank">https://updates.suse.com/SUSE/Products/SUSE-CAASP/4.5/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>SUSE_CaaS_Platform_4.5_x86_64:SUSE-CAASP-4.5-Updates</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Updates/SUSE-CAASP/4.5/x86_64/" target="_blank">https://updates.suse.com/SUSE/Updates/SUSE-CAASP/4.5/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>SUSE_Linux_Enterprise_Server_15_SP2_x86_64:SLE-Product-SLES15-SP2-Pool</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Products/SLE-Product-SLES/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Products/SLE-Product-SLES/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>SUSE_Linux_Enterprise_Server_15_SP2_x86_64:SLE-Product-SLES15-SP2-Updates</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Updates/SLE-Product-SLES/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Updates/SLE-Product-SLES/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>Server_Applications_Module_15_SP2_x86_64:SLE-Module-Server-Applications15-SP2-Pool</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Products/SLE-Module-Server-Applications/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Products/SLE-Module-Server-Applications/15-SP2/x86_64/</a></p></td></tr><tr><td align="left" valign="top"><p>Server_Applications_Module_15_SP2_x86_64:SLE-Module-Server-Applications15-SP2-Updates</p></td><td align="left" valign="top"><p><a class="link" href="https://updates.suse.com/SUSE/Updates/SLE-Module-Server-Applications/15-SP2/x86_64/" target="_blank">https://updates.suse.com/SUSE/Updates/SLE-Module-Server-Applications/15-SP2/x86_64/</a></p></td></tr></tbody></table></div></li><li class="listitem "><p>Check if <code class="literal">skuba</code> was indeed upgraded for 4.5:</p><div class="verbatim-wrap"><pre class="screen">skuba version</pre></div><div id="id-1.7.2.3.2.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The version must be &gt;= <code class="literal">skuba-2.1</code>.
<code class="literal">skuba 2</code> corresponds to SUSE CaaS Platform 4.5, while <code class="literal">skuba 1.0-1.4</code> corresponds to SUSE CaaS Platform 4.</p></div></li></ol></div></div><div class="sect2" id="_upgrade_the_cluster"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrade the cluster</span> <a title="Permalink" class="permalink" href="#_upgrade_the_cluster">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>And now run the skuba cluster upgrade commands as it’s done below.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>First, check if there are any addons or components to upgrade before you upgrade the nodes:</p><div class="verbatim-wrap"><pre class="screen">skuba cluster upgrade plan
skuba addon upgrade plan
skuba addon upgrade apply</pre></div></li><li class="listitem "><p>Then, check with <code class="literal">cluster status</code> if all nodes have the same Kubernetes version (which must be 1.17.x):</p><div class="verbatim-wrap"><pre class="screen">skuba cluster status</pre></div><div id="id-1.7.2.4.2.1.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If not all nodes are properly upgraded to the same Kubernetes version, then the ones with an older Kubernetes version must be upgraded before attempting a migration.
Refer to the update documentation of the previous version to bring all nodes to the latest update state.</p></div></li><li class="listitem "><p>Once all nodes have the same Kubernetes version, you must upgrade the CRI-O config:</p><div class="verbatim-wrap"><pre class="screen">skuba cluster upgrade localconfig</pre></div></li><li class="listitem "><p>Run <code class="literal">skuba node upgrade</code>:</p><div class="verbatim-wrap"><pre class="screen">skuba node upgrade apply --user sles --sudo --target &lt;IP of the node you’ve migrated&gt;</pre></div></li><li class="listitem "><p>Before repeating the same cycle with the rest of the nodes, <span class="strong"><strong>please make sure</strong></span> that all the components of the kubernetes stack <span class="strong"><strong>are running</strong></span> on the freshly upgraded node.
You can do this with the following command:</p><div class="verbatim-wrap"><pre class="screen">kubectl get all -n kube-system</pre></div></li></ul></div></li><li class="listitem "><p>Now repeat the above steps for all nodes to bring them to the upgraded state.</p></li><li class="listitem "><p>After upgrading all the nodes, make sure you run another addon upgrade across the cluster:</p><div class="verbatim-wrap"><pre class="screen">skuba addon upgrade plan
skuba addon upgrade apply</pre></div></li></ol></div><p>After following all these instructions you should be running SUSE CaaS Platform 4.5.
Refer to the <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-CAASP/4.5/" target="_blank">release notes</a> for further information on the new features that this release brings.
Enjoy!</p></div></div></div><div class="chapter " id="_security"><div class="titlepage"><div><div><h1 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Security</span> <a title="Permalink" class="permalink" href="#_security">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_network_access_considerations"><span class="number">6.1 </span><span class="name">Network Access Considerations</span></a></span></dt><dt><span class="section"><a href="#_access_control"><span class="number">6.2 </span><span class="name">Access Control</span></a></span></dt><dt><span class="section"><a href="#authentication"><span class="number">6.3 </span><span class="name">Authentication</span></a></span></dt><dt><span class="section"><a href="#sec-admin-security-users"><span class="number">6.4 </span><span class="name">Managing LDAP Users and Groups</span></a></span></dt><dt><span class="section"><a href="#rbac"><span class="number">6.5 </span><span class="name">Role-Based Access Control (RBAC)</span></a></span></dt><dt><span class="section"><a href="#admission"><span class="number">6.6 </span><span class="name">Admission Controllers</span></a></span></dt><dt><span class="section"><a href="#_pod_security_policies"><span class="number">6.7 </span><span class="name">Pod Security Policies</span></a></span></dt><dt><span class="section"><a href="#nginx-ingress"><span class="number">6.8 </span><span class="name">NGINX Ingress Controller</span></a></span></dt><dt><span class="section"><a href="#_certificates"><span class="number">6.9 </span><span class="name">Certificates</span></a></span></dt></dl></div></div><div class="sect1" id="_network_access_considerations"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Access Considerations</span> <a title="Permalink" class="permalink" href="#_network_access_considerations">#</a></h2></div></div></div><p>It is good security practice not to expose the kubernetes API server on the public internet.
Use network firewalls that only allow access from trusted subnets.</p></div><div class="sect1" id="_access_control"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Control</span> <a title="Permalink" class="permalink" href="#_access_control">#</a></h2></div></div></div><p>Users access the API using <code class="literal">kubectl</code>, client libraries, or by making REST requests.
Both human users and Kubernetes service accounts can be authorized for API access.
When a request reaches the API, it goes through several stages, that can be explained with the following three questions:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Authentication: <span class="strong"><strong>who are you?</strong></span> This is accomplished via <a class="xref" href="#authentication" title="6.3. Authentication">Section 6.3, “Authentication”</a> to validate the user’s entity and respond to the corresponding user group after successful login.</p></li><li class="listitem "><p>Authorization: <span class="strong"><strong>what kind of access do you have?</strong></span> This is accomplished via <a class="xref" href="#rbac" title="6.5. Role-Based Access Control (RBAC)">Section 6.5, “Role-Based Access Control (RBAC)”</a> API, that is a set of permissions for the previously authenticated user. Permissions are purely additive (there are no "deny" rules). A role can be defined within a namespace with a Role, or cluster-wide with a ClusterRole.</p></li><li class="listitem "><p>Admission Control: <span class="strong"><strong>what are you trying to do?</strong></span> This is accomplished via <a class="xref" href="#admission" title="6.6. Admission Controllers">Section 6.6, “Admission Controllers”</a>. They can modify (mutate) or validate (accept or reject) requests.</p></li></ol></div><p>Unlike authentication and authorization, if any admission controller rejects, then the request is immediately rejected.</p><p>Users can access with a Web browser or command line to do the authentication or self-configure <code class="literal">kubectl</code> to access authorized resources.</p><div class="sect2" id="_authentication_and_authorization_flow"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authentication and Authorization Flow</span> <a title="Permalink" class="permalink" href="#_authentication_and_authorization_flow">#</a></h3></div></div></div><p>Authentication is composed of:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>Dex</strong></span> (<a class="link" href="https://github.com/dexidp/dex" target="_blank">https://github.com/dexidp/dex</a>) is an identity provider service
(idP) that uses OIDC (Open ID Connect: <a class="link" href="https://openid.net/connect/" target="_blank">https://openid.net/connect/</a>)
to drive authentication for client applications.
It acts as a portal to defer authentication to the provider through connected
identity providers (connectors).</p></li><li class="listitem "><p><span class="strong"><strong>Client</strong></span>:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Web browser: <span class="strong"><strong>Gangway</strong></span> (<a class="link" href="https://github.com/heptiolabs/gangway" target="_blank">https://github.com/heptiolabs/gangway</a>):
a Web application that enables authentication flow for your SUSE CaaS Platform.
The user can log in, authorize access, download <code class="literal">kubeconfig</code>, or self-configure <code class="literal">kubectl</code>.</p></li><li class="listitem "><p>Command-line: <code class="literal">skuba auth login</code>, a CLI application that enables authentication flow for your SUSE CaaS Platform. The user can log in, authorize access, and get <code class="literal">kubeconfig</code>.</p></li></ol></div></li></ul></div><p>For authorization (Role-Based Access Control, RBAC), administrators can use <code class="literal">kubectl</code> to create corresponding
<code class="literal">RoleBinding</code> or <code class="literal">ClusterRoleBinding</code> for a user or group to limit resource access.</p><div class="sect3" id="_web_flow"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Web Flow</span> <a title="Permalink" class="permalink" href="#_web_flow">#</a></h4></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/oidc_flow_web.png" target="_blank"><img src="images/oidc_flow_web.png" width="" alt="oidc flow web" /></a></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The user requests access through Gangway.</p></li><li class="listitem "><p>Gangway redirects to Dex.</p></li><li class="listitem "><p>Dex redirects to a connected identity provider (connector).
User login and a request to approve access are generated.</p></li><li class="listitem "><p>Dex continues with OIDC authentication flow on behalf of the user
and creates/updates data to Kubernetes CRDs.</p></li><li class="listitem "><p>Dex redirects the user to Gangway.
This redirect includes (ID/refresh) tokens.</p></li><li class="listitem "><p>Gangway returns a link to download <code class="literal">kubeconfig</code> or self-configures <code class="literal">kubectl</code>
instructions to the user.</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/rbac-configure-kubectl.png" target="_blank"><img src="images/rbac-configure-kubectl.png" width="" alt="rbac configure kubectl" /></a></div></div></li><li class="listitem "><p>User downloads <code class="literal">kubeconf</code> or self-configures <code class="literal">kubectl</code>.</p></li><li class="listitem "><p>User uses <code class="literal">kubectl</code> to connect to the Kubernetes API server.</p></li><li class="listitem "><p>Kubernetes CRDs validate the Kubernetes API server request and return a response.</p></li><li class="listitem "><p>The <code class="literal">kubectl</code> connects to the authorized Kubernetes resources through the Kubernetes API server.</p></li></ol></div></div><div class="sect3" id="_cli_flow"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CLI Flow</span> <a title="Permalink" class="permalink" href="#_cli_flow">#</a></h4></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/oidc_flow_cli.png" target="_blank"><img src="images/oidc_flow_cli.png" width="" alt="oidc flow cli" /></a></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>User requests access through <code class="literal">skuba auth login</code> with the Dex server URL,
username and password.</p></li><li class="listitem "><p>Dex uses received username and password to log in and approve the access
request to the connected identity providers (connectors).</p></li><li class="listitem "><p>Dex continues with the OIDC authentication flow on behalf of the user and
creates/updates data to the Kubernetes CRDs.</p></li><li class="listitem "><p>Dex returns the ID token and refreshes token to <code class="literal">skuba auth login</code>.</p></li><li class="listitem "><p><code class="literal">skuba auth login</code> generates the kubeconfig file <code class="literal">kubeconf.txt</code>.</p></li><li class="listitem "><p>User uses <code class="literal">kubectl</code> to connect the Kubernetes API server.</p></li><li class="listitem "><p>Kubernetes CRDs validate the Kubernetes API server request and return a response.</p></li><li class="listitem "><p>The <code class="literal">kubectl</code> connects to the authorized Kubernetes resources through Kubernetes API server.</p></li></ol></div></div></div></div><div class="sect1" id="authentication"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authentication</span> <a title="Permalink" class="permalink" href="#authentication">#</a></h2></div></div></div><p>SUSE CaaS Platform supports user authentication via an external LDAP server like "389 Directory Server" (389-ds)
and "Active Directory" by updating the built-in Dex LDAP connector configuration, the administrators can update
LDAP identity providers before or after platform deployment.</p><div class="sect2" id="_deploying_an_external_ldap_server"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying an External LDAP Server</span> <a title="Permalink" class="permalink" href="#_deploying_an_external_ldap_server">#</a></h3></div></div></div><p>If you already have an existed LDAP server, you could skip this part.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Deploying an External 389 Directory Server</p><p>The 389 Directory Server image <code class="literal">registry.suse.com/caasp/v4.5/389-ds:1.4.3</code>
will <span class="strong"><strong>automatically generate a self-signed certificate</strong></span> and key.
The following instructions show how to deploy the "389 Directory Server"
with a customized configuration using container commands.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Prepare the customized 389 Directory configuration and enter it
into the terminal in the following format:</p><div class="verbatim-wrap"><pre class="screen">DS_DM_PASSWORD=                                 # Admin Password
DS_SUFFIX="dc=example,dc=org"                   # Domain Suffix
DATA_DIR=&lt;PWD&gt;/389_ds_data                      # Directory Server Data on Host Machine to Mount</pre></div></li><li class="listitem "><p>Execute the following command to deploy 389-ds in the same terminal.
This will start a non-TLS port (<code class="literal">389</code>) and a TLS port (<code class="literal">636</code>) together with an
automatically self-signed certificate and key.</p><div class="verbatim-wrap"><pre class="screen">docker run -d \
-p 389:3389 \
-p 636:3636 \
-e DS_DM_PASSWORD=&lt;DS_DM_PASSWORD&gt; \
-e DS_SUFFIX=&lt;DS_SUFFIX&gt; \
-v &lt;DATA_DIR&gt;:/data \
--name 389-ds registry.suse.com/caasp/v4.5/389-ds:1.4.3</pre></div></li></ol></div></li><li class="listitem "><p>Deploying an External 389 Directory Server with an External Certificate</p><p>To replace the automatically generated certificate with your own, follow these steps:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Stop the running container:</p><div class="verbatim-wrap"><pre class="screen">docker stop 389-ds</pre></div></li><li class="listitem "><p>Copy the external certificate <code class="literal">ca.cert</code> and <code class="literal">pwdfile.txt</code> to a mounted data directory <code class="literal">&lt;DATA_DIR&gt;/ssca/</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">ca.cert</code>: CA Certificate.</p></li><li class="listitem "><p><code class="literal">pwdfile.txt</code>: Password for the CA Certificate.</p></li></ul></div></li><li class="listitem "><p>Copy the external certificate <code class="literal">Server-Cert-Key.pem</code>, <code class="literal">Server-Cert.crt</code>, and <code class="literal">pwdfile-import.txt</code> to a mounted data directory <code class="literal">&lt;DATA_DIR&gt;/config/</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">Server-Cert-Key.pem</code>: PRIVATE KEY.</p></li><li class="listitem "><p><code class="literal">Server-Cert.crt</code>: CERTIFICATE.</p></li><li class="listitem "><p><code class="literal">pwdfile-import.txt</code>: Password for the PRIVATE KEY.</p></li></ul></div></li><li class="listitem "><p>Execute the following command to run the 389 Directory Server with a mounted data
the directory from the previous step:</p><div class="verbatim-wrap"><pre class="screen">docker start 389-ds</pre></div></li></ol></div></li></ul></div><div id="id-1.8.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Known Issues</h6><p>The error message is a warning for 389-ds version <code class="literal">1.4.3</code> when replacing external certificates.</p><div class="verbatim-wrap"><pre class="screen">ERR - attrcrypt_cipher_init - No symmetric key found for cipher AES in backend exampleDB, attempting to create one...
INFO - attrcrypt_cipher_init - Key for cipher AES successfully generated and stored
ERR - attrcrypt_cipher_init - No symmetric key found for cipher 3DES in backend exampleDB, attempting to create one...
INFO - attrcrypt_cipher_init - Key for cipher 3DES successfully generated and stored</pre></div><p>It is due to the encrypted key being stored in the <code class="literal">dse.ldif</code>.
When replacing the key and certificate in <code class="literal">/data/config</code>, 389ds searches <code class="literal">dse.ldif</code> for symmetric key and create one if it does not exist.
389-ds developers are planning a fix that switches 389-ds to use the <code class="literal">nssdb</code> exclusively.</p></div></div><div class="sect2" id="_examples_of_usage"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examples of Usage</span> <a title="Permalink" class="permalink" href="#_examples_of_usage">#</a></h3></div></div></div><p>In both directories, <code class="literal">user-regular1</code> and <code class="literal">user-regular2</code> are members of the <code class="literal">k8s-users</code> group,
and <code class="literal">user-admin</code> is a member of the <code class="literal">k8s-admins</code> group.</p><p>In Active Directory, <code class="literal">user-bind</code> is a simple user that is a member of the default Domain Users group.
Hence, we can use it to authenticate, because it has read-only access to Active Directory.
The mail attribute is used to create the RBAC rules.</p><div class="sect3" id="_389_directory_server"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">389 Directory Server:</span> <a title="Permalink" class="permalink" href="#_389_directory_server">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Example LDIF configuration to initialize LDAP using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: dc=example,dc=org
objectClass: top
objectClass: domain
dc: example</pre></div><div class="verbatim-wrap"><pre class="screen">dn: cn=Directory Administrators,dc=example,dc=org
objectClass: top
objectClass: groupofuniquenames
cn: Directory Administrators
uniqueMember: cn=Directory Manager</pre></div><div class="verbatim-wrap"><pre class="screen">dn: ou=Groups,dc=example,dc=org
objectClass: top
objectClass: organizationalunit
ou: Groups</pre></div><div class="verbatim-wrap"><pre class="screen">dn: ou=People,dc=example,dc=org
objectClass: top
objectClass: organizationalunit
ou: People</pre></div><div class="verbatim-wrap"><pre class="screen">dn: ou=Users,dc=example,dc=org
objectclass: top
objectclass: organizationalUnit
ou: Users</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to configure ACL using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: dc=example,dc=org
changetype: modify
add: aci
aci: (targetattr!="userPassword || aci")(version 3.0; acl "Enable anonymous access"; allow (read, search, compare) userdn="ldap:///anyone";)
aci: (targetattr="carLicense || description || displayName || facsimileTelephoneNumber || homePhone || homePostalAddress || initials || jpegPhoto || labeledURI || mail || mobile || pager || photo || postOfficeBox || postalAddress || postalCode || preferredDeliveryMethod || preferredLanguage || registeredAddress || roomNumber || secretary || seeAlso || st || street || telephoneNumber || telexNumber || title || userCertificate || userPassword || userSMIMECertificate || x500UniqueIdentifier")(version 3.0; acl "Enable self write for common attributes"; allow (write) userdn="ldap:///self";)
aci: (targetattr ="*")(version 3.0;acl "Directory Administrators Group";allow (all) (groupdn = "ldap:///cn=Directory Administrators, dc=example,dc=org");)</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to create user <code class="literal">user-regular1</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: uid=user-regular1,ou=Users,dc=example,dc=org
changetype: add
uid: user-regular1
userPassword: SSHA_PASSWORD <span id="CO2-1"></span><span class="callout">1</span>
objectClass: posixaccount
objectClass: inetOrgPerson
objectClass: person
objectClass: inetUser
objectClass: organizationalPerson
uidNumber: 1200
gidNumber: 500
givenName: User
mail: user-regular1@example.org
sn: Regular1
homeDirectory: /home/regular1
cn: User Regular1</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO2-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>SSHA_PASSWORD: The user’s new hashed password.</p></td></tr></table></div><p>Use <code class="literal">/usr/sbin/slappasswd</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/slappasswd -h {SSHA} -s &lt;USER_PASSWORD&gt;</pre></div><p>Use <code class="literal">/usr/bin/pwdhash</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/bin/pwdhash -s SSHA &lt;USER_PASSWORD&gt;</pre></div></li><li class="listitem "><p>Example LDIF configuration to create user <code class="literal">user-regular2</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: uid=user-regular2,ou=Users,dc=example,dc=org
changetype: add
uid: user-regular2
userPassword: SSHA_PASSWORD <span id="CO3-1"></span><span class="callout">1</span>
objectClass: posixaccount
objectClass: inetOrgPerson
objectClass: person
objectClass: inetUser
objectClass: organizationalPerson
uidNumber: 1300
gidNumber: 500
givenName: User
mail: user-regular2@example.org
sn: Regular1
homeDirectory: /home/regular2
cn: User Regular2</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO3-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>SSHA_PASSWORD: The user’s new hashed password.</p></td></tr></table></div><p>Use <code class="literal">/usr/sbin/slappasswd</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/slappasswd -h {SSHA} -s &lt;USER_PASSWORD&gt;</pre></div><p>Use <code class="literal">/usr/bin/pwdhash</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/bin/pwdhash -s SSHA &lt;USER_PASSWORD&gt;</pre></div></li><li class="listitem "><p>Example LDIF configuration to create user <code class="literal">user-admin</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: uid=user-admin,ou=Users,dc=example,dc=org
changetype: add
uid: user-admin
userPassword: SSHA_PASSWORD <span id="CO4-1"></span><span class="callout">1</span>
objectClass: posixaccount
objectClass: inetOrgPerson
objectClass: person
objectClass: inetUser
objectClass: organizationalPerson
uidNumber: 1000
gidNumber: 100
givenName: User
mail: user-admin@example.org
sn: Admin
homeDirectory: /home/admin
cn: User Admin</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO4-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>SSHA_PASSWORD: The user’s new hashed password.</p></td></tr></table></div><p>Use <code class="literal">/usr/sbin/slappasswd</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/slappasswd -h {SSHA} -s &lt;USER_PASSWORD&gt;</pre></div><p>Use <code class="literal">/usr/bin/pwdhash</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/bin/pwdhash -s SSHA &lt;USER_PASSWORD&gt;</pre></div></li><li class="listitem "><p>Example LDIF configuration to create group <code class="literal">k8s-users</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=k8s-users,ou=Groups,dc=example,dc=org
changetype: add
gidNumber: 500
objectClass: groupOfNames
objectClass: posixGroup
cn: k8s-users
ou: Groups
memberUid: user-regular1
memberUid: user-regular2</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to create group <code class="literal">k8s-admins</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=k8s-admins,ou=Groups,dc=example,dc=org
changetype: add
gidNumber: 100
objectClass: groupOfNames
objectClass: posixGroup
cn: k8s-admins
ou: Groups
memberUid: user-admin</pre></div></div></li></ul></div></div><div class="sect3" id="_active_directory"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Active Directory</span> <a title="Permalink" class="permalink" href="#_active_directory">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Example LDIF configuration to create user <code class="literal">user-regular1</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=user-regular1,ou=Users,dc=example,dc=org
objectClass: top
objectClass: person
objectClass: organizationalPerson
objectClass: user
cn: user-regular1
sn: Regular1
givenName: User
distinguishedName: cn=user-regular1,ou=Users,dc=example,dc=org
displayName: User Regular1
memberOf: cn=Domain Users,ou=Users,dc=example,dc=org
memberOf: cn=k8s-users,ou=Groups,dc=example,dc=org
name: user-regular1
sAMAccountName: user-regular1
objectCategory: cn=Person,cn=Schema,cn=Configuration,dc=example,dc=org
mail: user-regular1@example.org</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to create user <code class="literal">user-regular2</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=user-regular2,ou=Users,dc=example,dc=org
objectClass: top
objectClass: person
objectClass: organizationalPerson
objectClass: user
cn: user-regular2
sn: Regular2
givenName: User
distinguishedName: cn=user-regular2,ou=Users,dc=example,dc=org
displayName: User Regular2
memberOf: cn=Domain Users,ou=Users,dc=example,dc=org
memberOf: cn=k8s-users,ou=Groups,dc=example,dc=org
name: user-regular2
sAMAccountName: user-regular2
objectCategory: cn=Person,cn=Schema,cn=Configuration,dc=example,dc=org
mail: user-regular2@example.org</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to create user <code class="literal">user-bind</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=user-bind,ou=Users,dc=example,dc=org
objectClass: top
objectClass: person
objectClass: organizationalPerson
objectClass: user
cn: user-bind
sn: Bind
givenName: User
distinguishedName: cn=user-bind,ou=Users,dc=example,dc=org
displayName: User Bind
memberOf: cn=Domain Users,ou=Users,dc=example,dc=org
name: user-bind
sAMAccountName: user-bind
objectCategory: cn=Person,cn=Schema,cn=Configuration,dc=example,dc=org
mail: user-bind@example.org</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to create user <code class="literal">user-admin</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=user-admin,ou=Users,dc=example,dc=org
objectClass: top
objectClass: person
objectClass: organizationalPerson
objectClass: user
cn: user-admin
sn: Admin
givenName: User
distinguishedName: cn=user-admin,ou=Users,dc=example,dc=org
displayName: User Admin
memberOf: cn=Domain Users,ou=Users,dc=example,dc=org
memberOf: cn=k8s-admins,ou=Groups,dc=example,dc=org
name: user-admin
sAMAccountName: user-admin
objectCategory: cn=Person,cn=Schema,cn=Configuration,dc=example,dc=org
mail: user-admin@example.org</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to create group <code class="literal">k8s-users</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=k8s-users,ou=Groups,dc=example,dc=org
objectClass: top
objectClass: group
cn: k8s-users
member: cn=user-regular1,ou=Users,dc=example,dc=org
member: cn=user-regular2,ou=Users,dc=example,dc=org
distinguishedName: cn=k8s-users,ou=Groups,dc=example,dc=org
name: k8s-users
sAMAccountName: k8s-users
objectCategory: cn=Group,cn=Schema,cn=Configuration,dc=example,dc=org</pre></div></div></li><li class="listitem "><p>Example LDIF configuration to create group <code class="literal">k8s-admins</code> using an LDAP command:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">dn: cn=k8s-admins,ou=Groups,dc=example,dc=org
objectClass: top
objectClass: group
cn: k8s-admins
member: cn=user-admin,ou=Users,dc=example,dc=org
distinguishedName: cn=k8s-admins,ou=Groups,dc=example,dc=org
name: k8s-admins
sAMAccountName: k8s-admins
objectCategory: cn=Group,cn=Schema,cn=Configuration,dc=example,dc=org</pre></div></div></li></ul></div></div></div></div><div class="sect1" id="sec-admin-security-users"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing LDAP Users and Groups</span> <a title="Permalink" class="permalink" href="#sec-admin-security-users">#</a></h2></div></div></div><p>You can use standard LDAP administration tools for managing organizations, groups and users remotely.
To do so, install the <code class="literal">openldap2-client</code> package on a computer in your network
and make sure that the computer can connect to the LDAP server
(389 Directory Server) on port <code class="literal">389</code> or secure port <code class="literal">636</code>.</p><div class="sect2" id="_adding_a_new_organizational_unit"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New Organizational Unit</span> <a title="Permalink" class="permalink" href="#_adding_a_new_organizational_unit">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To add a new organizational unit, create an LDIF file (<code class="literal">create_ou_groups.ldif</code>) like this:</p><div class="verbatim-wrap"><pre class="screen">dn: ou=OU_NAME,dc=example,dc=org
changetype: add
objectclass: top
objectclass: organizationalUnit
ou: OU_NAME</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Substitute OU_NAME with an organizational unit name of your choice.</p></li></ul></div></li><li class="listitem "><p>Run <code class="literal">ldapmodify</code> to add the new organizational unit:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
BIND_DN="cn=Directory Manager"                  # Admin User
LDIF_FILE=./create_ou_groups.ldif               # LDIF Configuration File
DS_DM_PASSWORD=                                 # Admin Password

ldapmodify -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -D "&lt;BIND_DN&gt;" -f &lt;LDIF_FILE&gt; -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div></div><div class="sect2" id="_removing_an_organizational_unit"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing an Organizational Unit</span> <a title="Permalink" class="permalink" href="#_removing_an_organizational_unit">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To remove an organizational unit, create an LDIF file (<code class="literal">delete_ou_groups.ldif</code>) like this:</p><div class="verbatim-wrap"><pre class="screen">dn: ou=OU_NAME,dc=example,dc=org
changetype: delete</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Substitute OU_NAME with the name of the organizational unit you would like to remove.</p></li></ul></div></li><li class="listitem "><p>Execute <code class="literal">ldapmodify</code> to remove the organizational unit:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
BIND_DN="cn=Directory Manager"                  # Admin User
LDIF_FILE=./delete_ou_groups.ldif               # LDIF Configuration File
DS_DM_PASSWORD=                                 # Admin Password

ldapmodify -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -D "&lt;BIND_DN&gt;" -f &lt;LDIF_FILE&gt; -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div></div><div class="sect2" id="_adding_a_new_group_to_an_organizational_unit"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New Group to an Organizational Unit</span> <a title="Permalink" class="permalink" href="#_adding_a_new_group_to_an_organizational_unit">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To add a new group to an organizational unit, create an LDIF file (<code class="literal">create_groups.ldif</code>) like this:</p><div class="verbatim-wrap"><pre class="screen">dn: cn=GROUP,ou=OU_NAME,dc=example,dc=org
changetype: add
objectClass: top
objectClass: groupOfNames
cn: GROUP</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>GROUP: Group name</p></li><li class="listitem "><p>OU_NAME: Organizational unit name</p></li></ul></div></li><li class="listitem "><p>Run <code class="literal">ldapmodify</code> to add the new group to the organizational unit:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
BIND_DN="cn=Directory Manager"                  # Admin User
LDIF_FILE=./create_groups.ldif                  # LDIF Configuration File
DS_DM_PASSWORD=                                 # Admin Password

ldapmodify -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -D "&lt;BIND_DN&gt;" -f &lt;LDIF_FILE&gt; -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div></div><div class="sect2" id="_removing_a_group_from_an_organizational_unit"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Group from an Organizational Unit</span> <a title="Permalink" class="permalink" href="#_removing_a_group_from_an_organizational_unit">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To remove a group from an organizational unit, create an LDIF file (<code class="literal">delete_ou_groups.ldif</code>) like this:</p><div class="verbatim-wrap"><pre class="screen">dn: cn=GROUP,ou=OU_NAME,dc=example,dc=org
changetype: delete</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>GROUP: Group name</p></li><li class="listitem "><p>OU_NAME: organizational unit name</p></li></ul></div></li><li class="listitem "><p>Execute <code class="literal">ldapmodify</code> to remove the group from the organizational unit:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
BIND_DN="cn=Directory Manager"                  # Admin User
LDIF_FILE=./delete_ou_groups.ldif               # LDIF Configuration File
DS_DM_PASSWORD=                                 # Admin Password

ldapmodify -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -D "&lt;BIND_DN&gt;" -f &lt;LDIF_FILE&gt; -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div><div class="sect3" id="sec-admin-security-users-add"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.4.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New User</span> <a title="Permalink" class="permalink" href="#sec-admin-security-users-add">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To add a new user, create an LDIF file (<code class="literal">new_user.ldif</code>) like this:</p><div class="verbatim-wrap"><pre class="screen">dn: uid=USERID,ou=OU_NAME,dc=example,dc=org
objectClass: person
objectClass: inetOrgPerson
objectClass: top
uid: USERID
userPassword: PASSWORD_HASH
givenname: FIRST_NAME
sn: SURNAME
cn: FULL_NAME
mail: E-MAIL_ADDRESS</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>USERID: User ID (UID) of the new user. This value must be a unique number.</p></li><li class="listitem "><p>OU_NAME: organizational unit name</p></li><li class="listitem "><p>PASSWORD_HASH: The user’s hashed password.SSHA_PASSWORD: The user’s new hashed password.</p><p>Use <code class="literal">/usr/sbin/slappasswd</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/slappasswd -h {SSHA} -s &lt;USER_PASSWORD&gt;</pre></div><p>Use <code class="literal">/usr/bin/pwdhash</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/bin/pwdhash -s SSHA &lt;USER_PASSWORD&gt;</pre></div></li><li class="listitem "><p>FIRST_NAME: The user’s first name</p></li><li class="listitem "><p>SURNAME: The user’s last name</p></li><li class="listitem "><p>FULL_NAME: The user’s full name</p></li><li class="listitem "><p>E-MAIL_ADDRESS: The user’s e-mail address</p></li></ul></div></li><li class="listitem "><p>Execute <code class="literal">ldapadd</code> to add the new user:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
BIND_DN="cn=Directory Manager"                  # Admin User
LDIF_FILE=./new_user.ldif                       # LDIF Configuration File
DS_DM_PASSWORD=                                 # Admin Password

ldapadd -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -D
"&lt;BIND_DN&gt;" -f &lt;LDIF_FILE&gt; -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div></div><div class="sect3" id="_showing_user_attributes"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.4.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Showing User Attributes</span> <a title="Permalink" class="permalink" href="#_showing_user_attributes">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To show the attributes of a user, use the <code class="literal">ldapsearch</code> command:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
USERID=user1
BASE_DN="uid=&lt;USERID&gt;,dc=example,dc=org"
BIND_DN="cn=Directory Manager"                  # Admin User
DS_DM_PASSWORD=                                 # Admin Password

ldapsearch -v -x -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -b
"&lt;BASE_DN&gt;" -D "&lt;BIND_DN&gt;" -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div></div><div class="sect3" id="_modifying_a_user"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.4.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying a User</span> <a title="Permalink" class="permalink" href="#_modifying_a_user">#</a></h4></div></div></div><p>The following procedure shows how to modify a user in the LDAP server.
See the LDIF files for examples of how to change rootdn password, a user password and add a user to the
<code class="literal">Administrators</code> group.
To modify other fields, you can use the password example, replacing <code class="literal">userPassword</code>
with other field names you want to change.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create an LDIF file (<code class="literal">modify_rootdn.ldif</code>), which contains the change to the LDAP server:</p><div class="verbatim-wrap"><pre class="screen">dn: cn=config
changetype: modify
replace: nsslapd-rootpw
nsslapd-rootpw: NEW_PASSWORD</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>NEW_PASSWORD: The user’s new hashed password.</p><p>Use <code class="literal">/usr/sbin/slappasswd</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/slappasswd -h {SSHA} -s &lt;USER_PASSWORD&gt;</pre></div><p>Use <code class="literal">/usr/bin/pwdhash</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/bin/pwdhash -s SSHA &lt;USER_PASSWORD&gt;</pre></div></li></ul></div></li><li class="listitem "><p>Create an LDIF file (<code class="literal">modify_user.ldif</code>), which contains the change to the LDAP server:</p><div class="verbatim-wrap"><pre class="screen">dn: uid=USERID,ou=OU_NAME,dc=example,dc=org
changetype: modify
replace: userPassword
userPassword: NEW_PASSWORD</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>USERID: The desired user’s ID</p></li><li class="listitem "><p>OU_NAME: organizational unit name</p></li><li class="listitem "><p>NEW_PASSWORD: The user’s new hashed password.</p><p>Use <code class="literal">/usr/sbin/slappasswd</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/slappasswd -h {SSHA} -s &lt;USER_PASSWORD&gt;</pre></div><p>Use <code class="literal">/usr/bin/pwdhash</code> to generate the SSHA hash.</p><div class="verbatim-wrap"><pre class="screen">/usr/bin/pwdhash -s SSHA &lt;USER_PASSWORD&gt;</pre></div></li></ul></div></li><li class="listitem "><p>Add the user to the <code class="literal">Administrators</code> group:</p><div class="verbatim-wrap"><pre class="screen">dn: cn=Administrators,ou=Groups,dc=example,dc=org
changetype: modify
add: uniqueMember
uniqueMember: uid=USERID,ou=OU_NAME,dc=example,dc=org</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>USERID: Substitute with the user’s ID.</p></li><li class="listitem "><p>OU_NAME: organizational unit name</p></li></ul></div></li><li class="listitem "><p>Execute <code class="literal">ldapmodify</code> to change user attributes:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
BIND_DN="cn=Directory Manager"                  # Admin User
LDIF_FILE=./modify_user.ldif                    # LDIF Configuration File
DS_DM_PASSWORD=                                 # Admin Password

ldapmodify -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -D
"&lt;BIND_DN&gt;" -f &lt;LDIF_FILE&gt; -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div></div><div class="sect3" id="_deleting_a_user"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.4.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deleting a User</span> <a title="Permalink" class="permalink" href="#_deleting_a_user">#</a></h4></div></div></div><p>To delete a user from the LDAP server, follow these steps:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create an LDIF file (<code class="literal">delete_user.ldif</code>) that specifies the name of the entry:</p><div class="verbatim-wrap"><pre class="screen">dn: uid=USER_ID,ou=OU_NAME,dc=example,dc=org
changetype: delete</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>USERID: Substitute this with the user’s ID.</p></li><li class="listitem "><p>OU_NAME: organizational unit name</p></li></ul></div></li><li class="listitem "><p>Run <code class="literal">ldapmodify</code> to delete the user:</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                              # ldap, ldaps
LDAP_NODE_FQDN=localhost                        # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                         # Non-TLS (:389), TLS (:636)
BIND_DN="cn=Directory Manager"                  # Admin User
LDIF_FILE=./delete_user.ldif                    # LDIF Configuration File
DS_DM_PASSWORD=                                 # Admin Password

ldapmodify -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt; -D "&lt;BIND_DN&gt;" -f &lt;LDIF_FILE&gt; -w &lt;DS_DM_PASSWORD&gt;</pre></div></li></ol></div></div><div class="sect3" id="_changing_your_own_ldap_password_from_cli"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.4.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing Your own LDAP Password from CLI</span> <a title="Permalink" class="permalink" href="#_changing_your_own_ldap_password_from_cli">#</a></h4></div></div></div><p>To perform a change to your own user password from CLI.</p><div class="verbatim-wrap"><pre class="screen">LDAP_PROTOCOL=ldap                                  # ldap, ldaps
LDAP_NODE_FQDN=localhost                            # FQDN of 389 Directory Server
LDAP_NODE_PROTOCOL=:389                             # Non-TLS (:389), TLS (:636)
BIND_DN=                                            # User's binding dn
DS_DM_PASSWORD=                                     # Old Password
NEW_DS_DM_PASSWORD=                                 # New Password

ldappasswd -v -H &lt;LDAP_PROTOCOL&gt;://&lt;LDAP_NODE_FQDN&gt;&lt;LDAP_NODE_PROTOCOL&gt;  -x -D "&lt;BIND_DN&gt;" -w &lt;DS_DM_PASSWORD&gt; -a &lt;DS_DM_PASSWORD&gt; -s &lt;NEW_DS_DM_PASSWORD&gt;</pre></div></div></div><div class="sect2" id="configure-auth-connector"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Authentication Connector</span> <a title="Permalink" class="permalink" href="#configure-auth-connector">#</a></h3></div></div></div><p>Administrators can update the authentication connector settings before or after SUSE CaaS Platform
deployment as follows:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Based on the manifest in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/base/dex.yaml</code>, provide a kustomize patch to <code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/patches/custom.yaml</code> of the form of strategic merge patch or a JSON 6902 patch.</p><p>Read <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge</a> and <a class="link" href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902" target="_blank">https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902</a> to get more information.</p></li><li class="listitem "><p>Adapt ConfigMap by adding LDAP configuration to the connector section.
For detailed configuration of the LDAP connector, refer to the Dex documentation <a class="link" href="https://github.com/dexidp/dex/blob/v2.23.0/Documentation/connectors/ldap.md" target="_blank">https://github.com/dexidp/dex/blob/v2.23.0/Documentation/connectors/ldap.md</a>.</p><div id="id-1.8.5.7.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Besides the LDAP connector, you can also set up other connectors.
For additional connectors, refer to the available connector configurations in the Dex repository:
<a class="link" href="https://github.com/dexidp/dex/tree/v2.23.0/Documentation/connectors" target="_blank">https://github.com/dexidp/dex/tree/v2.23.0/Documentation/connectors</a>.</p></div><div id="id-1.8.5.7.3.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Base64 ecoded PEM file</h6><p>A base64 encoded PEM file can be generated by running:</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;ROOT_CA_PEM_FILE&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</pre></div></div><p>Examples of Usage (<code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/patches/custom.yaml</code>):</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>LDAP 389-DS TLS Connector:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ConfigMap
metadata:
  name: oidc-dex-config
  namespace: kube-system
data:
  config.yaml: |
    connectors:
    - type: ldap
      # Required field for connector id.
      id: 389ds
      # Required field for connector name.
      name: 389ds
      config:
        # Host and optional port of the LDAP server in the form "host:port".
        # If the port is not supplied, it will be guessed based on "insecureNoSSL",
        # and "startTLS" flags. 389 for insecure or StartTLS connections, 636
        # otherwise.
        host: ldap.example.org:636

        # The following field is required if the LDAP host is not using TLS (port 389).
        # Because this option inherently leaks passwords to anyone on the same network
        # as dex, THIS OPTION MAY BE REMOVED WITHOUT WARNING IN A FUTURE RELEASE.
        #
        # insecureNoSSL: true

        # If a custom certificate isn't provided, this option can be used to turn on
        # TLS certificate checks. As noted, it is insecure and shouldn't be used outside
        # of explorative phases.
        #
        insecureSkipVerify: true

        # When connecting to the server, connect using the ldap:// protocol then issue
        # a StartTLS command. If unspecified, connections will use the ldaps:// protocol
        #
        # startTLS: true

        # Path to a trusted root certificate file. Default: use the host's root CA.
        # rootCA: /etc/dex/pki/ca.crt

        # A raw certificate file can also be provided inline.
        rootCAData: &lt;BASE64_ENCODED_PEM_FILE&gt;

        # The DN and password for an application service account. The connector uses
        # these credentials to search for users and groups. Not required if the LDAP
        # server provides access for anonymous auth.
        # Please note that if the bind password contains a `$`, it has to be saved in an
        # environment variable which should be given as the value to `bindPW`.
        bindDN: cn=Directory Manager
        bindPW: &lt;BIND_DN_PASSWORD&gt;

        # The attribute to display in the provided password prompt. If unset, will
        # display "Username"
        usernamePrompt: Email Address

        # User search maps a username and password entered by a user to a LDAP entry.
        userSearch:
          # BaseDN to start the search from. It will translate to the query
          # "(&amp;(objectClass=person)(mail=&lt;USERNAME&gt;))".
          baseDN: ou=Users,dc=example,dc=org
          # Optional filter to apply when searching the directory.
          filter: "(objectClass=person)"

          # username attribute used for comparing user entries. This will be translated
          # and combined with the other filter as "(&lt;attr&gt;=&lt;USERNAME&gt;)".
          username: mail
          # The following three fields are direct mappings of attributes on the user entry.
          # String representation of the user.
          idAttr: DN
          # Required. Attribute to map to Email.
          emailAttr: mail
          # Maps to display the name of users. No default value.
          nameAttr: cn

        # Group search queries for groups given a user entry.
        groupSearch:
          # BaseDN to start the search from. It will translate to the query
          # "(&amp;(objectClass=group)(member=&lt;USER_UID&gt;))".
          baseDN: ou=Groups,dc=example,dc=org
          # Optional filter to apply when searching the directory.
          filter: "(objectClass=groupOfNames)"

          # Following two fields are used to match a user to a group. It adds an additional
          # requirement to the filter that an attribute in the group must match the user's
          # attribute value.
          userAttr: uid
          groupAttr: memberUid

          # Represents group name.
          nameAttr: cn</pre></div></li><li class="listitem "><p>Active Directory TLS Connector using email:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ConfigMap
metadata:
  name: oidc-dex-config
  namespace: kube-system
data:
  config.yaml: |
    connectors:
    - type: ldap
      # Required field for connector id.
      id: AD
      # Required field for connector name.
      name: AD
      config:
        # Host and optional port of the LDAP server in the form "host:port".
        # If the port is not supplied, it will be guessed based on "insecureNoSSL",
        # and "startTLS" flags. 389 for insecure or StartTLS connections, 636
        # otherwise.
        host: ad.example.org:636

        # Following field is required if the LDAP host is not using TLS (port 389).
        # Because this option inherently leaks passwords to anyone on the same network
        # as dex, THIS OPTION MAY BE REMOVED WITHOUT WARNING IN A FUTURE RELEASE.
        #
        # insecureNoSSL: true

        # If a custom certificate isn't provided, this option can be used to turn on
        # TLS certificate checks. As noted, it is insecure and shouldn't be used outside
        # of explorative phases.
        #
        # insecureSkipVerify: true

        # When connecting to the server, connect using the ldap:// protocol then issue
        # a StartTLS command. If unspecified, connections will use the ldaps:// protocol
        #
        # startTLS: true

        # Path to a trusted root certificate file. Default: use the host's root CA.
        # rootCA: /etc/dex/ldap.ca

        # A raw certificate file can also be provided inline.
        rootCAData: &lt;BASE_64_ENCODED_PEM_FILE&gt;

        # The DN and password for an application service account. The connector uses
        # these credentials to search for users and groups. Not required if the LDAP
        # server provides access for anonymous auth.
        # Please note that if the bind password contains a `$`, it has to be saved in an
        # environment variable which should be given as the value to `bindPW`.
        bindDN: cn=user-admin,ou=Users,dc=example,dc=org
        bindPW: &lt;BIND_DN_PASSWORD&gt;

        # The attribute to display in the provided password prompt. If unset, will
        # display "Username"
        usernamePrompt: Email Address

        # User search maps a username and password entered by a user to a LDAP entry.
        userSearch:
          # BaseDN to start the search from. It will translate to the query
          # "(&amp;(objectClass=person)(mail=&lt;USERNAME&gt;))".
          baseDN: ou=Users,dc=example,dc=org
          # Optional filter to apply when searching the directory.
          filter: "(objectClass=person)"

          # username attribute used for comparing user entries. This will be translated
          # and combined with the other filter as "(&lt;attr&gt;=&lt;USERNAME&gt;)".
          username: mail
          # The following three fields are direct mappings of attributes on the user entry.
          # String representation of the user.
          idAttr: distinguishedName
          # Required. Attribute to map to Email.
          emailAttr: mail
          # Maps to display the name of users. No default value.
          nameAttr: sAMAccountName

        # Group search queries for groups given a user entry.
        groupSearch:
          # BaseDN to start the search from. It will translate to the query
          # "(&amp;(objectClass=group)(member=&lt;USER_UID&gt;))".
          baseDN: ou=Groups,dc=example,dc=org
          # Optional filter to apply when searching the directory.
          filter: "(objectClass=group)"

          # Following two fields are used to match a user to a group. It adds an additional
          # requirement to the filter that an attribute in the group must match the user's
          # attribute value.
          userAttr: distinguishedName
          groupAttr: member

          # Represents group name.
          nameAttr: sAMAccountName</pre></div></li><li class="listitem "><p>Active Directory TLS Connector using sAMAccountName:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ConfigMap
metadata:
  name: oidc-dex-config
  namespace: kube-system
data:
  config.yaml: |
    connectors:
    - type: ldap
      # Required field for connector id.
      id: AD
      # Required field for connector name.
      name: AD
      config:
        # Host and optional port of the LDAP server in the form "host:port".
        # If the port is not supplied, it will be guessed based on "insecureNoSSL",
        # and "startTLS" flags. 389 for insecure or StartTLS connections, 636
        # otherwise.
        host: ad.example.org:636

        # Following field is required if the LDAP host is not using TLS (port 389).
        # Because this option inherently leaks passwords to anyone on the same network
        # as dex, THIS OPTION MAY BE REMOVED WITHOUT WARNING IN A FUTURE RELEASE.
        #
        # insecureNoSSL: true

        # If a custom certificate isn't provided, this option can be used to turn on
        # TLS certificate checks. As noted, it is insecure and shouldn't be used outside
        # of explorative phases.
        #
        # insecureSkipVerify: true

        # When connecting to the server, connect using the ldap:// protocol then issue
        # a StartTLS command. If unspecified, connections will use the ldaps:// protocol
        #
        # startTLS: true

        # Path to a trusted root certificate file. Default: use the host's root CA.
        # rootCA: /etc/dex/ldap.ca

        # A raw certificate file can also be provided inline.
        rootCAData: &lt;BASE_64_ENCODED_PEM_FILE&gt;

        # The DN and password for an application service account. The connector uses
        # these credentials to search for users and groups. Not required if the LDAP
        # server provides access for anonymous auth.
        # Please note that if the bind password contains a `$`, it has to be saved in an
        # environment variable which should be given as the value to `bindPW`.
        bindDN: cn=user-admin,ou=Users,dc=example,dc=org
        bindPW: &lt;BIND_DN_PASSWORD&gt;

        # The attribute to display in the provided password prompt. If unset, will
        # display "Username"
        usernamePrompt: sAMAccountName

        # User search maps a username and password entered by a user to a LDAP entry.
        userSearch:
          # BaseDN to start the search from. It will translate to the query
          # "(&amp;(objectClass=person)(mail=&lt;USERNAME&gt;))".
          baseDN: ou=Users,dc=example,dc=org
          # Optional filter to apply when searching the directory.
          filter: "(objectClass=person)"

          # username attribute used for comparing user entries. This will be translated
          # and combined with the other filter as "(&lt;attr&gt;=&lt;USERNAME&gt;)".
          username: sAMAccountName
          # The following three fields are direct mappings of attributes on the user entry.
          # String representation of the user.
          idAttr: sAMAccountName
          # Required. Attribute to map to Email.
          emailAttr: mail
          # Maps to display the name of users. No default value.
          nameAttr: sAMAccountName

        # Group search queries for groups given a user entry.
        groupSearch:
          # BaseDN to start the search from. It will translate to the query
          # "(&amp;(objectClass=group)(member=&lt;USER_UID&gt;))".
          baseDN: ou=Groups,dc=example,dc=org
          # Optional filter to apply when searching the directory.
          filter: "(objectClass=group)"

          # Following two fields are used to match a user to a group. It adds an additional
          # requirement to the filter that an attribute in the group must match the user's
          # attribute value.
          userAttr: distinguishedName
          groupAttr: member

          # Represents group name.
          nameAttr: sAMAccountName</pre></div></li></ul></div></li><li class="listitem "><p>Create a <code class="literal">kustomization.yaml</code> file in <code class="literal">&lt;CLUSTER_NAME&gt;/addons/dex/kustomization.yaml</code></p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - base/dex.yaml
patches:
  - patches/custom.yaml</pre></div></li><li class="listitem "><p>Apply the changes with:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kubectl apply -k &lt;CLUSTER_NAME&gt;/addons/dex/</pre></div></li><li class="listitem "><p>Then, refer to <a class="xref" href="#sec-admin-security-rbac-apply" title="6.5.2. User Access">Section 6.5.2, “User Access”</a> to access through Web or CLI.</p></li></ol></div></div></div><div class="sect1" id="rbac"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Role-Based Access Control (RBAC)</span> <a title="Permalink" class="permalink" href="#rbac">#</a></h2></div></div></div><p>Kubernetes RBAC uses the <code class="literal">rbac.authorization.k8s.io</code> API group to drive authorization decisions,
allowing administrators to dynamically configure policies through the Kubernetes API.</p><p>The administrators can use Kubernetes RBAC to design user or group authorizations.</p><div class="sect2" id="sec-admin-security-role"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Role Management</span> <a title="Permalink" class="permalink" href="#sec-admin-security-role">#</a></h3></div></div></div><p>Roles define, which <span class="emphasis"><em>subjects</em></span> (users or groups) can use which <span class="emphasis"><em>verbs</em></span> (operations) on which <span class="emphasis"><em>resources</em></span>.
The following sections provide an overview of the verbs, resources, and how to create roles.
Roles can then be assigned to users and groups.</p><div class="sect3" id="sec-admin-security-role-verb"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">List of Verbs</span> <a title="Permalink" class="permalink" href="#sec-admin-security-role-verb">#</a></h4></div></div></div><p>This section provides an overview of the most common <span class="emphasis"><em>verbs</em></span> (operations) used for defining roles.
Verbs correspond to sub-commands of <code class="literal">kubectl</code>.</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.8.6.4.3.3.1"><span class="term ">create</span></dt><dd><p>Create a resource.</p></dd><dt id="id-1.8.6.4.3.3.2"><span class="term ">delete</span></dt><dd><p>Delete resources.</p></dd><dt id="id-1.8.6.4.3.3.3"><span class="term ">deletecollection</span></dt><dd><p>Delete a collection of a resource (can only be invoked using the Kubernetes API).</p></dd><dt id="id-1.8.6.4.3.3.4"><span class="term ">get</span></dt><dd><p>Display individual resources.</p></dd><dt id="id-1.8.6.4.3.3.5"><span class="term ">list</span></dt><dd><p>Display collections.</p></dd><dt id="id-1.8.6.4.3.3.6"><span class="term ">patch</span></dt><dd><p>Update an API object in place.</p></dd><dt id="id-1.8.6.4.3.3.7"><span class="term ">proxy</span></dt><dd><p>Allows running <code class="literal">kubectl</code> in a mode where it acts as a reverse proxy.</p></dd><dt id="id-1.8.6.4.3.3.8"><span class="term ">update</span></dt><dd><p>Update fields of a resource, for example, annotations or labels.</p></dd><dt id="id-1.8.6.4.3.3.9"><span class="term ">watch</span></dt><dd><p>Watch resource.</p></dd></dl></div></div><div class="sect3" id="sec-admin-security-role-resource"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">List of Resources</span> <a title="Permalink" class="permalink" href="#sec-admin-security-role-resource">#</a></h4></div></div></div><p>This section provides an overview of the most common <span class="emphasis"><em>resources</em></span> used for defining roles.</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.8.6.4.4.3.1"><span class="term ">Autoscaler</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a></p></dd><dt id="id-1.8.6.4.4.3.2"><span class="term ">ConfigMaps</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/</a></p></dd><dt id="id-1.8.6.4.4.3.3"><span class="term ">Cronjob</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/</a></p></dd><dt id="id-1.8.6.4.4.3.4"><span class="term ">DaemonSet</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/daemonset/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a></p></dd><dt id="id-1.8.6.4.4.3.5"><span class="term ">Deployment</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></p></dd><dt id="id-1.8.6.4.4.3.6"><span class="term ">Ingress</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/services-networking/ingress/</a></p></dd><dt id="id-1.8.6.4.4.3.7"><span class="term ">Job</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/</a></p></dd><dt id="id-1.8.6.4.4.3.8"><span class="term ">Namespace</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/</a></p></dd><dt id="id-1.8.6.4.4.3.9"><span class="term ">Node</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/architecture/nodes/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/architecture/nodes/</a></p></dd><dt id="id-1.8.6.4.4.3.10"><span class="term ">Pod</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/workloads/pods/pod-overview/</a></p></dd><dt id="id-1.8.6.4.4.3.11"><span class="term ">Persistent Volumes</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/storage/persistent-volumes/</a></p></dd><dt id="id-1.8.6.4.4.3.12"><span class="term ">Secrets</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/configuration/secret/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/configuration/secret/</a></p></dd><dt id="id-1.8.6.4.4.3.13"><span class="term ">Service</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/services-networking/service/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/services-networking/service/</a></p></dd><dt id="id-1.8.6.4.4.3.14"><span class="term ">ReplicaSets</span></dt><dd><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/replicaset/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/workloads/controllers/replicaset/</a></p></dd></dl></div></div><div class="sect3" id="sec-admin-security-role-create"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.5.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Roles</span> <a title="Permalink" class="permalink" href="#sec-admin-security-role-create">#</a></h4></div></div></div><p>Roles are defined in YAML files.
To apply role definitions to Kubernetes, use <code class="literal">kubectl apply -f YAML_FILE</code>.
The following examples provide an overview of different use cases of roles.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Simple Role for Core Resource:</p><p>This example allows us to <code class="literal">get</code>, <code class="literal">watch</code>, and <code class="literal">list</code> all <code class="literal">pods</code> in the namespace <code class="literal">default</code>.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: view-pods <span id="CO5-1"></span><span class="callout">1</span>
  namespace: default <span id="CO5-2"></span><span class="callout">2</span>
rules:
- apiGroups: [""] <span id="CO5-3"></span><span class="callout">3</span>
  resources: ["pods"] <span id="CO5-4"></span><span class="callout">4</span>
  verbs: ["get", "watch", "list"] <span id="CO5-5"></span><span class="callout">5</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Name of the role. This is required to associate the rule with
a group or user. For details, refer to
<a class="xref" href="#sec-admin-security-role-create_binding" title="6.5.1.4. Create Role Bindings">Section 6.5.1.4, “Create Role Bindings”</a>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Namespace the new group should be allowed to access. Use <code class="literal">default</code>
for Kubernetes' default namespace.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Kubernetes API groups. Use <code class="literal">""</code> for the core group. Use
<code class="literal">kubectl api-resources</code> to list all API groups.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Kubernetes resources. For a list of available resources, refer to
<a class="xref" href="#sec-admin-security-role-resource" title="6.5.1.2. List of Resources">Section 6.5.1.2, “List of Resources”</a>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO5-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Kubernetes verbs. For a list of available verbs, refer to
<a class="xref" href="#sec-admin-security-role-verb" title="6.5.1.1. List of Verbs">Section 6.5.1.1, “List of Verbs”</a>.</p></td></tr></table></div></li><li class="listitem "><p>Cluster Role for Creation of Pods:</p><p>This example creates a cluster role to allow <code class="literal">create pods</code> cluster-wide.
Note the <code class="literal">ClusterRole</code> value for <code class="literal">kind</code>.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: admin-create-pods <span id="CO6-1"></span><span class="callout">1</span>
rules:
- apiGroups: [""] <span id="CO6-2"></span><span class="callout">2</span>
  resources: ["pods"] <span id="CO6-3"></span><span class="callout">3</span>
  verbs: ["create"] <span id="CO6-4"></span><span class="callout">4</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Name of the cluster role. This is required to associate the rule with
a group or user. For details, refer to
<a class="xref" href="#sec-admin-security-role-create_binding" title="6.5.1.4. Create Role Bindings">Section 6.5.1.4, “Create Role Bindings”</a>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Kubernetes API groups. Use <code class="literal">""</code> for the core group. Use
<code class="literal">kubectl api-resources</code> to list all API groups.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Kubernetes resources. For a list of available resources, refer to <a class="xref" href="#sec-admin-security-role-resource" title="6.5.1.2. List of Resources">Section 6.5.1.2, “List of Resources”</a>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO6-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Kubernetes verbs. For a list of available verbs, refer to <a class="xref" href="#sec-admin-security-role-verb" title="6.5.1.1. List of Verbs">Section 6.5.1.1, “List of Verbs”</a>.</p></td></tr></table></div></li></ul></div></div><div class="sect3" id="sec-admin-security-role-create_binding"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.5.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Role Bindings</span> <a title="Permalink" class="permalink" href="#sec-admin-security-role-create_binding">#</a></h4></div></div></div><p>To bind a group or user to a role or cluster role, create a YAML file that contains the role or cluster role binding description.
Then apply the binding with <code class="literal">kubectl apply -f YAML_FILE</code>.
The following examples provide an overview of different use cases of role bindings.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Binding a User to a Role:</p><p>This example shows how to bind a user to a defined role.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: &lt;ROLE_BINDING_NAME&gt; <span id="CO7-1"></span><span class="callout">1</span>
  namespace: &lt;NAMESPACE&gt; <span id="CO7-2"></span><span class="callout">2</span>
subjects:
- kind: User
  name: &lt;LDAP_USER_NAME&gt; <span id="CO7-3"></span><span class="callout">3</span>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: &lt;ROLE_NAME&gt; <span id="CO7-4"></span><span class="callout">4</span>
  apiGroup: rbac.authorization.k8s.io</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Defines a name for this new role binding.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Name of the namespace to which the binding applies.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Name of the LDAP user to which this binding applies.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO7-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Name of the role used. For defining rules, refer to
<a class="xref" href="#sec-admin-security-role-create" title="6.5.1.3. Creating Roles">Section 6.5.1.3, “Creating Roles”</a>.</p></td></tr></table></div></li><li class="listitem "><p>Binding a User to a Cluster Role:</p><p>This example shows how to bind a user to a defined cluster role.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: &lt;CLUSTER_ROLE_BINDING_NAME&gt; <span id="CO8-1"></span><span class="callout">1</span>
subjects:
- kind: User
  name: &lt;LDAP_USER_NAME&gt; <span id="CO8-2"></span><span class="callout">2</span>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: &lt;CLUSER_ROLE_NAME&gt; <span id="CO8-3"></span><span class="callout">3</span>
  apiGroup: rbac.authorization.k8s.io</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Defines a name for this new cluster role binding.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Name of the LDAP user to which this binding applies.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO8-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Name of the cluster role used. For defining rules, refer to <a class="xref" href="#sec-admin-security-role-create" title="6.5.1.3. Creating Roles">Section 6.5.1.3, “Creating Roles”</a>.</p></td></tr></table></div></li><li class="listitem "><p>Binding a Group to a Role:</p><p>This example shows how to bind a group to a defined role.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: &lt;ROLE_BINDING_NAME&gt; <span id="CO9-1"></span><span class="callout">1</span>
  namespace: &lt;NAMESPACE&gt; <span id="CO9-2"></span><span class="callout">2</span>
subjects:
- kind: Group
  name: &lt;LDAP_GROUP_NAME&gt; <span id="CO9-3"></span><span class="callout">3</span>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: &lt;ROLE_NAME&gt; <span id="CO9-4"></span><span class="callout">4</span>
  apiGroup: rbac.authorization.k8s.io</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Defines a name for this new role binding.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Name of the namespace to which the binding applies.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Name of the LDAP group to which this binding applies.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO9-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Name of the role used. For defining rules, refer to
<a class="xref" href="#sec-admin-security-role-create" title="6.5.1.3. Creating Roles">Section 6.5.1.3, “Creating Roles”</a>.</p></td></tr></table></div></li><li class="listitem "><p>Binding a Group to a Cluster Role:</p><p>This example shows how to bind a group to a defined cluster role.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: &lt;CLUSTER_ROLE_BINDING_NAME&gt; <span id="CO10-1"></span><span class="callout">1</span>
subjects:
- kind: Group
  name: &lt;LDAP_GROUP_NAME&gt; <span id="CO10-2"></span><span class="callout">2</span>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: &lt;CLUSER_ROLE_NAME&gt; <span id="CO10-3"></span><span class="callout">3</span>
  apiGroup: rbac.authorization.k8s.io</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Defines a name for this new cluster role binding.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Name of the LDAP group to which this binding applies.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO10-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Name of the cluster role used. For defining rules, refer to <a class="xref" href="#sec-admin-security-role-create" title="6.5.1.3. Creating Roles">Section 6.5.1.3, “Creating Roles”</a>.</p></td></tr></table></div></li></ul></div><div id="id-1.8.6.4.6.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>When creating new Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings, it is important to keep in mind the <code class="literal">Principle of Least Privilege</code>:</p><p><span class="emphasis"><em>"define rules such that the account bound to the Role or ClusterRole has the minimum
amount of permissions needed to fulfill its purpose and no more."</em></span></p><p>For instance, granting the <code class="literal">admin</code> ClusterRole to most accounts is most likely unnecessary,
when a reduced-scope role would be enough to fulfill the account’s purpose.
This helps reduce the attack surface if an account is compromised.</p><p>It is also recommended to periodically review your Roles and ClusterRoles to ensure they are still required and are not overly-permissive.</p></div></div></div><div class="sect2" id="sec-admin-security-rbac-apply"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">User Access</span> <a title="Permalink" class="permalink" href="#sec-admin-security-rbac-apply">#</a></h3></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Using the web browser:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Go to the login page at <code class="literal">https://&lt;CONTROL_PLANE_IP/FQDN&gt;:32001</code>.</p></li><li class="listitem "><p>Click "Sign In".</p></li><li class="listitem "><p>Choose a login method.</p></li><li class="listitem "><p>Enter login credentials.</p></li><li class="listitem "><p>Download <code class="literal">kubeconfig</code> or self-configure <code class="literal">kubectl</code> with the provided setup instructions.</p></li></ol></div></li><li class="listitem "><p>Using the CLI:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Run <code class="literal">skuba auth login</code> with Dex server URL <code class="literal">https://&lt;CONTROL_PLANE_IP/FQDN&gt;:32000</code>,
login username and password.</p></li><li class="listitem "><p>The kubeconfig <code class="literal">kubeconf.txt</code> is generated at your current directory.</p></li></ol></div></li></ul></div></div><div class="sect2" id="_access_kubernetes_resources"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Kubernetes Resources</span> <a title="Permalink" class="permalink" href="#_access_kubernetes_resources">#</a></h3></div></div></div><p>The user can now access resources in the authorized <code class="literal">&lt;NAMESPACE&gt;</code>.</p><p>If the user has the proper permissions to access the resources, the output should look like this:</p><div class="verbatim-wrap"><pre class="screen"># kubectl -n &lt;NAMESPACE&gt; get pod

NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   dex-844dc9b8bb-w2zkm                 1/1     Running   0          19d
kube-system   gangway-944dc9b8cb-w2zkm             1/1     Running   0          19d
kube-system   cilium-76glw                         1/1     Running   0          27d
kube-system   cilium-fvgcv                         1/1     Running   0          27d
kube-system   cilium-j5lpx                         1/1     Running   0          27d
kube-system   cilium-operator-5d9cc4fbb7-g5plc     1/1     Running   0          34d
kube-system   cilium-vjf6p                         1/1     Running   8          27d
kube-system   coredns-559fbd6bb4-2r982             1/1     Running   9          46d
kube-system   coredns-559fbd6bb4-89k2j             1/1     Running   9          46d
kube-system   etcd-my-master                       1/1     Running   5          46d
kube-system   kube-apiserver-&lt;CLUSTER_NAME&gt;            1/1     Running   0          19d
kube-system   kube-controller-manager-my-master    1/1     Running   14         46d
kube-system   kube-proxy-62hls                     1/1     Running   4          46d
kube-system   kube-proxy-fhswj                     1/1     Running   0          46d
kube-system   kube-proxy-r4h42                     1/1     Running   1          39d
kube-system   kube-proxy-xsdf4                     1/1     Running   0          39d
kube-system   kube-scheduler-my-master             1/1     Running   13         46d</pre></div><p>If the user does not have the right permissions to access a resource,
they will receive a <code class="literal">Forbidden</code> message.</p><div class="verbatim-wrap"><pre class="screen">Error from server (Forbidden): pods is forbidden</pre></div></div><div class="sect2" id="_oidc_tokens"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OIDC Tokens</span> <a title="Permalink" class="permalink" href="#_oidc_tokens">#</a></h3></div></div></div><p>The kubeconfig file (<code class="literal">kubeconf.txt</code>) contains the OIDC tokens necessary to perform authentication and authorization in the cluster.
OIDC tokens have an <span class="strong"><strong>expiration date</strong></span> which means that they need to be refreshed after some time.</p><div id="id-1.8.6.7.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If you use the same user in multiple <code class="literal">kubeconfig</code> files distributed among multiple machines,
this can lead to issues. Due to the nature of access and refresh tokens (<a class="link" href="https://tools.ietf.org/html/rfc6749#page-10" target="_blank">https://tools.ietf.org/html/rfc6749#page-10</a>) only one of the machines will be fully able to refresh the token set at any given time.</p><p>The user will be able to download multiple 'kubeconfig' files. However, the file with the same user is likely to be valid only for single access until expiration.</p><p>Dex regards one session per user. The <code class="literal">id-token</code> and <code class="literal">refresh-token</code> are refreshed together.
If a second user is trying to login to get a new <code class="literal">id-token</code>, Dex will invalidate the previous <code class="literal">id-token</code> and <code class="literal">refresh-token</code> for the first user.
The first user is still able to continue using the old <code class="literal">id-token</code> until expiration. After expiration, the first user is not allowed to refresh the <code class="literal">id-token</code> due to the invalid <code class="literal">refresh-token</code>.
Only the second user will have a valid <code class="literal">refresh-token</code> now. The first user will encounter an error like: <code class="literal">"msg="failed to rotate keys: keys already rotated by another server instance"</code>.</p><p>If sharing the same <code class="literal">id-token</code> in many places, all of them can be used until expiration.
The first user to refresh the <code class="literal">id-token</code> and <code class="literal">refresh token</code> will be able to continue accessing the cluster.
All other users will encounter an error <code class="literal">Refresh token is invalid or has already been claimed by another client</code> because the <code class="literal">refresh-token</code> got updated by the first user.</p><p>Please use separate users for each <code class="literal">kubeconfig</code> file to avoid this situation.
Find out how to add more users in <a class="xref" href="#sec-admin-security-users-add" title="6.4.4.1. Adding a New User">Section 6.4.4.1, “Adding a New User”</a>.
You can also check information about the user and the respective OIDC tokens in the <code class="literal">kubeconfig</code> file under the <code class="literal">users</code> section:</p><div class="verbatim-wrap"><pre class="screen">users:
- name: myuser
  user:
    auth-provider:
      config:
        client-id: oidc
        client-secret: &lt;SECRET&gt;
        id-token:  &lt;ID_TOKEN&gt;
        idp-issuer-url: https://&lt;IP&gt;:&lt;PORT&gt;
        refresh-token: &lt;REFRESH_TOKEN&gt;
      name: oidc</pre></div></div></div></div><div class="sect1" id="admission"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Admission Controllers</span> <a title="Permalink" class="permalink" href="#admission">#</a></h2></div></div></div><div class="sect2" id="_introduction"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction">#</a></h3></div></div></div><p>After user authentication and authorization, <span class="strong"><strong>admission</strong></span> takes place to complete the access control for the Kubernetes API.
As the final step in the access control process, admission enhances the security layer by mandating a reasonable security baseline across a specific namespace or the entire cluster.
The built-in PodSecurityPolicy admission controller is perhaps the most prominent example of it.</p><p>Apart from the security aspect, admission controllers can enforce custom policies to adhere to certain best-practices such as having good labels, annotation, resource limits, or other settings.
It is worth noting that instead of only validating the request, admission controllers are also capable of "fixing" a request by mutating it, such as automatically adding resource limits if the user forgets to.</p><p>The admission is controlled by admission controllers which may only be configured by the cluster administrator. The admission control process happens in <span class="strong"><strong>two phases</strong></span>:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>In the first phase, <span class="strong"><strong>mutating</strong></span> admission controllers are run. They are empowered to automatically change the requested object to comply with certain cluster policies by making modifications to it if needed.</p></li><li class="listitem "><p>In the second phase, <span class="strong"><strong>validating</strong></span> admission controllers are run. Based on the results of the previous mutating phase, an admission controller can either allow the request to proceed and reach <code class="literal">etcd</code> or deny it.</p></li></ol></div><div id="id-1.8.7.2.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If any of the controllers in either phase reject the request, the entire request is rejected immediately and an error is returned to the end-user.</p></div></div><div class="sect2" id="_configured_admission_controllers"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configured admission controllers</span> <a title="Permalink" class="permalink" href="#_configured_admission_controllers">#</a></h3></div></div></div><div id="id-1.8.7.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Any modification of this list prior to the creation of the cluster will be overwritten by these default settings.</p><p>The ability to add or remove individual admission controllers will be provided with one of the upcoming releases of SUSE CaaS Platform.</p></div><p>The complete list of admission controllers can be found at <a class="link" href="https://v1-18.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do" target="_blank">https://v1-18.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do</a></p><p>The default admission controllers enabled in SUSE CaaS Platform are:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><code class="literal">NodeRestriction</code></p></li><li class="listitem "><p><code class="literal">PodSecurityPolicy</code></p></li></ol></div></div></div><div class="sect1" id="_pod_security_policies"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pod Security Policies</span> <a title="Permalink" class="permalink" href="#_pod_security_policies">#</a></h2></div></div></div><div id="id-1.8.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Please note that criteria for designing PodSecurityPolicy are not part of this document.</p></div><p>"Pod Security Policy" (stylized as <code class="literal">PodSecurityPolicy</code> and abbreviated "PSP") is a security
measure implemented by Kubernetes to control which specifications a pod must meet
to be allowed to run in the cluster. They control various aspects of execution of
pods and interactions with other parts of the software infrastructure.</p><p>You can find more general information about PodSecurityPolicy in the <a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank">Kubernetes Docs</a>.</p><p>User access to the cluster is controlled via "Role Based Access Control (RBAC)".
Each PodSecurityPolicy is associated with one or more users or
service accounts so they are allowed to launch pods with the associated
specifications. The policies are associated with users or  service accounts via
role bindings.</p><div id="id-1.8.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The default policies shipped with SUSE CaaS Platform are a good start, but depending
on security requirements, adjustments should be made or additional policies should be created.</p></div><div class="sect2" id="_default_policies"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Default Policies</span> <a title="Permalink" class="permalink" href="#_default_policies">#</a></h3></div></div></div><p>SUSE CaaS Platform 4 currently ships with two default policies:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Privileged (full access everywhere)</p></li><li class="listitem "><p>Unprivileged (only very basic access)</p></li></ul></div><p>All pods running the containers for the basic SUSE CaaS Platform software are
deployed into the <code class="literal">kube-system</code> namespace and run with the "privileged" policy.</p><p>All authenticated system users (<code class="literal">group system:authenticated</code>) and service accounts in kube-system (<code class="literal">system:serviceaccounts:kube-system</code>)
have a RoleBinding (<code class="literal">suse:caasp:psp:privileged</code>) to run pods using the privileged policy in the kube-system namespace.</p><p>Any other pods launched in any other namespace are, by default, deployed in
unprivileged mode.</p><div id="id-1.8.8.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>You must configure RBAC rules and PodSecurityPolicy to provide proper functionality
and security.</p></div></div><div class="sect2" id="_policy_definition"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Policy Definition</span> <a title="Permalink" class="permalink" href="#_policy_definition">#</a></h3></div></div></div><p>The policy definitions are embedded in the <a class="link" href="https://github.com/SUSE/skuba/blob/master/pkg/skuba/actions/cluster/init/manifests.go" target="_blank">cluster bootstrap manifest (GitHub)</a>.</p><p>During the bootstrap with <code class="literal">skuba</code>, the policy files will be stored on your
workstation in the cluster definition folder under <code class="literal">addons/psp/base</code>. These policy files
will be installed automatically for all cluster nodes.</p><p>The file names of the files created are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">podsecuritypolicy-unprivileged.yaml</code></p><p>and</p></li><li class="listitem "><p><code class="literal">podsecuritypolicy-privileged.yaml</code>.</p></li></ul></div><div class="sect3" id="configure-psp"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Policy File Examples</span> <a title="Permalink" class="permalink" href="#configure-psp">#</a></h4></div></div></div><p>This is the unprivileged policy as a configuration file. You can use this
as a basis to develop your own PodSecurityPolicy which should be saved as <code class="literal">custom-psp.yaml</code>
<code class="literal">addons/psp/patches</code> directory.</p><div class="verbatim-wrap"><pre class="screen">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: suse.caasp.psp.unprivileged
  annotations:
    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  # Privileged
  privileged: false
  # Volumes and File Systems
  volumes:
    # Kubernetes Pseudo Volume Types
    - configMap
    - secret
    - emptyDir
    - downwardAPI
    - projected
    - persistentVolumeClaim
    # Networked Storage
    - nfs
    - rbd
    - cephFS
    - glusterfs
    - fc
    - iscsi
    # Cloud Volumes
    - cinder
    - gcePersistentDisk
    - awsElasticBlockStore
    - azureDisk
    - azureFile
    - vsphereVolume
  allowedHostPaths:
    # Note: We don't allow hostPath volumes above, but set this to a path we
    # control anyway as a belt+braces protection. /dev/null may be a better
    # option, but the implications of pointing this towards a device are
    # unclear.
    - pathPrefix: /opt/kubernetes-hostpath-volumes
  readOnlyRootFilesystem: false
  # Users and groups
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  # Privilege Escalation
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  # Capabilities
  allowedCapabilities: []
  defaultAddCapabilities: []
  requiredDropCapabilities: []
  # Host namespaces
  hostPID: false
  hostIPC: false
  hostNetwork: false
  hostPorts:
  - min: 0
    max: 65535
  # SELinux
  seLinux:
    # SELinux is unused in CaaSP
    rule: 'RunAsAny'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: suse:caasp:psp:unprivileged
rules:
  - apiGroups: ['extensions']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames: ['suse.caasp.psp.unprivileged']
---
# Allow all users and serviceaccounts to use the unprivileged
# PodSecurityPolicy
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse:caasp:psp:default
roleRef:
  kind: ClusterRole
  name: suse:caasp:psp:unprivileged
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: Group
  apiGroup: rbac.authorization.k8s.io
  name: system:serviceaccounts
- kind: Group
  apiGroup: rbac.authorization.k8s.io
  name: system:authenticated</pre></div></div></div><div class="sect2" id="_creating_a_podsecuritypolicy"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a PodSecurityPolicy</span> <a title="Permalink" class="permalink" href="#_creating_a_podsecuritypolicy">#</a></h3></div></div></div><p>In order to properly secure and run your Kubernetes workloads you must configure
RBAC rules for your desired users create a PodSecurityPolicy adequate for your respective
workloads and then link the user accounts to the PodSecurityPolicy using (Cluster)RoleBinding.</p><p><a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank">https://v1-18.docs.kubernetes.io/docs/concepts/policy/pod-security-policy/</a></p></div></div><div class="sect1" id="nginx-ingress"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NGINX Ingress Controller</span> <a title="Permalink" class="permalink" href="#nginx-ingress">#</a></h2></div></div></div><p>Kubernetes ingress exposes HTTP and HTTPS routes from the outside of a cluster to services created inside the cluster.
An Ingress controller with an ingress controller service is responsible for supporting the Kubernetes ingress.
In order to use Kubernetes ingress, you need to install the ingress controller with the ingress controller service exposed to the outside of the cluster.
Traffic routing is controlled by rules defined on the Ingress resource from the backend services.</p><div class="sect2" id="_configure_and_deploy_nginx_ingress_controller"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure and deploy NGINX ingress controller</span> <a title="Permalink" class="permalink" href="#_configure_and_deploy_nginx_ingress_controller">#</a></h3></div></div></div><div class="sect3" id="_define_networking_configuration"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.8.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Define networking configuration</span> <a title="Permalink" class="permalink" href="#_define_networking_configuration">#</a></h4></div></div></div><p>Choose which networking configuration the ingress controller should have.
Create a file <code class="literal">nginx-ingress-config-values.yaml</code> with one of the following examples as content:</p><div class="verbatim-wrap"><pre class="screen"># Enable the creation of pod security policy
podSecurityPolicy:
  enabled: false

# Create a specific service account
serviceAccount:
  create: true
  name: nginx-ingress

controller:
  # Number of controller pods
  replicaCount: 3
  [ADD CONTENT HERE] <span id="CO11-1"></span><span class="callout">1</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO11-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Add one of the following sections at this point to configure for a specific type of exposing the service.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: The services will be publicly exposed on each node of the cluster, including master nodes, at port <code class="literal">32443</code> for <code class="literal">HTTPS</code>.</p><div class="verbatim-wrap"><pre class="screen">  # Publish services on port HTTPS/32443
  # These services are exposed on each node
  service:
    enableHttp: false
    type: NodePort
    nodePorts:
      https: 32443</pre></div></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: The services will be exposed on specific nodes of the cluster, at port <code class="literal">443</code> for <code class="literal">HTTPS</code>.</p><div class="verbatim-wrap"><pre class="screen">  # These services are exposed on the node with IP 10.86.4.158
  service:
    enableHttp: false
    externalIPs:
      - 10.86.4.158</pre></div></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: The services will be exposed on the loadbalancer that the cloud provider serves.</p><div class="verbatim-wrap"><pre class="screen">  # These services are exposed on IP from a cluster cloud provider
  service:
    enableHttp: false
    type: LoadBalancer</pre></div></li></ul></div></td></tr></table></div></div><div class="sect3" id="_deploy_ingress_controller_from_helm_chart"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.8.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy ingress controller from helm chart</span> <a title="Permalink" class="permalink" href="#_deploy_ingress_controller_from_helm_chart">#</a></h4></div></div></div><div id="id-1.8.9.3.3.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>For complete instructions on how to install Helm and Tiller refer to <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>.</p></div><p>Add the <a class="link" href="https://kubernetes-charts.suse.com/" target="_blank">SUSE helm charts repository</a> by running:</p><div class="verbatim-wrap"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com</pre></div><p>Then you can deploy the ingress controller and use the previously created configuration file to configure the networking type.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install --name nginx-ingress suse/nginx-ingress \
--namespace nginx-ingress \
--values nginx-ingress-config-values.yaml</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create namespace nginx-ingress
helm install nginx-ingress suse/nginx-ingress \
--namespace nginx-ingress \
--values nginx-ingress-config-values.yaml</pre></div><p>The result should be two running pods:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl -n nginx-ingress get pod
NAME                                             READY     STATUS    RESTARTS   AGE
nginx-ingress-controller-74cffccfc-p8xbb         1/1       Running   0          4s
nginx-ingress-default-backend-6b9b546dc8-mfkjk   1/1       Running   0          4s</pre></div><p>Depending on the networking configuration you chose before, the result should be two services:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span></p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get svc -n nginx-ingress
NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
nginx-ingress-controller        NodePort    10.100.108.7     &lt;none&gt;        443:32443/TCP   2d1h
nginx-ingress-default-backend   ClusterIP   10.109.118.128   &lt;none&gt;        80/TCP          2d1h</pre></div></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span></p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get svc -n nginx-ingress
NAME                            TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
nginx-ingress-controller        LoadBalancer   10.103.103.27  10.86.4.158   443:30275/TCP   12s
nginx-ingress-default-backend   ClusterIP      10.100.48.17   &lt;none&gt;        80/TCP          12s</pre></div></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span></p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get svc -n nginx-ingress
NAME                            TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
nginx-ingress-controller        LoadBalancer   10.106.160.255   10.86.5.176   443:31868/TCP   3h58m
nginx-ingress-default-backend   ClusterIP      10.111.140.50    &lt;none&gt;        80/TCP          3h58m</pre></div></li></ul></div></div><div class="sect3" id="_create_dns_entries"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.8.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create DNS entries</span> <a title="Permalink" class="permalink" href="#_create_dns_entries">#</a></h4></div></div></div><p>You should configure proper DNS names in any production environment. <code class="literal">k8s-dashboard.com</code> will be the domain name we will use in the ingress resource.
These values are only for example purposes.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span></p></li></ul></div><p>The services will be publicly exposed on each node of the cluster at port <code class="literal">32443</code> for HTTPS.
In this example, we will use a worker node with IP <code class="literal">10.86.14.58</code>.</p><div class="verbatim-wrap"><pre class="screen">k8s-dashboard.com                      IN  A       10.86.14.58</pre></div><p>Or add this entry to /etc/hosts</p><div class="verbatim-wrap"><pre class="screen">10.86.14.58 k8s-dashboard.com</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span></p></li></ul></div><p>The services will be exposed on a specific node of the cluster, at the assigned port for HTTPS.
In this example, we used the external IP <code class="literal">10.86.4.158</code>.</p><div class="verbatim-wrap"><pre class="screen">k8s-dashboard.com                      IN  A       10.86.4.158</pre></div><p>Or add this entry to /etc/hosts</p><div class="verbatim-wrap"><pre class="screen">10.86.4.158 k8s-dashboard.com</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span></p></li></ul></div><p>The services will be exposed on an assigned node of the cluster, at the assigned port for HTTPS.
In this example, LoadBalancer provided the external IP <code class="literal">10.86.5.176</code>.</p><div class="verbatim-wrap"><pre class="screen">k8s-dashboard.com                      IN  A       10.86.5.176</pre></div><p>Or add this entry to /etc/hosts</p><div class="verbatim-wrap"><pre class="screen">10.86.5.176 k8s-dashboard.com</pre></div></div></div><div class="sect2" id="_deploy_kubernetes_dashboard_as_an_example"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy Kubernetes Dashboard as an example</span> <a title="Permalink" class="permalink" href="#_deploy_kubernetes_dashboard_as_an_example">#</a></h3></div></div></div><div id="id-1.8.9.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>This example uses the upstream chart for the Kubernetes dashboard. There is currently no officially supported
version of the Kubernetes dashboard available from SUSE.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Deploy Kubernetes dashboard.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml</pre></div></li><li class="listitem "><p>Create the <code class="literal">cluster-admin</code> account to access the Kubernetes dashboard.</p><p>This will show how to create simple admin user using Service Account, grant it the admin permission then use the token to access the kubernetes dashboard.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create serviceaccount dashboard-admin -n kube-system

kubectl create clusterrolebinding dashboard-admin \
--clusterrole=cluster-admin \
--serviceaccount=kube-system:dashboard-admin</pre></div></li><li class="listitem "><p>Create the TLS secret.</p><p>Please refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> on how to sign the trusted certificate. In this example, crt and key are generated by a self-signed certificate.</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
-keyout /tmp/dashboard-tls.key -out /tmp/dashboard-tls.crt \
-subj "/CN=k8s-dashboard.com/O=k8s-dashboard"

kubectl create secret tls dashboard-tls \
--key /tmp/dashboard-tls.key --cert /tmp/dashboard-tls.crt \
-n kubernetes-dashboard</pre></div></li><li class="listitem "><p>Create the ingress resource.</p><p>We will create an ingress to access the backend service using the ingress controller.
Create <code class="literal">dashboard-ingress.yaml</code> with the appropriate values</p><div class="verbatim-wrap"><pre class="screen">apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/ingress.class: nginx
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/secure-backends: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
    - hosts:
      - k8s-dashboard.com
      secretName: dashboard-tls
  rules:
  - host: k8s-dashboard.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubernetes-dashboard
          servicePort: 443</pre></div></li><li class="listitem "><p>Deploy dashboard ingress.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -f dashboard-ingress.yaml</pre></div><p>The result will look like this:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get ing -n kubernetes-dashboard
NAMESPACE            NAME                 HOSTS               ADDRESS   PORTS     AGE
kubernetes-dashboard dashboard-ingress    k8s-dashboard.com             80, 443   2d</pre></div></li><li class="listitem "><p>Access Kubernetes Dashboard
Kubernetes dashboard will be accessible through ingress domain name with the configured ingress controller port.</p><div id="id-1.8.9.4.3.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Access Token</h6><p>Now we’re ready to get the token from dashboard-admin by following command.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl describe secrets -n kube-system \
$(kubectl -n kube-system get secret | awk '/dashboard-admin/{print $1}')</pre></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://k8s-dashboard.com:32443</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://k8s-dashboard.com</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://k8s-dashboard.com</code></p></li></ul></div></li></ol></div></div></div><div class="sect1" id="_certificates"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Certificates</span> <a title="Permalink" class="permalink" href="#_certificates">#</a></h2></div></div></div><p>During the installation of SUSE CaaS Platform, a CA (Certificate Authority) certificate is generated,
which is then used to authenticate and verify all communication. This process also creates
and distributes client and server certificates for the components.</p><div class="sect2" id="_communication_security"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Communication Security</span> <a title="Permalink" class="permalink" href="#_communication_security">#</a></h3></div></div></div><p>Communication is secured with TLS v1.2 using the AES 128 CBC cipher.
All certificates are 2048 bit RSA encrypted.</p></div><div class="sect2" id="_certificate_validity"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Certificate Validity</span> <a title="Permalink" class="permalink" href="#_certificate_validity">#</a></h3></div></div></div><p>The CA certificate is valid for 3650 days (10 years) by default.
Client and server certificates are valid for 365 days (1 year) by default.</p></div><div class="sect2" id="_certificate_location"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Certificate Location</span> <a title="Permalink" class="permalink" href="#_certificate_location">#</a></h3></div></div></div><p>Required CAs for SUSE CaaS Platform are stored on all control plane nodes:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Common Name</th><th align="left" valign="top">Path</th><th align="left" valign="top">Description</th></tr></thead><tbody><tr><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/ca.crt,key</p></td><td align="left" valign="top"><p>kubernetes general CA</p></td></tr><tr><td align="left" valign="top"><p>etcd-ca</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/etcd/ca.crt,key</p></td><td align="left" valign="top"><p>Etcd cluster</p></td></tr><tr><td align="left" valign="top"><p>kubelet-ca</p></td><td align="left" valign="top"><p>/var/lib/kubelet/pki/kubelet-ca.crt,key</p></td><td align="left" valign="top"><p>Kubelet components</p></td></tr><tr><td align="left" valign="top"><p>front-proxy-ca</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/front-proxy-ca.crt,key</p></td><td align="left" valign="top"><p>Front-proxy components</p></td></tr></tbody></table></div><p>The control plane certificates stored in the Kubernetes cluster on control plane nodes:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><thead><tr><th align="left" valign="top">Common Name</th><th align="left" valign="top">Parent CA</th><th align="left" valign="top">Path</th><th align="left" valign="top">Kind</th></tr></thead><tbody><tr><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>/etc/kubernetes/pki/ca.crt,key</p></td><td align="left" valign="top"><p>CA</p></td></tr><tr><td align="left" valign="top"><p>kube-apiserver</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/apiserver.crt,key</p></td><td align="left" valign="top"><p>Server</p></td></tr><tr><td align="left" valign="top"><p>kube-apiserver-etcd-client</p></td><td align="left" valign="top"><p>etcd-ca</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/apiserver-etcd-client.crt,key</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>kube-apiserver-kubelet-client</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/apiserver-kubelet-client.crt,key</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>etcd-ca</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>/etc/kubernetes/pki/etcd/ca.crt,key</p></td><td align="left" valign="top"><p>CA</p></td></tr><tr><td align="left" valign="top"><p>kube-etcd-healthcheck-client</p></td><td align="left" valign="top"><p>etcd-ca</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/etcd/healthcheck-client.crt,key</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>kube-etcd-peer</p></td><td align="left" valign="top"><p>etcd-ca</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/etcd/peer.crt,key</p></td><td align="left" valign="top"><p>Server,Client</p></td></tr><tr><td align="left" valign="top"><p>kube-etcd-server</p></td><td align="left" valign="top"><p>etcd-ca</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/etcd/server.crt,key</p></td><td align="left" valign="top"><p>Server,Client</p></td></tr><tr><td align="left" valign="top"><p>kubelet-ca</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>/var/lib/kubeket/pki/kubelet-ca.crt,key</p></td><td align="left" valign="top"><p>CA</p></td></tr><tr><td align="left" valign="top"><p>system:node:&lt;nodeName&gt;</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/var/lib/kubeket/pki/kubelet-client-current.pem</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>system:node:&lt;nodeName&gt;</p></td><td align="left" valign="top"><p>kubelet-ca</p></td><td align="left" valign="top"><p>/var/lib/kubelet/pki/kubelet-server-current.pem</p></td><td align="left" valign="top"><p>Server</p></td></tr><tr><td align="left" valign="top"><p>front-proxy-ca</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>/etc/kubernetes/pki/front-proxy-ca.crt,key</p></td><td align="left" valign="top"><p>CA</p></td></tr><tr><td align="left" valign="top"><p>front-proxy-client</p></td><td align="left" valign="top"><p>front-proxy-ca</p></td><td align="left" valign="top"><p>/etc/kubernetes/pki/front-proxy-client.crt,key</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>kubernetes-admin</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/etc/kubernetes/admin.conf</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>system:kube-controller-manager</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/etc/kubernetes/controller-manager.conf</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>system:kube-scheduler</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/etc/kubernetes/scheduler.conf</p></td><td align="left" valign="top"><p>Client</p></td></tr><tr><td align="left" valign="top"><p>system:node:&lt;nodeName&gt;</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>/etc/kubernetes/kubelet.conf</p></td><td align="left" valign="top"><p>Client</p></td></tr></tbody></table></div><div id="id-1.8.10.5.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>If a node was bootstrapped/joined before Kubernetes version 1.17, you have to manually modify the contents of <code class="literal">kubelet.conf</code> to point to the automatically rotated kubelet client certificates by replacing <code class="literal">client-certificate-data</code> and <code class="literal">client-key-data</code> with:</p><div class="verbatim-wrap highlight bash"><pre class="screen">client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
client-key: /var/lib/kubelet/pki/kubelet-client-current.pem</pre></div></div><p>The addon certificates stored in the Kubernetes cluster Secret resource:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><thead><tr><th align="left" valign="top">Common Name</th><th align="left" valign="top">Parent CA</th><th align="left" valign="top">Secret Resource Name</th><th align="left" valign="top">Kind</th></tr></thead><tbody><tr><td align="left" valign="top"><p>oidc-dex</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>oidc-dex-cert</p></td><td align="left" valign="top"><p>Server</p></td></tr><tr><td align="left" valign="top"><p>oidc-gangway</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>oidc-gangway-cert</p></td><td align="left" valign="top"><p>Server</p></td></tr><tr><td align="left" valign="top"><p>metrics-server</p></td><td align="left" valign="top"><p>kubernetes</p></td><td align="left" valign="top"><p>metrics-server-cert</p></td><td align="left" valign="top"><p>Server</p></td></tr><tr><td align="left" valign="top"><p>cilium-etcd-client</p></td><td align="left" valign="top"><p>etcd-ca</p></td><td align="left" valign="top"><p>cilium-secret</p></td><td align="left" valign="top"><p>Client</p></td></tr></tbody></table></div></div><div class="sect2" id="_monitoring_certificates"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Certificates</span> <a title="Permalink" class="permalink" href="#_monitoring_certificates">#</a></h3></div></div></div><p>We use cert-exporter to monitor nodes' on-host certificates and addons' secret certificates. The cert-exporter collects the metrics of certificates expiration periodically (1 hour by default) and exposes them through the <code class="literal">/metrics</code> endpoint. Then, the Prometheus server can scrape these metrics from the endpoint periodically.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com
helm install suse/cert-exporter --name &lt;RELEASE_NAME&gt;</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com
helm install &lt;RELEASE_NAME&gt; suse/cert-exporter</pre></div><div class="sect3" id="_prerequisites_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To monitor certificates, we need to set up monitoring stack by following the <a class="xref" href="#monitoring-stack" title="8.1. Monitoring Stack">Section 8.1, “Monitoring Stack”</a> on how to deploy it.</p></li><li class="listitem "><p>Label the skuba addon certificates</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl label --overwrite secret oidc-dex-cert -n kube-system caasp.suse.com/skuba-addon=true
kubectl label --overwrite secret oidc-gangway-cert -n kube-system caasp.suse.com/skuba-addon=true
kubectl label --overwrite secret metrics-server-cert -n kube-system caasp.suse.com/skuba-addon=true
kubectl label --overwrite secret cilium-secret -n kube-system caasp.suse.com/skuba-addon=true</pre></div><div id="id-1.8.10.6.6.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You might see the following console output:</p><div class="verbatim-wrap highlight bash"><pre class="screen">secret/oidc-dex-cert not labeled
secret/oidc-gangway-cert not labeled
secret/metrics-server-cert not labeled
secret/cilium-secret not labeled</pre></div><p>This is because <code class="literal">skuba</code> has already added the labels for you.</p></div></li></ol></div></div><div class="sect3" id="_prometheus_alerts"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prometheus Alerts</span> <a title="Permalink" class="permalink" href="#_prometheus_alerts">#</a></h4></div></div></div><p>Use Prometheus alerts to reactively receive the status of the certificates, follow the <a class="xref" href="#alertmanager-configuration-example" title="8.1.3.2.3. Alertmanager Configuration Example">Section 8.1.3.2.3, “Alertmanager Configuration Example”</a> on how to configure the Prometheus Alertmanager and Prometheus Server.</p></div><div class="sect3" id="_grafana_dashboards"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Grafana Dashboards</span> <a title="Permalink" class="permalink" href="#_grafana_dashboards">#</a></h4></div></div></div><p>Use Grafana to proactively monitor the status of the certificates, follow the <a class="xref" href="#adding-grafana-dashboards" title="8.1.3.2.6. Adding Grafana Dashboards">Section 8.1.3.2.6, “Adding Grafana Dashboards”</a> to install the Grafana dashboard to monitors certificates.</p></div><div class="sect3" id="_monitor_custom_secret_certificates"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitor Custom Secret Certificates</span> <a title="Permalink" class="permalink" href="#_monitor_custom_secret_certificates">#</a></h4></div></div></div><p>You can monitor custom secret TLS certificates that you created manually or using <a class="link" href="https://cert-manager.io/" target="_blank">cert-manager</a>.</p><p>For example:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Monitor cert-manager issued certificates in the <code class="literal">cert-managert-test</code> namespace.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install suse/cert-exporter \
    --name &lt;RELEASE_NAME&gt; \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=cert-manager \
    --set customSecret.certs[0].namespace=cert-manager-test \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].annotationSelector="{cert-manager.io/certificate-name}"</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install &lt;RELEASE_NAME&gt; suse/cert-exporter \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=cert-manager \
    --set customSecret.certs[0].namespace=cert-manager-test \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].annotationSelector="{cert-manager.io/certificate-name}"</pre></div></li><li class="listitem "><p>Monitor certificates in all namespaces filtered by label selector.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install suse/cert-exporter \
    --name &lt;RELEASE_NAME&gt; \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=self-signed-cert \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].labelSelector="{key=value}"</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install &lt;RELEASE_NAME&gt; suse/cert-exporter \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=self-signed-cert \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].labelSelector="{key=value}"</pre></div></li><li class="listitem "><p>Deploy both 1. and 2. together.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install suse/cert-exporter \
    --name &lt;RELEASE_NAME&gt; \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=cert-manager \
    --set customSecret.certs[0].namespace=cert-manager-test \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].annotationSelector="{cert-manager.io/certificate-name}" \
    --set customSecret.certs[1].name=self-signed-cert \
    --set customSecret.certs[1].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[1].labelSelector="{key=value}"</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install &lt;RELEASE_NAME&gt; suse/cert-exporter \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=cert-manager \
    --set customSecret.certs[0].namespace=cert-manager-test \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].annotationSelector="{cert-manager.io/certificate-name}" \
    --set customSecret.certs[1].name=self-signed-cert \
    --set customSecret.certs[1].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[1].labelSelector="{key=value}"</pre></div></li><li class="listitem "><p>Monitor custom certificates only, disregarding node and addon certificates.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install suse/cert-exporter \
    --name &lt;RELEASE_NAME&gt; \
    --set node.enabled=false \
    --set addon.enabled=false \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=cert-manager \
    --set customSecret.certs[0].namespace=cert-manager-test \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].annotationSelector="{cert-manager.io/certificate-name}" \
    --set customSecret.certs[1].name=self-signed-cert \
    --set customSecret.certs[1].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[1].labelSelector="{key=value}"</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install &lt;RELEASE_NAME&gt; suse/cert-exporter \
    --set node.enabled=false \
    --set addon.enabled=false \
    --set customSecret.enabled=true \
    --set customSecret.certs[0].name=cert-manager \
    --set customSecret.certs[0].namespace=cert-manager-test \
    --set customSecret.certs[0].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[0].annotationSelector="{cert-manager.io/certificate-name}" \
    --set customSecret.certs[1].name=self-signed-cert \
    --set customSecret.certs[1].includeKeys="{*.crt,*.pem}" \
    --set customSecret.certs[1].labelSelector="{key=value}"</pre></div></li></ol></div></div></div><div class="sect2" id="_deployment_with_a_custom_ca_certificate"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment with a Custom CA Certificate</span> <a title="Permalink" class="permalink" href="#_deployment_with_a_custom_ca_certificate">#</a></h3></div></div></div><div id="id-1.8.10.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Please plan carefully when deploying with a custom CA certificate. This certificate
can not be reconfigured once deployed and requires a full re-installation of the
cluster to replace.</p></div><p>Administrators can provide custom CA certificates (root CAs or intermediate CAs)
during cluster deployment and decide which CA components to replace (multiple CA certificates) or if to replace all with a single CA certificate.</p><p>After you have run <code class="literal">skuba cluster init</code>, go to the <code class="literal">&lt;CLUSTER_NAME&gt;</code> folder that has been generated,
Create a <code class="literal">pki</code> folder and put your custom CA certificate into the <code class="literal">pki</code> folder.</p><div id="id-1.8.10.7.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Extracting Certificate And Key From Combined PEM File</h6><p>Some PKIs will issue certificates and keys in a combined <code class="literal">.pem</code> file.
In order to use the contained certificate, you must extract them into separate files using <code class="literal">openssl</code>.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Extract the certificate:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -in /path/to/file.pem -out /path/to/file.crt</pre></div></li><li class="listitem "><p>Extract the key:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl rsa -in /path/to/file.pem -out /path/to/file.key</pre></div></li></ol></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Replacing the Kubernetes CA certificate:</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki
cp &lt;CUSTOM_APISERVER_CA_CERT_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/ca.crt
cp &lt;CUSTOM_APISERVER_CA_KEY_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/ca.key
chmod 644 &lt;CLUSTER_NAME&gt;/pki/ca.crt
chmod 600 &lt;CLUSTER_NAME&gt;/pki/ca.key</pre></div></li><li class="listitem "><p>Replacing the <code class="literal">etcd</code> CA certificate:</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki/etcd
cp &lt;CUSTOM_ETCD_CA_CERT_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/etcd/ca.crt
cp &lt;CUSTOM_ETCD_CA_KEY_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/etcd/ca.key
chmod 644 &lt;CLUSTER_NAME&gt;/pki/etcd/ca.crt
chmod 600 &lt;CLUSTER_NAME&gt;/pki/etcd/ca.key</pre></div></li><li class="listitem "><p>Replacing the <code class="literal">kubelet</code> CA certificate:</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki
cp &lt;CUSTOM_KUBELET_CA_CERT_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/kubelet-ca.crt
cp &lt;CUSTOM_KUBELET_CA_KEY_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/kubelet-ca.key
chmod 644 &lt;CLUSTER_NAME&gt;/pki/kubelet-ca.crt
chmod 600 &lt;CLUSTER_NAME&gt;/pki/kubelet-ca.key</pre></div></li><li class="listitem "><p>Replacing the <code class="literal">front-end proxy</code> CA certificate:</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki
cp &lt;CUSTOM_FRONTPROXY_CA_CERT_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/front-proxy-ca.crt
cp &lt;CUSTOM_FRONTPROXY_CA_KEY_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/front-proxy-ca.key
chmod 644 &lt;CLUSTER_NAME&gt;/pki/front-proxy-ca.crt
chmod 600 &lt;CLUSTER_NAME&gt;/pki/front-proxy-ca.key</pre></div></li></ul></div><p>After this process, bootstrap the cluster with <code class="literal">skuba node bootstrap</code>.</p></div><div class="sect2" id="_replace_oidc_server_certificate_signed_by_a_trusted_ca_certificate"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replace OIDC Server Certificate Signed By A Trusted CA Certificate</span> <a title="Permalink" class="permalink" href="#_replace_oidc_server_certificate_signed_by_a_trusted_ca_certificate">#</a></h3></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>During Cluster Deployment:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>With a Trusted CA Key:</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki
cp &lt;CUSTOM_OIDC_CA_CERT_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/oidc-ca.crt
cp &lt;CUSTOM_OIDC_CA_KEY_PATH&gt; &lt;CLUSTER_NAME&gt;/pki/oidc-ca.key
chmod 644 &lt;CLUSTER_NAME&gt;/pki/oidc-ca.crt
chmod 600 &lt;CLUSTER_NAME&gt;/pki/oidc-ca.key</pre></div><p>After this process, bootstrap the cluster with <code class="literal">skuba node bootstrap</code>.
skuba uploads the local OIDC CA certificate to the remote path specified in the local file <code class="literal">kubeadm-init.conf</code> key <code class="literal">oidc-ca-file</code>. When installing the add-on, skuba generates the OIDC server certificates and keys which are signed by the provided OIDC CA certificate and key pair and then stored to its Secret resource.</p></li><li class="listitem "><p>Without a Trusted CA Key:</p><p>Use command <code class="literal">skuba cert generate-csr</code> to generate the OIDC server CSRs and keys in <code class="literal">&lt;my-cluster&gt;/pki</code> folder.
After the CA signs the CSRs and issued the server certificates, put the OIDC CA certificate and the OIDC server certificates in <code class="literal">&lt;my-cluster&gt;/pki</code> folder.</p><p>After this process, bootstrap the cluster with <code class="literal">skuba node bootstrap</code>.
skuba uploads the local OIDC CA certificate to the remote path specified in the local file <code class="literal">kubeadm-init.conf</code> key <code class="literal">oidc-ca-file</code>. At the time installing the add-on, skuba uploads the OIDC CA certificate and OIDC server certificate and key pair to its Secret resource.</p></li></ul></div></li><li class="listitem "><p>After Cluster Deployment:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>With a Trusted CA Key:</p><p>Please refer to <a class="xref" href="#addon-certificate-rotation" title="6.9.7.3. Addon Certificate Rotation">Section 6.9.7.3, “Addon Certificate Rotation”</a> on how to use cert-manager and reloader to issue <code class="literal">oidc-dex</code> and <code class="literal">oidc-gangway</code> certificates signed by trusted CA certificate/key.</p></li><li class="listitem "><p>Without a Trusted CA Key:</p><div id="id-1.8.10.8.2.2.2.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Because the custom trusted CA certificate is not in the Kubernetes cluster, administrators must handle server certificate rotation manually before the certificate expires.</p></div><div id="id-1.8.10.8.2.2.2.2.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>The <code class="literal">oidc-dex</code> and <code class="literal">oidc-gangway</code> server certificate and key is replaced when <code class="literal">skuba addon upgrade apply</code> contains a dex or gangway addon upgrade.
Make sure to reapply your changes after running <code class="literal">skuba addon upgrade apply</code>, had you modified the default settings of oidc-dex and oidc-gangway addons.</p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Replace the <code class="literal">oidc-dex</code> server certificate:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the original <code class="literal">oidc-dex</code> server certificate and key from secret resource.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p pki.bak
kubectl get secret oidc-dex-cert -n kube-system -o yaml | tee pki.bak/oidc-dex-cert.yaml &gt; /dev/null

cat pki.bak/oidc-dex-cert.yaml | grep tls.crt | awk '{print $2}' | base64 --decode | tee pki.bak/oidc-dex.crt &gt; /dev/null
cat pki.bak/oidc-dex-cert.yaml | grep tls.key | awk '{print $2}' | base64 --decode | tee pki.bak/oidc-dex.key &gt; /dev/null</pre></div></li><li class="listitem "><p>Get the original SAN IP address(es) and DNS(s), run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -noout -text -in pki.bak/oidc-dex.crt | grep -oP '(?&lt;=IP Address:)[^,]+'
openssl x509 -noout -text -in pki.bak/oidc-dex.crt | grep -oP '(?&lt;=DNS:)[^,]+'</pre></div></li><li class="listitem "><p>Sign the <code class="literal">oidc-dex</code> server certificate with the trusted CA certificate.</p><p>Please refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> on how to sign the trusted certificate. The <code class="literal">server.conf</code> for IP.1 is the original SAN IP address if present, DNS.1 is the original SAN DNS if present.</p><p>Then, import your trusted certificate into the Kubernetes cluster.
The trusted CA certificates is <code class="literal">&lt;TRUSTED_CA_CERT_PATH&gt;</code>, trusted server certificate and key are <code class="literal">&lt;SIGNED_OIDC_DEX_SERVER_CERT_PATH&gt;</code> and <code class="literal">&lt;SIGNED_OIDC_DEX_SERVER_KEY_PATH&gt;</code>.</p></li><li class="listitem "><p>Create a secret manifest file <code class="literal">oidc-dex-cert.yaml</code> and update the secret data <code class="literal">ca.crt</code>, <code class="literal">tls.crt</code>, and <code class="literal">tls.key</code> with base64; encoded with trusted CA certificate, signed oidc-dex server certificate and key respectively.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: oidc-dex-cert
  namespace: kube-system
  labels:
    caasp.suse.com/skuba-addon: "true"
type: kubernetes.io/tls
data:
  ca.crt: cat &lt;TRUSTED_CA_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.crt: cat &lt;SIGNED_OIDC_DEX_SERVER_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.key: cat &lt;SIGNED_OIDC_DEX_SERVER_KEY_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</pre></div></li><li class="listitem "><p>Apply the secret manifest file and restart <code class="literal">oidc-dex</code> pods.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl replace -f oidc-dex-cert.yaml
kubectl rollout restart deployment/oidc-dex -n kube-system</pre></div></li></ol></div></li><li class="listitem "><p>Replace the <code class="literal">oidc-gangway</code> server certificate:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the original <code class="literal">oidc-gangway</code> server certificate and key from secret resource.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p pki.bak
kubectl get secret oidc-gangway-cert -n kube-system -o yaml | tee pki.bak/oidc-gangway-cert.yaml &gt; /dev/null

cat pki.bak/oidc-gangway-cert.yaml | grep tls.crt | awk '{print $2}' | base64 --decode | tee pki.bak/oidc-gangway.crt &gt; /dev/null
cat pki.bak/oidc-gangway-cert.yaml | grep tls.key | awk '{print $2}' | base64 --decode | tee pki.bak/oidc-gangway.key &gt; /dev/null</pre></div></li><li class="listitem "><p>Get the original SAN IP address(es) and DNS(s), run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -noout -text -in pki.bak/oidc-gangway.crt | grep -oP '(?&lt;=IP Address:)[^,]+'
openssl x509 -noout -text -in pki.bak/oidc-gangway.crt | grep -oP '(?&lt;=DNS:)[^,]+'</pre></div></li><li class="listitem "><p>Sign the <code class="literal">oidc-gangway</code> server certificate with the trusted CA certificate.</p><p>Please refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> on how to sign the trusted certificate. The <code class="literal">server.conf</code> for IP.1 is the original SAN IP address if present, DNS.1 is the original SAN DNS if present.</p><p>Then, import your trusted certificate into the Kubernetes cluster.
The trusted CA certificates is <code class="literal">&lt;TRUSTED_CA_CERT_PATH&gt;</code>, trusted server certificate and key are <code class="literal">&lt;SIGNED_OIDC_GANGWAY_SERVER_CERT_PATH&gt;</code> and <code class="literal">&lt;SIGNED_OIDC_GANGWAY_SERVER_KEY_PATH&gt;</code>.</p></li><li class="listitem "><p>Create a secret manifest file <code class="literal">oidc-gangway-cert.yaml</code> and update the secret data <code class="literal">ca.crt</code>, <code class="literal">tls.crt</code>, and <code class="literal">tls.key</code> with base64; encoded with trusted CA certificate, signed <code class="literal">oidc-gangway</code> server certificate and key respectively.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: oidc-gangway-cert
  namespace: kube-system
  labels:
    caasp.suse.com/skuba-addon: "true"
type: kubernetes.io/tls
data:
  ca.crt: cat &lt;TRUSTED_CA_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.crt: cat &lt;SIGNED_OIDC_GANGWAY_SERVER_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.key: cat &lt;SIGNED_OIDC_GANGWAY_SERVER_KEY_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</pre></div></li><li class="listitem "><p>Apply the secret manifest file and restart <code class="literal">oidc-gangway</code> pods.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl replace -f oidc-gangway-cert.yaml
kubectl rollout restart deployment/oidc-gangway -n kube-system</pre></div></li></ol></div></li><li class="listitem "><p>Replace the OIDC CA for <code class="literal">kube-apiserver</code>:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Copy custom OIDC CA which was used for <code class="literal">oidc-dex</code> and <code class="literal">oidc-gangway</code> to <code class="literal">/etc/kubernetes/pki/oidc-ca.crt</code> on all SUSE CaaS Platform master nodes</p><div class="verbatim-wrap highlight bash"><pre class="screen">ssh &lt;USERNAME&gt;@&lt;MASTER_NODE_IP_ADDRESS/FQDN&gt;
sudo mv oidc-ca.crt /etc/kubernetes/pki/oidc-ca.crt</pre></div></li><li class="listitem "><p>Update <code class="literal">oidc-ca-file</code> option in <code class="literal">kubeadm</code> configmap</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get configmap -n kube-system kubeadm-config -o yaml &gt; kubeadm-config.yaml
sed -i "s|oidc-ca-file: .*|oidc-ca-file: /etc/kubernetes/pki/oidc-ca.crt|" kubeadm-config.yaml
kubectl apply -f kubeadm-config.yaml</pre></div></li><li class="listitem "><p>Update <code class="literal">oidc-ca-file</code> in static pod manifest for kube-apiserver in <code class="literal">/etc/kubernetes/manifests/kube-apiserver.yaml</code> on all SUSE CaaS Platform master nodes</p><div class="verbatim-wrap highlight bash"><pre class="screen">sed -i "s|oidc-ca-file=.*|oidc-ca-file=/etc/kubernetes/pki/oidc-ca.crt|" /etc/kubernetes/manifests/kube-apiserver.yaml</pre></div></li></ol></div></li></ul></div></li></ul></div></li></ul></div></div><div class="sect2" id="_automatic_certificate_renewal"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Automatic Certificate Renewal</span> <a title="Permalink" class="permalink" href="#_automatic_certificate_renewal">#</a></h3></div></div></div><p>SUSE CaaS Platform renews the control plane certificates and kubeconfigs automatically in two ways:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><span class="strong"><strong>During node upgrade</strong></span>:
When the node is upgraded, all the <code class="literal">kubeadm</code> managed certificates and kubeconfigs get rotated.
The time to rotate the kubelet client and server certificate is controlled by <code class="literal">kubelet</code> daemon.</p><div id="id-1.8.10.9.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>During node upgrade, neither the <code class="literal">kubelet</code> client certificate nor server certificate get rotated.</p></div></li><li class="listitem "><p><span class="strong"><strong>Via the <code class="literal">kucero</code> addon</strong></span>:
If the administrator is not able to upgrade the cluster, the <code class="literal">kucero</code> (KUbernetes control plane CErtificate ROtation) addon rotates all the <code class="literal">kubeadm</code> managed certificates and kubeconfigs and signs <code class="literal">kubelet</code> server CSR.
<code class="literal">kucero</code> is a <code class="literal">kubeadm</code> checker/renewer in the form of a DaemonSet. Its job is to periodically check and renew control plane <code class="literal">kubeadm</code> managed certificates/kubeconfigs, and check the <code class="literal">kubelet</code> client and server enables auto rotation, and also a signer to sign <code class="literal">kubelet</code> server CSR.</p></li></ol></div><div id="id-1.8.10.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Time to rotate the kubelet client and server certificate</h6><p>The <code class="literal">kubelet</code> client and server certificate renews automatically at approximately 70%-90% of the total lifetime of the certificate, the <code class="literal">kubelet</code> daemon would use new client and server certificates without downtime.</p></div><div id="id-1.8.10.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Kubelet client and server certificate signing flow</h6><p>The configuration which controls the <code class="literal">kubelet</code> daemon to send out the CSR within the Kubernetes cluster is controlled by the configuration <code class="literal">/var/lib/kubelet/config.yaml</code>.
The key <code class="literal">rotateCertificates</code> controls the kubelet client certificate; the key <code class="literal">serverTLSBootstrap</code> controls the kubelet server certificate.</p><p>When the client or server certificate is going to expire, the <code class="literal">kubelet</code> daemon sends the <code class="literal">kubelet</code> client or server CSR within the Kubernetes cluster.
The <code class="literal">kube-controller-manager</code> signs the kubelet client CSR with the Kubernetes CA cert/key pair, <code class="literal">kucero</code> addon signs the <code class="literal">kubelet</code> server CSR with the <code class="literal">kubelet</code> CA cert/key pair.
Then, the <code class="literal">kubelet</code> daemon saves the signed certificate under the folder <code class="literal">/var/lib/kubelet/pki</code> and updates the client or server certificate symlink points to the latest signed certificate.</p><p>The path of <code class="literal">kubelet</code> client certificate is <code class="literal">/var/lib/kubelet/pki/kubelet-client-current.pem</code>.
The path of <code class="literal">kubelet</code> server certificate is <code class="literal">/var/lib/kubelet/pki/kubelet-server-current.pem</code>.</p></div><div class="sect3" id="_control_plane_nodes_certificate_rotation"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane Nodes Certificate Rotation</span> <a title="Permalink" class="permalink" href="#_control_plane_nodes_certificate_rotation">#</a></h4></div></div></div><p>Control Plane Node Certificates are rotated in two ways:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><span class="strong"><strong>During node upgrade</strong></span>:
when doing a control plane update, <code class="literal">skuba node upgrade apply</code> runs <code class="literal">kubeadm upgrade</code> commands behind the scenes. <code class="literal">kubeadm upgrade apply</code> and
<code class="literal">kubeadm upgrade node</code> renews and uses new <code class="literal">kubeadm</code> managed certificates on the node, including those stored in kubeconfig files, regardless of the remaining time for which the certificate was still valid.</p></li><li class="listitem "><p><span class="strong"><strong>Via the <code class="literal">kucero</code> addon:</strong></span></p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p><code class="literal">kubeadm</code> managed certificates/kubeconfigs:
a <code class="literal">kubeadm</code> checker/renewer to periodical checks (default interval is 1 hour) the kubeadm managed certificates/kubeconfigs, and rotates the certificates/kubeconfigs if the residual time is less than the total time (default 720 hours).
Administrators can change the default time to renew the certificates/kubeconfigs by adding <code class="literal">--renew-before=&lt;duration&gt;`</code> (duration format is XhYmZs) to the kucero daemonset <span class="emphasis"><em>or</em></span> change the default polling period for checking the certificates/kubeconfigs by adding <code class="literal">--polling-period=&lt;duration&gt;</code> (duration format is <code class="literal">XhYmZs</code>).</p></li><li class="listitem "><p><code class="literal">kubelet</code> client and server certificates:
A <code class="literal">kubelet</code> configuration checker/updater periodically checks (default interval is 1 hour) if the kubelet configuration enables the client and server auto rotation.
If not, kucero will help enable the client and server auto-rotation by configuring <code class="literal">rotateCertificates: true</code> and <code class="literal">serverTLSBootstrap: true</code> in <code class="literal">/var/lib/kubelet/config.yaml</code>.
After that, the <code class="literal">kubelet</code> daemon will send out the CSR within the Kubernetes cluster if the client or server is going to expire, the corresponding CSR signer and approver will sign and approve the CSR, then the <code class="literal">kubelet</code> daemon saves the signed certificate under the folder <code class="literal">/var/lib/kubelet/pki</code> and updates the symlink points to the latest signed certificate.</p></li></ol></div></li></ol></div></div><div class="sect3" id="_worker_node_certificate_rotation"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Worker Node Certificate Rotation</span> <a title="Permalink" class="permalink" href="#_worker_node_certificate_rotation">#</a></h4></div></div></div><p>Worker Node Certificates are rotated in one way:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><span class="strong"><strong>Via the kucero addon:</strong></span></p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p><code class="literal">kubelet</code> client and server certificates:
A <code class="literal">kubelet</code> configuration checker/updater periodically checks (default interval is 1 hour) if the kubelet configuration enables the client and server auto rotation.
If not, kucero will help enable the client and server auto-rotation by configuring <code class="literal">rotateCertificates: true</code> and <code class="literal">serverTLSBootstrap: true</code> in <code class="literal">/var/lib/kubelet/config.yaml</code>.
After that, the <code class="literal">kubelet</code> daemon will send out the CSR within the Kubernetes cluster if the client or server is going to expire, the corresponding CSR signer and approver will sign and approve the CSR, then the <code class="literal">kubelet</code> daemon saves the signed certificate under the folder <code class="literal">/var/lib/kubelet/pki</code> and updates the symlink points to the latest signed certificate.</p></li></ol></div></li></ol></div></div><div class="sect3" id="addon-certificate-rotation"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Addon Certificate Rotation</span> <a title="Permalink" class="permalink" href="#addon-certificate-rotation">#</a></h4></div></div></div><p>The addon certificates can be automatically rotated by leveraging the functions of the open-source solutions <code class="literal">cert-manager</code> and <code class="literal">reloader</code>. <code class="literal">cert-manager</code> is for automatically rotating certificates stored in Secrets, and <code class="literal">reloader</code> is for watching and reconciling the updated Secrets to execute a rolling upgrade of the affected Deployments or DaemonSet.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install reloader via helm chart:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name &lt;RELEASE_NAME&gt; \
    --namespace cert-manager \
     suse/reloader</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install &lt;RELEASE_NAME&gt; \
    --namespace cert-manager \
    --create-namespace \
    suse/reloader</pre></div></li><li class="listitem "><p>Install cert-manager via helm chart:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name &lt;RELEASE_NAME&gt; \
    --namespace cert-manager \
    --set global.leaderElection.namespace=cert-manager \
    --set installCRDs=true \
    suse/cert-manager</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install &lt;RELEASE_NAME&gt; \
    --namespace cert-manager \
    --create-namespace \
    --set global.leaderElection.namespace=cert-manager \
    --set installCRDs=true \
    suse/cert-manager</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Cert-Manager CA Issuer Resource</p><p>The cert-manager CA issuer is a Kubernetes resource that represents a certificate authority (CA), which can generate signed certificates by honoring certificate signing requests (CSR). Each cert-manager certificate resource requires one referenced issuer in the ready state to be able to honor CSR requests.</p><div id="id-1.8.10.9.8.3.2.5.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>An <code class="literal">Issuer</code> is a namespaced resource, and it can not issue certificates to the certificate resources in other namespaces.</p><p>If you want to create a single Issuer that can be consumed in multiple namespaces, you should consider creating a <code class="literal">ClusterIssuer</code> resource. This is almost identical to the Issuer resource, however, it is cluster-wide so it can be used to issue certificates in all namespaces.</p></div></li><li class="listitem "><p>Cert-Manager Certificate Resource</p><p>The cert-manager has a custom resource, Certificate, which can be used to define a requested x509 certificate which will be renewed and kept up to date by an Issuer or ClusterIssuer resource.</p></li></ul></div></li></ol></div><div class="sect4" id="_client_certificate_rotation"><div class="titlepage"><div><div><h5 class="title"><span class="number">6.9.7.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Client Certificate Rotation</span> <a title="Permalink" class="permalink" href="#_client_certificate_rotation">#</a></h5></div></div></div><div id="id-1.8.10.9.8.4.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>If you are running a cluster using cilium version before 1.6, the cilium data is stored in the ETCD cluster, not the custom resources (CR). `skuba` generates a client certificate to read/write the cilium date to the ETCD cluster and the client certificate will expire after 1 year. Please follow the below steps to use cert-manager to automatically renew the cilium client certificate.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Check the SUSE CaaS Platform cilium version before 1.6</p><div class="verbatim-wrap highlight bash"><pre class="screen">CILIUM_OPERATOR=`kubectl get pod -l name=cilium-operator --namespace kube-system -o jsonpath='{.items[0].metadata.name}'`
kubectl exec -it ${CILIUM_OPERATOR} --namespace kube-system -- cilium-operator --version</pre></div></li><li class="listitem "><p>To let <code class="literal">reloader</code> do an automatic rolling upgrade of the cilium addon DaemonSet, we need to label the addons:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl annotate --overwrite daemonset/cilium -n kube-system secret.reloader.stakater.com/reload=cilium-secret</pre></div></li><li class="listitem "><p>Upload the ETCD CA cert/key pair to Secret in the <code class="literal">kube-system</code> namespace</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create secret tls etcd-ca --cert=pki/etcd/ca.crt --key=pki/etcd/ca.key -n kube-system</pre></div></li><li class="listitem "><p>Create a Cert-Manager CA Issuer Resource</p><p>Create a CA issuer called <code class="literal">etcd-ca</code> that will sign incoming certificate requests based on the CA certificate and private key stored in the secret <code class="literal">etcd-ca</code> used to trust newly signed certificates.</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; issuer-etcd-ca.yaml
apiVersion: cert-manager.io/v1alpha3
kind: Issuer
metadata:
  name: etcd-ca
  namespace: kube-system
spec:
  ca:
    secretName: etcd-ca
EOF

kubectl create -f issuer-etcd-ca.yaml</pre></div></li><li class="listitem "><p>Create a Cert-Manager Certificate Resource</p><p>Create a certificate resource <code class="literal">cilium-etcd-client</code> that will watch and auto-renews the secret <code class="literal">cilium-secret</code> if the certificate residual time is less than the <code class="literal">renewBefore</code> value.</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; cilium-etcd-client-certificate.yaml
apiVersion: cert-manager.io/v1alpha3
kind: Certificate
metadata:
  name: cilium-etcd-client-cert
  namespace: kube-system
spec:
  subject:
    organizations:
    - system:masters
  commonName: cilium-etcd-client
  duration: 8760h # 1 year
  renewBefore: 720h # 1 month
  secretName: cilium-secret
  issuerRef:
    name: etcd-ca
    kind: Issuer
    group: cert-manager.io
  isCA: false
  usages:
    - digital signature
    - key encipherment
    - client auth
  keySize: 2048
  keyAlgorithm: rsa
  keyEncoding: pkcs1
EOF

kubectl create -f cilium-etcd-client-certificate.yaml</pre></div></li></ol></div></div><div class="sect4" id="_server_certificates_rotation"><div class="titlepage"><div><div><h5 class="title"><span class="number">6.9.7.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Certificates Rotation</span> <a title="Permalink" class="permalink" href="#_server_certificates_rotation">#</a></h5></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Prerequisites</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To let <code class="literal">reloader</code> do an automatic rolling upgrade of the addon Deployments or DaemonSet, we need to label the addons:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl annotate --overwrite deployment/oidc-dex -n kube-system secret.reloader.stakater.com/reload=oidc-dex-cert

kubectl annotate --overwrite deployment/oidc-gangway -n kube-system secret.reloader.stakater.com/reload=oidc-gangway-cert

kubectl annotate --overwrite deployment/metrics-server -n kube-system secret.reloader.stakater.com/reload=metrics-server-cert</pre></div></li><li class="listitem "><p>Upload the Kubernetes CA cert/key pair to Secret in the <code class="literal">kube-system</code> namespace:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create secret tls kubernetes-ca --cert=pki/ca.crt --key=pki/ca.key -n kube-system</pre></div><div id="id-1.8.10.9.8.5.2.1.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you want to use a custom trusted CA certificate/key to sign the certificate, upload to the secret resource.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create secret tls custom-trusted-ca --cert=&lt;CUSTOM_TRUSTED_CA_CERT&gt; --key=&lt;CUSTOM_TRUSTED_CA_KEY&gt; -n kube-system</pre></div></div></li></ol></div></li><li class="listitem "><p>Create a Cert-Manager CA Issuer Resource</p><p>Create a CA issuer called <code class="literal">kubernetes-ca</code> that will sign incoming certificate requests based on the CA certificate and private key stored in the secret <code class="literal">kubernetes-ca</code> used to trust newly signed certificates.</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; issuer-kubernetes-ca.yaml
apiVersion: cert-manager.io/v1alpha3
kind: Issuer
metadata:
  name: kubernetes-ca <span id="CO12-1"></span><span class="callout">1</span>
  namespace: kube-system
spec:
  ca:
    secretName: kubernetes-ca <span id="CO12-2"></span><span class="callout">2</span>
EOF

kubectl create -f issuer-kubernetes-ca.yaml</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The issuer name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO12-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The secret reference name.</p></td></tr></table></div><div id="id-1.8.10.9.8.5.2.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you want to use custom trusted CA certificate/key to sign the certificate, create a custom trusted CA issuer.</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; custom-trusted-kubernetes-ca-issuer.yaml
apiVersion: cert-manager.io/v1alpha3
kind: Issuer <span id="CO13-1"></span><span class="callout">1</span>
metadata:
  name: custom-trusted-kubernetes-ca
  namespace: kube-system
spec:
  ca:
    secretName: custom-trusted-kubernetes-ca
EOF

kubectl create -f custom-trusted-kubernetes-ca-issuer.yaml</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO13-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Issuer or ClusterIssuer.</p></td></tr></table></div></div></li><li class="listitem "><p>Create a Cert-Manager Certificate Resource</p><p>Create a certificate resource that will watch and auto-renews the secret if the certificate residual time is less than the <code class="literal">renewBefore</code> value.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>oidc-dex certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; oidc-dex-certificate.yaml
apiVersion: cert-manager.io/v1alpha3
kind: Certificate
metadata:
  name: oidc-dex-cert
  namespace: kube-system
spec:
  subject:
    organizations:
    - system:masters
  commonName: oidc-dex
  duration: 8760h # 1 year <span id="CO14-1"></span><span class="callout">1</span>
  renewBefore: 720h # 1 month <span id="CO14-2"></span><span class="callout">2</span>
  # At least one of a DNS Name or IP address is required.
  dnsNames:
  - $(cat admin.conf | grep server | awk '{print $2}' | sed 's/https:\/\///g' | sed 's/:6443//g') <span id="CO14-3"></span><span class="callout">3</span>
  ipAddresses:
  - $(cat admin.conf | grep server | awk '{print $2}' | sed 's/https:\/\///g' | sed 's/:6443//g') <span id="CO14-4"></span><span class="callout">4</span>
  secretName: oidc-dex-cert
  issuerRef:
    name: kubernetes-ca <span id="CO14-5"></span><span class="callout">5</span>
    kind: Issuer <span id="CO14-6"></span><span class="callout">6</span>
    group: cert-manager.io
  isCA: false
  usages:
    - digital signature
    - key encipherment
    - server auth
  keySize: 2048
  keyAlgorithm: rsa
  keyEncoding: pkcs1
EOF

kubectl create -f oidc-dex-certificate.yaml</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Default length of certificate validity, in the format (XhYmZs).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Certificate renewal time before validity expires, in the format (XhYmZs).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>DNSNames is a list of subject alt names to be used on the Certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>IPAddresses is a list of IP addresses to be used on the Certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The cert-manager issuer name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO14-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Issuer or ClusterIssuer.</p></td></tr></table></div><p>This certificate will tell cert-manager to attempt to use the Issuer named kubernetes-ca to obtain a certificate key pair for the domain list in dnsNames and ipAddresses. If successful, the resulting key and certificate will be stored in a secret named oidc-dex-cert with keys of tls.key and tls.crt respectively.</p><p>The dnsNames and ipAddresses fields specify a list of Subject Alternative Names to be associated with the certificate.</p><p>The referenced Issuer must exist in the same namespace as the Certificate. A Certificate can alternatively reference a ClusterIssuer which is cluster-wide so it can be referenced from any namespace.</p><div id="id-1.8.10.9.8.5.2.3.3.1.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you want to use a custom trusted CA Issuer/ClusterIssuer, change the value of <code class="literal">name</code> under <code class="literal">issuerRef</code> to <code class="literal">custom-trusted-ca</code> and the value of  <code class="literal">kind</code> under <code class="literal">issuerRef</code> to <code class="literal">Issuer/ClusterIssuer</code>.</p></div></li><li class="listitem "><p>oidc-gangway certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; oidc-gangway-certificate.yaml
apiVersion: cert-manager.io/v1alpha3
kind: Certificate
metadata:
  name: oidc-gangway-cert
  namespace: kube-system
spec:
  subject:
    organizations:
    - system:masters
  commonName: oidc-gangway
  duration: 8760h # 1 year <span id="CO15-1"></span><span class="callout">1</span>
  renewBefore: 720h # 1 month <span id="CO15-2"></span><span class="callout">2</span>
  # At least one of a DNS Name or IP address is required.
  dnsNames:
  - $(cat admin.conf | grep server | awk '{print $2}' | sed 's/https:\/\///g' | sed 's/:6443//g') <span id="CO15-3"></span><span class="callout">3</span>
  ipAddresses:
  - $(cat admin.conf | grep server | awk '{print $2}' | sed 's/https:\/\///g' | sed 's/:6443//g') <span id="CO15-4"></span><span class="callout">4</span>
  secretName: oidc-gangway-cert
  issuerRef:
    name: kubernetes-ca <span id="CO15-5"></span><span class="callout">5</span>
    kind: Issuer <span id="CO15-6"></span><span class="callout">6</span>
    group: cert-manager.io
  isCA: false
  usages:
    - digital signature
    - key encipherment
    - server auth
  keySize: 2048
  keyAlgorithm: rsa
  keyEncoding: pkcs1
EOF

kubectl create -f oidc-gangway-certificate.yaml</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Default length of certificate validity, in the format (XhYmZs).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Certificate renewal time before validity expires, in the format (XhYmZs).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>DNSNames is a list of subject alt names to be used on the Certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>IPAddresses is a list of IP addresses to be used on the Certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The cert-manager issuer name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO15-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Issuer or ClusterIssuer.</p></td></tr></table></div><div id="id-1.8.10.9.8.5.2.3.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you want to use a custom trusted CA Issuer/ClusterIssuer, change the value of <code class="literal">name</code> under <code class="literal">issuerRef</code> to <code class="literal">custom-trusted-ca</code> and the value of  <code class="literal">kind</code> under <code class="literal">issuerRef</code> to <code class="literal">Issuer/ClusterIssuer</code>.</p></div></li><li class="listitem "><p>metrics-server certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; metrics-server-certificate.yaml
apiVersion: cert-manager.io/v1alpha3
kind: Certificate
metadata:
  name: metrics-server-cert
  namespace: kube-system
spec:
  subject:
    organizations:
    - system:masters
  commonName: metrics-server.kube-system.svc
  duration: 8760h # 1 year <span id="CO16-1"></span><span class="callout">1</span>
  renewBefore: 720h # 1 month <span id="CO16-2"></span><span class="callout">2</span>
  # At least one of a DNS Name or IP address is required.
  dnsNames:
  - $(cat admin.conf | grep server | awk '{print $2}' | sed 's/https:\/\///g' | sed 's/:6443//g') <span id="CO16-3"></span><span class="callout">3</span>
  ipAddresses:
  - $(cat admin.conf | grep server | awk '{print $2}' | sed 's/https:\/\///g' | sed 's/:6443//g') <span id="CO16-4"></span><span class="callout">4</span>
  secretName: metrics-server-cert
  issuerRef:
    name: kubernetes-ca <span id="CO16-5"></span><span class="callout">5</span>
    kind: Issuer <span id="CO16-6"></span><span class="callout">6</span>
    group: cert-manager.io
  isCA: false
  usages:
    - digital signature
    - key encipherment
    - server auth
  keySize: 2048
  keyAlgorithm: rsa
  keyEncoding: pkcs1
EOF

kubectl create -f metrics-server-certificate.yaml</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO16-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Default length of certificate validity, in the format (XhYmZs).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO16-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Certificate renewal time before validity expires, in the format (XhYmZs).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO16-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>DNSNames is a list of subject alt names to be used on the Certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO16-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>IPAddresses is a list of IP addresses to be used on the Certificate.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO16-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The cert-manager issuer name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO16-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Issuer or ClusterIssuer.</p></td></tr></table></div></li></ul></div></li></ul></div><div id="id-1.8.10.9.8.5.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Cert-Manager Known Issue</h6><p>Once the cert-manager has issued a certificate to the secret, if you change the certificate inside the secret manually, or you manually change the current certificate <code class="literal">duration</code> to a value lower than the value <code class="literal">renewBefore</code>, the certificate won’t be renewed immediately but will be scheduled to renew near the certificate expiry date.</p><p>This is because the cert-manager is not designed to pick up changes you make to the certificate in the secret.</p></div></div></div></div><div class="sect2" id="_manual_certificate_renewal"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manual Certificate Renewal</span> <a title="Permalink" class="permalink" href="#_manual_certificate_renewal">#</a></h3></div></div></div><div id="id-1.8.10.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If you are running multiple control plane nodes, you need to run the followings
commands sequentially on all control plane nodes.</p></div><div class="sect3" id="_renewing_control_plane_certificates"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Renewing Control Plane Certificates</span> <a title="Permalink" class="permalink" href="#_renewing_control_plane_certificates">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Replace kubeadm-managed certificates:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>To SSH into the control plane node, renew all <code class="literal">kubeadm</code> certificates and reboot,
run the following:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ssh &lt;USERNAME&gt;@&lt;MASTER_NODE_IP_ADDRESS/FQDN&gt;
sudo cp -r /etc/kubernetes/pki /etc/kubernetes/pki.bak
sudo kubeadm alpha certs renew all
sudo systemctl restart kubelet</pre></div></li><li class="listitem "><p>Copy the renewed <code class="literal">admin.conf</code> from one of the control plane nodes to your local environment:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ssh &lt;USERNAME&gt;@&lt;MASTER_NODE_IP_ADDRESS/FQDN&gt;
sudo cat /etc/kubernetes/admin.conf</pre></div></li></ol></div></li><li class="listitem "><p>Replace the <code class="literal">kubelet</code> server certificate:</p><div id="id-1.8.10.10.3.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>You need to generate <code class="literal">kubelet</code> server certificate for all the nodes on one of control plane nodes.
The <code class="literal">kubelet</code> CA certificate key only exists on the control plane nodes.
Therefore, after generating re-signed <code class="literal">kubelet</code> server certificate/key for worker nodes, you have to copy each <code class="literal">kubelet</code> server certificate/key from the control plane node to the corresponding worker node.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the original <code class="literal">kubelet</code> certificates and keys.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo cp -r /var/lib/kubelet/pki /var/lib/kubelet/pki.bak</pre></div></li><li class="listitem "><p>Sign each node <code class="literal">kubelet</code> server certificate with the CA certificate/key <code class="literal">/var/lib/kubelet/pki/kubelet-ca.crt</code> and <code class="literal">/var/lib/kubelet/pki/kubelet-ca.key</code>, make sure that the signed server certificate SAN is the same as the origin.
To get the original SAN IP address(es) and DNS(s), run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -noout -text -in /var/lib/kubelet/pki.bak/kubelet.crt | grep -oP '(?&lt;=IP Address:)[^,]+'
openssl x509 -noout -text -in /var/lib/kubelet/pki.bak/kubelet.crt | grep -oP '(?&lt;=DNS:)[^,]+'</pre></div></li><li class="listitem "><p>Finally, update the <code class="literal">kubelet</code> server certificate and key file <code class="literal">/var/lib/kubelet/kubelet.crt</code> and <code class="literal">/var/lib/kubelet/kubelet.key</code> respectively, and restart <code class="literal">kubelet</code> service.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo cp &lt;CUSTOM_KUBELET_SERVER_CERT_PATH&gt; /var/lib/kubelet/pki/kubelet.crt
sudo cp &lt;CUSTOM_KUBELET_SERVER_KEY_PATH&gt; /var/lib/kubelet/pki/kubelet.key
chmod 644 /var/lib/kubelet/pki/kubelet.crt
chmod 600 /var/lib/kubelet/pki/kubelet.key

sudo systemctl restart kubelet</pre></div></li></ol></div></li></ul></div></div><div class="sect3" id="_renewing_addon_certificates"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Renewing Addon Certificates:</span> <a title="Permalink" class="permalink" href="#_renewing_addon_certificates">#</a></h4></div></div></div><p>In the admin node, regenerate the certificates:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Replace the <code class="literal">oidc-dex</code> server certificate:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the original <code class="literal">oidc-dex</code> server certificate and key from secret resource.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki.bak
kubectl get secret oidc-dex-cert -n kube-system -o "jsonpath={.data['tls\.crt']}" | base64 --decode | tee &lt;CLUSTER_NAME&gt;/pki.bak/oidc-dex.crt &gt; /dev/null
kubectl get secret oidc-dex-cert -n kube-system -o "jsonpath={.data['tls\.key']}" | base64 --decode | tee &lt;CLUSTER_NAME&gt;/pki.bak/oidc-dex.key &gt; /dev/null</pre></div></li><li class="listitem "><p>Get the original SAN IP address(es) and DNS(s), run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -noout -text -in &lt;CLUSTER_NAME&gt;/pki.bak/oidc-dex.crt | grep -oP '(?&lt;=IP Address:)[^,]+'
openssl x509 -noout -text -in &lt;CLUSTER_NAME&gt;/pki.bak/oidc-dex.crt | grep -oP '(?&lt;=DNS:)[^,]+'</pre></div></li><li class="listitem "><p>Sign the <code class="literal">oidc-dex</code> server certificate with the default kubernetes CA certificate <span class="emphasis"><em>or</em></span> trusted CA certificate.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Default kubernetes CA certificate</p><p>Please refer to <a class="xref" href="#self-signed-server-certificate" title="6.9.9.2.2. Self-signed Server Certificate">Section 6.9.9.2.2, “Self-signed Server Certificate”</a> on how to sign the self signed server certificate. The default kubernetes CA certificate and key are located at <code class="literal">/etc/kubernetes/pki/ca.crt</code> and <code class="literal">/etc/kubernetes/pki/ca.key</code>. The <code class="literal">server.conf</code> for IP.1 is the original SAN IP address if present, DNS.1 is the original SAN DNS if present.</p></li><li class="listitem "><p>Trusted CA certificate</p><p>Please refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> on how to sign the trusted server certificate. The <code class="literal">server.conf</code> for IP.1 is the original SAN IP address if present, DNS.1 is the original SAN DNS if present.</p></li></ol></div></li><li class="listitem "><p>Import your certificate into the Kubernetes cluster.
The CA certificate is <code class="literal">&lt;CA_CERT_PATH&gt;</code>, server certificate and key are <code class="literal">&lt;SIGNED_OIDC_DEX_SERVER_CERT_PATH&gt;</code> and <code class="literal">&lt;SIGNED_OIDC_DEX_SERVER_KEY_PATH&gt;</code>.</p></li><li class="listitem "><p>Create a secret manifest file <code class="literal">oidc-dex-cert.yaml</code> and update the secret data <code class="literal">ca.crt</code>, <code class="literal">tls.crt</code>, and <code class="literal">tls.key</code> with base64; encoded with CA certificate, signed <code class="literal">oidc-dex</code> server certificate and key respectively.</p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: oidc-dex-cert
  namespace: kube-system
  labels:
    caasp.suse.com/skuba-addon: "true"
type: kubernetes.io/tls
data:
  ca.crt: cat &lt;CA_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.crt: cat &lt;SIGNED_OIDC_DEX_SERVER_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.key: cat &lt;SIGNED_OIDC_DEX_SERVER_KEY_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</pre></div></li><li class="listitem "><p>Apply the secret manifest file and restart <code class="literal">oidc-dex</code> pods.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl replace -f oidc-dex-cert.yaml
kubectl rollout restart deployment/oidc-dex -n kube-system</pre></div></li></ol></div></li><li class="listitem "><p>Replace the <code class="literal">oidc-gangway</code> server certificate:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the original <code class="literal">oidc-gangway</code> server certificate and key from secret resource.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki.bak
kubectl get secret oidc-gangway-cert -n kube-system -o "jsonpath={.data['tls\.crt']}" | base64 --decode | tee &lt;CLUSTER_NAME&gt;/pki.bak/oidc-gangway.crt &gt; /dev/null
kubectl get secret oidc-gangway-cert -n kube-system -o "jsonpath={.data['tls\.key']}" | base64 --decode | tee &lt;CLUSTER_NAME&gt;/pki.bak/oidc-gangway.key &gt; /dev/null</pre></div></li><li class="listitem "><p>Get the original SAN IP address(es) and DNS(s), run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -noout -text -in &lt;CLUSTER_NAME&gt;/pki.bak/oidc-gangway.crt | grep -oP '(?&lt;=IP Address:)[^,]+'
openssl x509 -noout -text -in &lt;CLUSTER_NAME&gt;/pki.bak/oidc-gangway.crt | grep -oP '(?&lt;=DNS:)[^,]+'</pre></div></li><li class="listitem "><p>Sign the <code class="literal">oidc-gangway</code> server certificate with the default kubernetes CA certificate <span class="emphasis"><em>or</em></span> trusted CA certificate.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Default kubernetes CA certificate</p><p>Please refer to <a class="xref" href="#self-signed-server-certificate" title="6.9.9.2.2. Self-signed Server Certificate">Section 6.9.9.2.2, “Self-signed Server Certificate”</a> on how to sign the self signed server certificate. The default kubernetes CA certificate and key are located at <code class="literal">/etc/kubernetes/pki/ca.crt</code> and <code class="literal">/etc/kubernetes/pki/ca.key</code>. The <code class="literal">server.conf</code> for IP.1 is the original SAN IP address if present, DNS.1 is the original SAN DNS if present.</p></li><li class="listitem "><p>Trusted CA certificate</p><p>Please refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> on how to sign the trusted server certificate. The <code class="literal">server.conf</code> for IP.1 is the original SAN IP address if present, DNS.1 is the original SAN DNS if present.</p></li></ol></div></li><li class="listitem "><p>Import your certificate into the Kubernetes cluster.
The CA certificates is <code class="literal">&lt;CA_CERT_PATH&gt;</code>, server certificate and key are <code class="literal">&lt;SIGNED_OIDC_GANGWAY_SERVER_CERT_PATH&gt;</code> and <code class="literal">&lt;SIGNED_OIDC_GANGWAY_SERVER_KEY_PATH&gt;</code>.</p></li><li class="listitem "><p>Create a secret manifest file <code class="literal">oidc-gangway-cert.yaml</code> and update the secret data <code class="literal">ca.crt</code>, <code class="literal">tls.crt</code>, and <code class="literal">tls.key</code> with base64; encoded with CA certificate, signed <code class="literal">oidc-gangway</code> server certificate and key respectively.</p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: oidc-gangway-cert
  namespace: kube-system
  labels:
    caasp.suse.com/skuba-addon: "true"
type: kubernetes.io/tls
data:
  ca.crt: cat &lt;CA_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.crt: cat &lt;SIGNED_OIDC_GANGWAY_SERVER_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.key: cat &lt;SIGNED_OIDC_GANGWAY_SERVER_KEY_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</pre></div></li><li class="listitem "><p>Apply the secret manifest file and restart <code class="literal">oidc-gangway</code> pods.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl replace -f oidc-gangway-cert.yaml
kubectl rollout restart deployment/oidc-gangway -n kube-system</pre></div></li></ol></div></li><li class="listitem "><p>Replace the <code class="literal">metrics-server</code> server certificate:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the original <code class="literal">metrics-server</code> server certificate and key from secret resource.</p><div class="verbatim-wrap highlight bash"><pre class="screen">mkdir -p &lt;CLUSTER_NAME&gt;/pki.bak
kubectl get secret metrics-server-cert -n kube-system -o "jsonpath={.data['tls\.crt']}" | base64 --decode | tee &lt;CLUSTER_NAME&gt;/pki.bak/metrics-server.crt &gt; /dev/null
kubectl get secret metrics-server-cert -n kube-system -o "jsonpath={.data['tls\.key']}" | base64 --decode | tee &lt;CLUSTER_NAME&gt;/pki.bak/metrics-server.key &gt; /dev/null</pre></div></li><li class="listitem "><p>Get the O/OU/CN, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -noout -subject -in &lt;CLUSTER_NAME&gt;/pki.bak/metrics-server.crt</pre></div></li><li class="listitem "><p>Get the original SAN IP address(es) and DNS(s), run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -noout -text -in &lt;CLUSTER_NAME&gt;/pki.bak/metrics-server.crt | grep -oP '(?&lt;=IP Address:)[^,]+'
openssl x509 -noout -text -in &lt;CLUSTER_NAME&gt;/pki.bak/metrics-server.crt | grep -oP '(?&lt;=DNS:)[^,]+'</pre></div></li><li class="listitem "><p>Sign the <code class="literal">metrics-server-cert</code> server certificate with the default Kubernetes CA certificate</p><p>Please refer to <a class="xref" href="#self-signed-server-certificate" title="6.9.9.2.2. Self-signed Server Certificate">Section 6.9.9.2.2, “Self-signed Server Certificate”</a> on how to sign the self signed server certificate. The default Kubernetes CA certificate and key are located at <code class="literal">/etc/kubernetes/pki/ca.crt</code> and <code class="literal">/etc/kubernetes/pki/ca.key</code>. The <code class="literal">server.conf</code> for O/OU/CN <span class="emphasis"><em>must be</em></span> the same as original one, <code class="literal">IP.1</code> is the original SAN IP address if present, <code class="literal">DNS.1</code> is the original SAN DNS if present.</p></li><li class="listitem "><p>Import your certificate into the Kubernetes cluster.
The CA certificates is <code class="literal">&lt;CA_CERT_PATH&gt;</code>, server certificate and key are <code class="literal">&lt;SIGNED_METRICS_SERVER_CERT_PATH&gt;</code> and <code class="literal">&lt;SIGNED_METRICS_SERVER_KEY_PATH&gt;</code>.</p></li><li class="listitem "><p>Create a secret manifest file <code class="literal">oidc-metrics-server-cert.yaml</code> and update the secret data <code class="literal">ca.crt</code>, <code class="literal">tls.crt</code>, and <code class="literal">tls.key</code> with base64; encoded with CA certificate, signed <code class="literal">metrics-server</code> server certificate and key respectively.</p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: metrics-server-cert
  namespace: kube-system
  labels:
    caasp.suse.com/skuba-addon: "true"
type: kubernetes.io/tls
data:
  ca.crt: cat &lt;CA_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.crt: cat &lt;SIGNED_METRICS_SERVER_CERT_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo
  tls.key: cat &lt;SIGNED_METRICS_SERVER_KEY_PATH&gt; | base64 | awk '{print}' ORS='' &amp;&amp; echo</pre></div></li><li class="listitem "><p>Apply the secret manifest file and restart <code class="literal">metrics-server</code> pods.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl replace -f metrics-server-cert.yaml
kubectl rollout restart deployment/metrics-server -n kube-system</pre></div></li></ol></div></li></ul></div></div></div><div class="sect2" id="_how_to_generate_certificates"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How To Generate Certificates</span> <a title="Permalink" class="permalink" href="#_how_to_generate_certificates">#</a></h3></div></div></div><div class="sect3" id="trusted-signed-certificate"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trusted 3rd-Party Signed Certificate</span> <a title="Permalink" class="permalink" href="#trusted-signed-certificate">#</a></h4></div></div></div><div class="sect4" id="trusted-server-certificate"><div class="titlepage"><div><div><h5 class="title"><span class="number">6.9.9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trusted Server Certificate</span> <a title="Permalink" class="permalink" href="#trusted-server-certificate">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Generate a private key by following the steps below from a terminal window:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl genrsa -aes256 -out server.key 2048</pre></div><p>Type the pass phrase to protect the key and press [Enter]</p><p>Re-enter the pass phrase.</p></li><li class="listitem "><p>Create a file <span class="emphasis"><em>server.conf</em></span> with the appropriate values</p><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[req_distinguished_name]
C = CZ <span id="CO17-1"></span><span class="callout">1</span>
ST = CZ <span id="CO17-2"></span><span class="callout">2</span>
L = Prague <span id="CO17-3"></span><span class="callout">3</span>
O = example <span id="CO17-4"></span><span class="callout">4</span>
OU = com <span id="CO17-5"></span><span class="callout">5</span>
CN = server.example.com <span id="CO17-6"></span><span class="callout">6</span>
emailAddress = admin@example.com <span id="CO17-7"></span><span class="callout">7</span>

[v3_req]
basicConstraints = critical,CA:FALSE
keyUsage = critical,digitalSignature,keyEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names

[alt_names]
IP.1 = &lt;SERVER-IP-ADDRESS&gt; <span id="CO17-8"></span><span class="callout">8</span>
DNS.1 = &lt;SERVER-FQDN&gt; <span id="CO17-9"></span><span class="callout">9</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Country Name (2 letter code).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>State or Province Name (full name).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Locality Name (eg, city).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Organization Name (eg, company).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Organizational Unit Name (eg, section).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Common Name (e.g. server FQDN or YOUR name)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Email Address</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Server IP address if present. Add more IP.X below if the server has more than one IP address.
Remove IP.1 if the server uses FQDN.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO17-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>Server FQDN if present. Add more DNS.X below if the server has more than one domain name.
Remove DNS.1 if the server uses an IP address.</p></td></tr></table></div></li><li class="listitem "><p>Generate a certificate signing request (CSR)</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl req -new -key server.key -config server.conf -out server.csr</pre></div><p>Enter the pass phrase of the private key created in Step 1.</p><p>Check the certificate signing request (CSR)</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl req -text -noout -verify -in server.csr</pre></div></li><li class="listitem "><p>Sign the certificate</p><p>Send the certificate signing request (CSR) to the 3rd party for signing.
You should receive the following files in return:</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Server certificate (public key)</p></li><li class="listitem "><p>Intermediate CA and/or bundles that chain to the Trusted Root CA</p></li></ol></div></li></ol></div></div><div class="sect4" id="trusted-client-certificate"><div class="titlepage"><div><div><h5 class="title"><span class="number">6.9.9.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trusted Client Certificate</span> <a title="Permalink" class="permalink" href="#trusted-client-certificate">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Generate a private key by following the steps below from a terminal window:</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl genrsa -aes256 -out client.key 2048</pre></div><p>Type the pass phrase to protect the key and press [Enter]</p><p>Re-enter the pass phrase.</p></li><li class="listitem "><p>Create a file <span class="emphasis"><em>client.conf</em></span> with the appropriate values</p><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[req_distinguished_name]
C = CZ <span id="CO18-1"></span><span class="callout">1</span>
ST = CZ <span id="CO18-2"></span><span class="callout">2</span>
L = Prague <span id="CO18-3"></span><span class="callout">3</span>
O = example <span id="CO18-4"></span><span class="callout">4</span>
OU = com <span id="CO18-5"></span><span class="callout">5</span>
CN = client.example.com <span id="CO18-6"></span><span class="callout">6</span>
emailAddress = admin@example.com <span id="CO18-7"></span><span class="callout">7</span>

[v3_req]
basicConstraints = critical,CA:FALSE
keyUsage = critical,digitalSignature,keyEncipherment
extendedKeyUsage = clientAuth</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO18-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Country Name (2 letter code).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO18-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>State or Province Name (full name).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO18-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Locality Name (eg, city).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO18-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Organization Name (eg, company).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO18-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Organizational Unit Name (eg, section).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO18-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Common Name (e.g. client FQDN or YOUR name)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO18-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Email Address</p></td></tr></table></div></li><li class="listitem "><p>Generate a certificate signing request (CSR)</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl req -new -key client.key -config client.conf -out client.csr</pre></div><p>Enter the pass phrase of the private key created in Step 1.</p><p>Check the certificate signing request (CSR)</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl req -text -noout -verify -in client.csr</pre></div></li><li class="listitem "><p>Sign the certificate</p><p>Send the certificate signing request (CSR) to the 3rd party for signing.
You should receive the following files in return:</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Client certificate (public key)</p></li><li class="listitem "><p>Intermediate CA and/or bundles that chain to the Trusted Root CA</p></li></ol></div></li></ol></div></div></div><div class="sect3" id="self-signed-certificate"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.9.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Self-signed Server Certificate</span> <a title="Permalink" class="permalink" href="#self-signed-certificate">#</a></h4></div></div></div><div id="id-1.8.10.11.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>In the case that you decide to use self-signed certificates, make sure that the Certificate Authority
used for signing is configured securely as a trusted Certificate Authority on the clients.</p></div><p>In some cases you want to create self-signed certificates for testing.
If you are using proper trusted 3rd-party CA signed certificates, skip the following steps and refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a>.</p><div class="sect4" id="self-signed-ca-certificate"><div class="titlepage"><div><div><h5 class="title"><span class="number">6.9.9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Self-signed CA Certificate</span> <a title="Permalink" class="permalink" href="#self-signed-ca-certificate">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a file <span class="emphasis"><em>ca.conf</em></span> with the appropriate values</p><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
x509_extensions = v3_ca
prompt = no

[req_distinguished_name]
C = CZ <span id="CO19-1"></span><span class="callout">1</span>
ST = CZ <span id="CO19-2"></span><span class="callout">2</span>
L = Prague <span id="CO19-3"></span><span class="callout">3</span>
O = example <span id="CO19-4"></span><span class="callout">4</span>
OU = com <span id="CO19-5"></span><span class="callout">5</span>
CN = Root CA <span id="CO19-6"></span><span class="callout">6</span>
emailAddress = admin@example.com <span id="CO19-7"></span><span class="callout">7</span>

[v3_ca]
basicConstraints = critical,CA:TRUE
keyUsage = critical,digitalSignature,keyEncipherment,keyCertSign</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO19-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Country Name (2 letter code).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO19-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>State or Province Name (full name).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO19-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Locality Name (eg, city).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO19-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Organization Name (eg, company).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO19-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Organizational Unit Name (eg, section).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO19-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Common Name (e.g. server FQDN or YOUR name)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO19-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Email Address</p></td></tr></table></div></li><li class="listitem "><p>Sign the CA certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl genrsa -out ca.key 2048
openssl req -key ca.key -new -x509 -days 3650 -sha256 -config ca.conf -out ca.crt</pre></div></li></ol></div></div><div class="sect4" id="self-signed-server-certificate"><div class="titlepage"><div><div><h5 class="title"><span class="number">6.9.9.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Self-signed Server Certificate</span> <a title="Permalink" class="permalink" href="#self-signed-server-certificate">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a file <span class="emphasis"><em>server.conf</em></span> with the appropriate values</p><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[req_distinguished_name]
C = CZ <span id="CO20-1"></span><span class="callout">1</span>
ST = CZ <span id="CO20-2"></span><span class="callout">2</span>
L = Prague <span id="CO20-3"></span><span class="callout">3</span>
O = example <span id="CO20-4"></span><span class="callout">4</span>
OU = com <span id="CO20-5"></span><span class="callout">5</span>
CN = example.com <span id="CO20-6"></span><span class="callout">6</span>
emailAddress = admin@example.com <span id="CO20-7"></span><span class="callout">7</span>

[v3_req]
basicConstraints = critical,CA:FALSE
keyUsage = critical,digitalSignature,keyEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names

[alt_names]
IP.1 = &lt;SERVER-IP-ADDRESS&gt; <span id="CO20-8"></span><span class="callout">8</span>
DNS.1 = &lt;SERVER-FQDN&gt; <span id="CO20-9"></span><span class="callout">9</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Country Name (2 letter code).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>State or Province Name (full name).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Locality Name (eg, city).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Organization Name (eg, company).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Organizational Unit Name (eg, section).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Common Name (e.g. server FQDN or YOUR name)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Email Address</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>Server IP address if present. Add more IP.X below if the server has more than one IP address.
Remove IP.1 if the server uses FQDN.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO20-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>Server FQDN if present. Add more DNS.X below if the server has more than one domain name.
Remove DNS.1 if the server uses an IP address.</p></td></tr></table></div></li><li class="listitem "><p>Generate the certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl genrsa -out server.key 2048
openssl req -key server.key -new -sha256 -out server.csr -config server.conf
openssl x509 -req -CA ca.crt -CAkey ca.key -CAcreateserial -in server.csr -out server.crt -days 365 -extensions v3_req -extfile server.conf</pre></div><p>Check the signed certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -text -noout -in server.crt</pre></div></li></ol></div></div><div class="sect4" id="self-signed-client-certificate"><div class="titlepage"><div><div><h5 class="title"><span class="number">6.9.9.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Self-signed Client Certificate</span> <a title="Permalink" class="permalink" href="#self-signed-client-certificate">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a file <span class="emphasis"><em>client.conf</em></span> with the appropriate values</p><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[req_distinguished_name]
C = CZ <span id="CO21-1"></span><span class="callout">1</span>
ST = CZ <span id="CO21-2"></span><span class="callout">2</span>
L = Prague <span id="CO21-3"></span><span class="callout">3</span>
O = example <span id="CO21-4"></span><span class="callout">4</span>
OU = com <span id="CO21-5"></span><span class="callout">5</span>
CN = client.example.com <span id="CO21-6"></span><span class="callout">6</span>
emailAddress = admin@example.com <span id="CO21-7"></span><span class="callout">7</span>

[v3_req]
basicConstraints = critical,CA:FALSE
keyUsage = critical,digitalSignature,keyEncipherment
extendedKeyUsage = clientAuth</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO21-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Country Name (2 letter code).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO21-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>State or Province Name (full name).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO21-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Locality Name (eg, city).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO21-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Organization Name (eg, company).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO21-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Organizational Unit Name (eg, section).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO21-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Common Name (e.g. server FQDN or YOUR name)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO21-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>Email Address</p></td></tr></table></div></li><li class="listitem "><p>Generate the certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl genrsa -out client.key 2048
openssl req -key client.key -new -sha256 -out client.csr -config client.conf
openssl x509 -req -CA ca.crt -CAkey ca.key -CAcreateserial -in client.csr -out client.crt -days 365 -extensions v3_req -extfile client.conf</pre></div><p>Check the signed certificate</p><div class="verbatim-wrap highlight bash"><pre class="screen">openssl x509 -text -noout -in client.crt</pre></div></li></ol></div></div></div></div></div></div><div class="chapter " id="_logging"><div class="titlepage"><div><div><h1 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging</span> <a title="Permalink" class="permalink" href="#_logging">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_introduction_2"><span class="number">7.1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="section"><a href="#tee-logging"><span class="number">7.2 </span><span class="name">Logging in skuba</span></a></span></dt><dt><span class="section"><a href="#_audit_log"><span class="number">7.3 </span><span class="name">Audit Log</span></a></span></dt><dt><span class="section"><a href="#centralized-logging"><span class="number">7.4 </span><span class="name">Centralized Logging</span></a></span></dt></dl></div></div><div class="sect1" id="_introduction_2"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction_2">#</a></h2></div></div></div><p>Logging is ubiquitous throughout SUSE CaaS Platform. Some tools will only print their
outputs to the currently running session shell and not create a "log file".</p><p>If you need to retain the output of these files you can <code class="literal">tee</code> them into a separate file (refer to <a class="xref" href="#tee-logging" title="7.2. Logging in skuba">Section 7.2, “Logging in skuba”</a>).</p><p>Many other service components will produce log files or other log info streams.
You can collect, store and evaluate these logs via <a class="xref" href="#centralized-logging" title="7.4. Centralized Logging">Section 7.4, “Centralized Logging”</a> for
use with the <a class="xref" href="#monitoring-stack" title="8.1. Monitoring Stack">Section 8.1, “Monitoring Stack”</a>.</p><div id="id-1.9.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you are looking for troubleshooting logs please refer to <a class="xref" href="#troubleshooting-logs" title="15.3. Log collection">Section 15.3, “Log collection”</a>.</p></div></div><div class="sect1" id="tee-logging"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging in skuba</span> <a title="Permalink" class="permalink" href="#tee-logging">#</a></h2></div></div></div><p>One important part of deploying and maintaining a product is to have reliable
logs. Tools like <code class="literal">skuba</code> take the approach of printing the output to the
standard output directly. This is not just common practice, but it also has the
advantage that then the user has more flexibility on how to manage said output.</p><p>Thus, whenever throughout this guide we write a <code class="literal">skuba</code> command, take into
account that the output will be printed into the standard output. If you would
also like to have the logs stored somewhere else for later inspection, you can
use tools like <code class="literal">tee</code>. For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node bootstrap --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt; | tee &lt;NODE_NAME&gt;-skuba-node-bootstrap.log</pre></div><p>Otherwise, you might want to use other tools to manage the logs for later
inspection. The point being that this guide will never consider how to manage
these logs because <code class="literal">skuba</code> itself does not. It’s up to you to manage these logs
in any way you find desirable.</p><p>Moreover, <code class="literal">skuba</code> has also various levels of log verbosity. This is managed by
the <code class="literal">-v, --verbosity</code> flag. This flag accepts an integer argument, ranging from
0 to 5, where a higher number means a higher level of verbosity. If you don’t
pass any arguments, then 0 is assumed. We recommend using the default argument,
since it will already log warnings and errors, among other relevant output,
whereas 5 can be a bit overwhelming. Thus, for the above example, we would
recommend something like:</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node bootstrap -v --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt; | tee &lt;NODE_NAME&gt;-skuba-node-bootstrap.log</pre></div><p>Now the <code class="literal">&lt;NODE_NAME&gt;-skuba-node-bootstrap.log</code> will have more useful information
than without the <code class="literal">-v</code> flag. We <span class="strong"><strong>strongly</strong></span> recommend using this flag in order to
get as much useful information as possible from a single run.</p></div><div class="sect1" id="_audit_log"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Log</span> <a title="Permalink" class="permalink" href="#_audit_log">#</a></h2></div></div></div><p>To track actions that have been performed on the cluster, you can enable the Kubernetes audit log during cluster bootstrap or on a running cluster.</p><p>This allows the audit logs to be written on the Kubernetes master nodes at <code class="literal">/var/log/kube-apiserver/audit.log</code> or the given path.</p><p>For more information on the audit log and its contents, see: <a class="link" href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/" target="_blank">https://kubernetes.io/docs/tasks/debug-application-cluster/audit/</a></p><div class="sect2" id="_limitations"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#_limitations">#</a></h3></div></div></div><p>The Kubernetes audit log only collects and stores actions performed on the level of the cluster. This does not include any resulting actions of application services.</p></div><div class="sect2" id="_enable_auditing_during_cluster_bootstrap"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Auditing During Cluster Bootstrap</span> <a title="Permalink" class="permalink" href="#_enable_auditing_during_cluster_bootstrap">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create audit policy file - <code class="literal">audit.yaml</code>. Here uses a simple policy for demonstration.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
  - level: Metadata <span id="CO22-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO22-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The audit level of the event. This sample will log all requests at the Metadata level.
For detailed information, refer to: <a class="link" href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy" target="_blank">https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy</a></p></td></tr></table></div></li><li class="listitem "><p>Create audit policy file directory on all master nodes.</p><div class="verbatim-wrap"><pre class="screen">sudo mkdir -p /etc/kubernetes/policies</pre></div></li><li class="listitem "><p>Copy audit policy file - <code class="literal">audit.yaml</code> to <code class="literal">/etc/kubernetes/policies/audit.yaml</code> on all master nodes.</p></li><li class="listitem "><p>Edit <code class="literal">kubeadm-init.conf</code> file in skuba init folder to add audit related configurations.</p><div class="verbatim-wrap"><pre class="screen">vi &lt;my_cluster&gt;/kubeadm-init.conf</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen"> ...
apiServer:
  extraArgs:
    audit-log-path: /var/log/kube-apiserver/audit.log
    audit-policy-file: /etc/kubernetes/policies/audit.yaml <span id="CO23-1"></span><span class="callout">1</span>
    audit-log-maxage: "30" <span id="CO23-2"></span><span class="callout">2</span>
    audit-log-maxsize: "100" <span id="CO23-3"></span><span class="callout">3</span>
    audit-log-maxbackup: "5" <span id="CO23-4"></span><span class="callout">4</span>
    audit-log-format: json <span id="CO23-5"></span><span class="callout">5</span>
  extraVolumes:
  - name: audit-policy
    hostPath: /etc/kubernetes/policies/audit.yaml <span id="CO23-6"></span><span class="callout">6</span>
    mountPath: /etc/kubernetes/policies/audit.yaml <span id="CO23-7"></span><span class="callout">7</span>
    readOnly: true
    pathType: File
  - name: audit-logs
    hostPath: /var/log/kube-apiserver <span id="CO23-8"></span><span class="callout">8</span>
    mountPath: /var/log/kube-apiserver <span id="CO23-9"></span><span class="callout">9</span>
    pathType: DirectoryOrCreate
 ...</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Path to the YAML file that defines the audit policy configuration.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The maximum number of days to retain old audit log files based on the timestamp encoded in their filename. (Default: 15)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The maximum size in megabytes of the audit log file before it gets rotated. (Default: 10)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The maximum number of old audit log files to retain. (Default: 20)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Format of saved audits. Known formats are "legacy", "json". "legacy" indicates 1-line text format for each event. "json" indicates structured json format.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The audit policy configuration file path from the host node’s filesystem.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>The audit policy configuration file path on the api-server pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>The audit log file directory from the host node’s filesystem.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO23-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>The audit log file directory on the api-server pod.</p></td></tr></table></div></li><li class="listitem "><p>Proceed with <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-deployment/bootstrap.html" target="_blank">Cluster Bootstrap</a>.</p></li><li class="listitem "><p>If everything is setup correctly, you should be able to see audit logs are written to <code class="literal">/var/log/kube-apiserver/audit.log</code>.</p></li></ol></div></div><div class="sect2" id="_enable_auditing_on_running_cluster"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Auditing On Running Cluster</span> <a title="Permalink" class="permalink" href="#_enable_auditing_on_running_cluster">#</a></h3></div></div></div><div id="id-1.9.4.7.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The following steps take effect only on the updated master nodes. You need to repeat the following steps on every master node in the cluster.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create audit policy file - <code class="literal">audit.yaml</code>. Here uses a simple policy for demonstration.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
  - level: Metadata <span id="CO24-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO24-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The audit level of the event. This sample will log all requests at the Metadata level. For detailed information, refer to: <a class="link" href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy" target="_blank">https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy</a></p></td></tr></table></div></li><li class="listitem "><p>Create audit policy file directory on master node.</p><div class="verbatim-wrap"><pre class="screen">sudo mkdir -p /etc/kubernetes/policies</pre></div></li><li class="listitem "><p>Copy audit policy file - <code class="literal">audit.yaml</code> to <code class="literal">/etc/kubernetes/policies/audit.yaml</code> on master node.</p></li><li class="listitem "><p>Edit <code class="literal">/etc/kubernetes/manifests/kube-apiserver.yaml</code>.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen"> ...
spec:
  containers:
  - command:
    - kube-apiserver
    - --audit-log-path=/var/log/kube-apiserver/audit.log
    - --audit-policy-file=/etc/kubernetes/policies/audit.yaml <span id="CO25-1"></span><span class="callout">1</span>
    - --audit-log-maxage=30 <span id="CO25-2"></span><span class="callout">2</span>
    - --audit-log-maxsize=100 <span id="CO25-3"></span><span class="callout">3</span>
    - --audit-log-maxbackup=5 <span id="CO25-4"></span><span class="callout">4</span>
    - --audit-log-format=json <span id="CO25-5"></span><span class="callout">5</span>
 ...
    volumeMounts:
    - mountPath: /etc/kubernetes/policies/audit.yaml <span id="CO25-6"></span><span class="callout">6</span>
      name: audit-policy
      readOnly: true
    - mountPath: /var/log/kube-apiserver <span id="CO25-7"></span><span class="callout">7</span>
      name: audit-logs
 ...
  volumes:
  - hostPath:
      path: /etc/kubernetes/policies/audit.yaml <span id="CO25-8"></span><span class="callout">8</span>
      type: File
    name: audit-policy
  - hostPath:
      path: /var/log/kube-apiserver <span id="CO25-9"></span><span class="callout">9</span>
      type: DirectoryOrCreate
    name: audit-logs
 ...</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Path to the YAML file that defines the audit policy configuration.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The maximum number of days to retain old audit log files based on the timestamp encoded in their filename. (Default: 15)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The maximum size in megabytes of the audit log file before it gets rotated. (Default: 10)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The maximum number of old audit log files to retain. (Default: 20)</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Format of saved audits. Known formats are "legacy", "json". "legacy" indicates 1-line text format for each event. "json" indicates structured json format.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The audit policy configuration file path on the api-server pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>The audit log file directory on the api-server pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-8"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>The audit policy configuration file path from the host node’s filesystem.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO25-9"><span class="callout">9</span></a> </p></td><td valign="top" align="left"><p>The audit log file directory from the host node’s filesystem.</p></td></tr></table></div></li><li class="listitem "><p>Restart kubelet.</p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart kubelet</pre></div></li><li class="listitem "><p>If everything is set up correctly, you should be able to see audit logs being written to <code class="literal">/var/log/kube-apiserver/audit.log</code>.</p></li></ol></div></div><div class="sect2" id="_disable_auditing"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable Auditing</span> <a title="Permalink" class="permalink" href="#_disable_auditing">#</a></h3></div></div></div><div id="id-1.9.4.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The following steps take effect only on the updated master nodes. You need to repeat the following steps on every master node in the cluster.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Remote access to the master node.</p></li></ol></div><div class="verbatim-wrap"><pre class="screen">ssh sles@&lt;master_node&gt;</pre></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit <code class="literal">/etc/kubernetes/manifests/kube-apiserver.yaml</code> and remove the following lines.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">...
   - --audit-log-path=/var/log/kube-apiserver/audit.log
   - --audit-policy-file=/etc/kubernetes/policies/audit.yaml
   - --audit-log-maxage=30
   - --audit-log-maxsize=100
   - --audit-log-maxbackup=5
   - --audit-log-format=json
...
   - mountPath: /etc/kubernetes/policies/audit.yaml
     name: audit-policy
     readOnly: true
   - mountPath: /var/log/kube-apiserver
     name: audit-logs
...
 - hostPath:
     path: /etc/kubernetes/policies/audit.yaml
     type: File
   name: audit-policy
 - hostPath:
     path: /var/log/kube-apiserver
     type: DirectoryOrCreate
   name: audit-logs</pre></div></div></li><li class="listitem "><p>Restart kubelet.</p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart kubelet</pre></div></li></ol></div></div></div><div class="sect1" id="centralized-logging"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Centralized Logging</span> <a title="Permalink" class="permalink" href="#centralized-logging">#</a></h2></div></div></div><p>Centralized Logging is a means of collecting logs from the SUSE CaaS Platform for centralized management.
It forwards system and Kubernetes cluster logs to a specified external logging service,
for example, Rsyslog server.</p><p>Collecting logs in a central location can be useful for audit or debug purposes or to analyze and visually present data.</p><div class="sect2" id="_prerequisites_3"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites_3">#</a></h3></div></div></div><p>In order to successfully use Centralized Logging, you first need to install <code class="literal">Helm</code>, and <code class="literal">Tiller</code> if using Helm 2.
Helm is used to install the log agents and provide custom logging settings.</p><p>Refer to <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>.</p></div><div class="sect2" id="_types_of_logs"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Types of Logs</span> <a title="Permalink" class="permalink" href="#_types_of_logs">#</a></h3></div></div></div><p>You can log the following groups of services. See <a class="xref" href="#_deployment" title="7.4.4. Deployment">Section 7.4.4, “Deployment”</a>
for more information on how to select and customize the logs.</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.9.5.5.3.1"><span class="term ">Kubernetes System Components</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Kubelet</p></li><li class="listitem "><p>Cri-o</p></li></ul></div></dd><dt id="id-1.9.5.5.3.2"><span class="term ">Kubernetes Control Plane Components</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>API Server</p></li><li class="listitem "><p>Controller Manager</p></li><li class="listitem "><p>Scheduler</p></li><li class="listitem "><p>Cilium</p></li><li class="listitem "><p>Kube-proxy</p></li><li class="listitem "><p>All resources in the kube-system namespaces</p></li></ul></div></dd><dt id="id-1.9.5.5.3.3"><span class="term ">Kubernetes Namespaces Pods</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>All namespaces in cluster except kube-system</p></li></ul></div></dd><dt id="id-1.9.5.5.3.4"><span class="term ">Kubernetes Audit Log</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-admin/_logging.html#_audit_log" target="_blank">Audit Log</a></p></li></ul></div></dd><dt id="id-1.9.5.5.3.5"><span class="term ">OS Components</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Kernel</p></li><li class="listitem "><p>Audit</p></li><li class="listitem "><p>Zypper</p></li><li class="listitem "><p>Network (wicked)</p></li></ul></div></dd></dl></div><p>Centralized Logging is also restricted to the following protocols: UDP, TCP, TCP + TLS, TCP + mTLS.</p></div><div class="sect2" id="_log_formats"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Log Formats</span> <a title="Permalink" class="permalink" href="#_log_formats">#</a></h3></div></div></div><p>The two supported syslog message formats are <span class="strong"><strong>RFC 5424</strong></span> and <span class="strong"><strong>RFC 3164</strong></span>.</p><div id="id-1.9.5.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The Kubernetes cluster metadata is included in the RFC 5424 message.</p></div><p>Example RFC 3164</p><div class="verbatim-wrap"><pre class="screen">2019-05-30T09:11:21.968458+00:00 worker1 k8s.system/crio[12080] level=debug msg="Endpoint successfully created" containerID=caa46f14a68e766b877af01442e58731845bb45d8ce1f856553440a02c958b2f eventUUID=e2405f2a-82ba-11e9-9a06-fa163eebdfd6 subsys=cilium-cni</pre></div><p>Example RFC 5424</p><div class="verbatim-wrap"><pre class="screen">&lt;133&gt;1 2019-05-30T08:28:38.784214+00:00 master0 k8s.pod/kube-system/kube-apiserver-master0/kube-apiserver - - [kube_meta namespace_id="1e030def-81db-11e9-a62b-fa163e1876c9" container_name="kube-apiserver" creation_timestamp="2019-05-29T06:29:31Z" host="master0" namespace_name="kube-system" master_url="https://kubernetes.default.svc.cluster.local:443" pod_id="4aaf10f9-81db-11e9-a62b-fa163e1876c9" pod_name="kube-apiserver-master0"] 2019-05-30T08:28:38.783780355+00:00 stderr F I0530 08:28:38.783710       1 log.go:172] http: TLS handshake error from 172.28.0.19:45888: tls: client offered only unsupported versions: [300]</pre></div></div><div class="sect2" id="_deployment"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment</span> <a title="Permalink" class="permalink" href="#_deployment">#</a></h3></div></div></div><p>After you have successfully installed it,
use Helm CLI to install log agents on each node,
and provide customized settings via specific command options.</p><p>The only three mandatory parameters for a successful deployment of Centralized Logging
are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal"><span class="strong"><strong>server.host</strong></span></code>, default value = rsyslog-server.default.svc.cluster.local</p></li><li class="listitem "><p><code class="literal"><span class="strong"><strong>server.port</strong></span></code>, default value = 514</p></li><li class="listitem "><p><code class="literal"><span class="strong"><strong>server.protocol</strong></span></code>, default value = TCP</p></li></ul></div><p>See <a class="xref" href="#log-optional_settings" title="7.4.6. Optional settings">Section 7.4.6, “Optional settings”</a> for the optional parameters and their default values.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Running the following will create the minimal working setup:</p></li></ul></div><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com
helm install suse/log-agent-rsyslog --name &lt;RELEASE_NAME&gt; --namespace kube-system --set server.host=${SERVER_HOST} --set server.port=${SERVER_PORT}</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com
helm install &lt;RELEASE_NAME&gt; suse/log-agent-rsyslog --namespace kube-system --set server.host=${SERVER_HOST} --set server.port=${SERVER_PORT}</pre></div><div id="id-1.9.5.7.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If not specified otherwise, Helm will install log agents with TCP as the default value for <code class="literal">server.protocol</code>.</p></div><div id="id-1.9.5.7.11" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Currently, Rsyslog can cause a segfault (as described in <a class="link" href="https://github.com/rsyslog/rsyslog/issues/4200" target="_blank">https://github.com/rsyslog/rsyslog/issues/4200</a>) as result of a conflict when both <code class="literal">imjournal</code> and <code class="literal">imfile</code> input modules are enabled. To avoid crashing Rsyslog, <code class="literal">imfile</code> and <code class="literal">imjournal</code> need to be mutually exclusive, which means they need to be configured in the Rsyslog helm chart to one of these:</p><div class="verbatim-wrap highlight bash"><pre class="screen">--set logs.osSystem.enabled=true \ <span id="CO26-1"></span><span class="callout">1</span>
--set logs.kubernetesSystem.enabled=false \ <span id="CO26-2"></span><span class="callout">2</span>
--set logs.kubernetesControlPlane.enabled=false \ <span id="CO26-3"></span><span class="callout">3</span>
--set logs.kubernetesUserNamespaces.enabled=false \ <span id="CO26-4"></span><span class="callout">4</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO26-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Enables imfile</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO26-2"><span class="callout">2</span></a> <a href="#CO26-3"><span class="callout">3</span></a> <a href="#CO26-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Disables imjournal</p></td></tr></table></div><div class="verbatim-wrap highlight bash"><pre class="screen">--set logs.osSystem.enabled=false \ <span id="CO27-1"></span><span class="callout">1</span>
--set logs.kubernetesSystem.enabled=true \ <span id="CO27-2"></span><span class="callout">2</span>
--set logs.kubernetesControlPlane.enabled=true \ <span id="CO27-3"></span><span class="callout">3</span>
--set logs.kubernetesUserNamespaces.enabled=true \ <span id="CO27-4"></span><span class="callout">4</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO27-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Disables imfile</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO27-2"><span class="callout">2</span></a> <a href="#CO27-3"><span class="callout">3</span></a> <a href="#CO27-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Enables imjournal</p></td></tr></table></div><p>If it is required to send both <code class="literal">imjournal</code> and <code class="literal">imfile</code>, Rsyslog needs to be installed directly on the host nodes to send the <code class="literal">imjournal</code> log and again deployed via the helm chart on the Kubernetes cluster to send <code class="literal">imfile</code> logs.</p></div><p>After this step, all of the log agents will initialize then start to forward logs from each node to the configured remote Rsyslog server.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>To check the installation progress, use the <code class="literal">helm status</code> command:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">helm status &lt;RELEASE_NAME&gt; --namespace kube-system</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>To uninstall log agents, use the <code class="literal">helm delete</code> command:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">helm delete --purge &lt;RELEASE_NAME&gt; --namespace kube-system</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap"><pre class="screen">helm uninstall &lt;RELEASE_NAME&gt; --namespace kube-system</pre></div></div><div class="sect2" id="_queuing"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Queuing</span> <a title="Permalink" class="permalink" href="#_queuing">#</a></h3></div></div></div><p>Centralized Logging supports a configurable buffered queue.
This can be used to improve log processing throughput and eliminate possible data loss,
for instance after log agents shutdown, restart or in case of an unresponsive remote server.
The queue files are located under <code class="literal">/var/log/containers/{RELEASE_NAME}-log-agent-rsyslog</code> on every node in the cluster.
Queue files remain even after the log agents are deleted.</p><p>The buffered queue can be enabled/disabled with the following parameter:</p><p><code class="literal"><span class="strong"><strong>queue.enabled</strong></span></code>, default value = false</p><p>Setting <code class="literal">queue.enabled</code> to <code class="literal">false</code> means that data will be stored in-memory only.
Setting the parameter to <code class="literal">true</code> will set the data store to a mixture of in-memory and in-disk.
Data will then be stored in memory until the queue is filled up, after which storing is switched to disk.
Enabling the queue also automatically saves the queue to disk at service shutdown.</p><p>Additional parameters to define queue size and its disk usage are:</p><p><code class="literal"><span class="strong"><strong>queue.size</strong></span></code>, default value = 50000</p><p>This option sets the number of messages allowed for the in-memory queue.
This setting affects the Kubernetes cluster logs (<code class="literal">kubernetes-control-plane</code> and <code class="literal">kubernetes-USER_NAME-space</code>).</p><p><code class="literal"><span class="strong"><strong>queue.maxDiskSpace</strong></span></code>, default value = 2147483648</p><p>This option sets the maximum size allowed for disk storage (in bytes).
The storage is divided so that 20 percent of it is for journal logs and 80 percent for the remaining logs.</p></div><div class="sect2" id="log-optional_settings"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional settings</span> <a title="Permalink" class="permalink" href="#log-optional_settings">#</a></h3></div></div></div><div id="id-1.9.5.9.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Options with empty default values are set as not specified.</p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Function</th><th align="left" valign="top">Default value</th></tr></thead><tbody><tr><td align="left" valign="top"><p>image.repository</p></td><td align="left" valign="top"><p>specifies image repository to pull from</p></td><td align="left" valign="top"><p>registry.suse.com/caasp/v4.5/rsyslog</p></td></tr><tr><td align="left" valign="top"><p>image.tag</p></td><td align="left" valign="top"><p>specifies image tag to pull</p></td><td align="left" valign="top"><p>8.39.0</p></td></tr><tr><td align="left" valign="top"><p>kubernetesPodAnnotationsEnabled</p></td><td align="left" valign="top"><p>enables kubernetes meta annotations in pod logs</p></td><td align="left" valign="top"><p>false</p></td></tr><tr><td align="left" valign="top"><p>kubernetesPodLabelsEnabled</p></td><td align="left" valign="top"><p>enables kubernetes meta labels in pod logs</p></td><td align="left" valign="top"><p>false</p></td></tr><tr><td align="left" valign="top"><p>logs.kubernetesAudit.enabled</p></td><td align="left" valign="top"><p>enables Kubernetes audit logs</p></td><td align="left" valign="top"><p>true</p></td></tr><tr><td align="left" valign="top"><p>logs.kubernetesAudit.logDir</p></td><td align="left" valign="top"><p>Kubernetes audit log directory</p></td><td align="left" valign="top"><p>/var/log/kube-apiserver</p></td></tr><tr><td align="left" valign="top"><p>logs.kubernetesAudit.logFile</p></td><td align="left" valign="top"><p>Kubernetes audit log filename</p></td><td align="left" valign="top"><p>audit.log</p></td></tr><tr><td align="left" valign="top"><p>logs.kubernetesControlPlane.enabled</p></td><td align="left" valign="top"><p>enables Kubernetes control plane logs</p></td><td align="left" valign="top"><p>true</p></td></tr><tr><td align="left" valign="top"><p>logs.kubernetesSystem.enabled</p></td><td align="left" valign="top"><p>enables Kubernetes system logs (kubelet, crio)</p></td><td align="left" valign="top"><p>true</p></td></tr><tr><td align="left" valign="top"><p>logs.kubernetesUserNamespaces.enabled</p></td><td align="left" valign="top"><p>enables Kubernetes user namespaces logs</p></td><td align="left" valign="top"><p>false</p></td></tr><tr><td align="left" valign="top"><p>logs.kubernetesUserNamespaces.exclude</p></td><td align="left" valign="top"><p>excludes Kubernetes logs for specific namespaces</p></td><td align="left" valign="top"><p>- ""</p></td></tr><tr><td align="left" valign="top"><p>logs.osSystem.enabled</p></td><td align="left" valign="top"><p>enables OS logs (auditd, kernel, wicked, zypper)</p></td><td align="left" valign="top"><p>true</p></td></tr><tr><td align="left" valign="top"><p>persistStateInterval</p></td><td align="left" valign="top"><p>sets interval (number-of-messages) for data state persistency</p></td><td align="left" valign="top"><p>100</p></td></tr><tr><td align="left" valign="top"><p>queue.enabled</p></td><td align="left" valign="top"><p>enables Rsyslog queue</p></td><td align="left" valign="top"><p>false</p></td></tr><tr><td align="left" valign="top"><p>queue.maxDiskSpace</p></td><td align="left" valign="top"><p>sets maximum Rsyslog queue disk space in bytes</p></td><td align="left" valign="top"><p>2147483648</p></td></tr><tr><td align="left" valign="top"><p>queue.size</p></td><td align="left" valign="top"><p>sets Rsyslog queue size in bytes</p></td><td align="left" valign="top"><p>50000</p></td></tr><tr><td align="left" valign="top"><p>resources.limits.cpu</p></td><td align="left" valign="top"><p>sets CPU limits</p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>resources.limits.memory</p></td><td align="left" valign="top"><p>sets memory limits</p></td><td align="left" valign="top"><p>512 Mi</p></td></tr><tr><td align="left" valign="top"><p>resources.requests.cpu</p></td><td align="left" valign="top"><p>sets CPU for requests</p></td><td align="left" valign="top"><p>100m</p></td></tr><tr><td align="left" valign="top"><p>resources.requests.memory</p></td><td align="left" valign="top"><p>sets memory for requests</p></td><td align="left" valign="top"><p>512 Mi</p></td></tr><tr><td align="left" valign="top"><p>resumeInterval</p></td><td align="left" valign="top"><p>specifies time (seconds) after failure before retry is attempted</p></td><td align="left" valign="top"><p>30</p></td></tr><tr><td align="left" valign="top"><p>resumeRetryCount</p></td><td align="left" valign="top"><p>sets number of retries after first failure before the log is discarded. -1 is unlimited</p></td><td align="left" valign="top"><p>-1</p></td></tr><tr><td align="left" valign="top"><p>server.tls.clientCert</p></td><td align="left" valign="top"><p>sets TLS client certificate</p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>server.tls.clientKey</p></td><td align="left" valign="top"><p>sets TLS client key</p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>server.tls.enabled</p></td><td align="left" valign="top"><p>enables TLS</p></td><td align="left" valign="top"><p>false</p></td></tr><tr><td align="left" valign="top"><p>server.tls.permittedPeer</p></td><td align="left" valign="top"><p>sets a list of TLS/fingerprints or TLS/names with permission to access the server</p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>server.tls.rootCa</p></td><td align="left" valign="top"><p>specifies TLS root certificate authority</p></td><td align="left" valign="top"> </td></tr></tbody></table></div></div></div></div><div class="chapter " id="_monitoring"><div class="titlepage"><div><div><h1 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring</span> <a title="Permalink" class="permalink" href="#_monitoring">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#monitoring-stack"><span class="number">8.1 </span><span class="name">Monitoring Stack</span></a></span></dt><dt><span class="section"><a href="#_health_checks"><span class="number">8.2 </span><span class="name">Health Checks</span></a></span></dt><dt><span class="section"><a href="#horizontal-pod-autoscaler"><span class="number">8.3 </span><span class="name">Horizontal Pod Autoscaler</span></a></span></dt><dt><span class="section"><a href="#_stratos_web_console"><span class="number">8.4 </span><span class="name">Stratos Web Console</span></a></span></dt></dl></div></div><div class="sect1" id="monitoring-stack"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Stack</span> <a title="Permalink" class="permalink" href="#monitoring-stack">#</a></h2></div></div></div><div id="id-1.10.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The described monitoring approach in this document is a generalized example of one way of monitoring a SUSE CaaS Platform cluster.</p><p>Please apply best practices to develop your own monitoring approach using the described examples and available health checking endpoints.</p></div><div class="sect2" id="_introduction_3"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction_3">#</a></h3></div></div></div><p>This document aims to describe monitoring in a Kubernetes cluster.</p><p>The monitoring stack consists of a monitoring/trending system and a visualization platform.
Additionally you can use the in-memory metrics-server to perform automatic scaling (Refer to: <a class="xref" href="#horizontal-pod-autoscaler" title="8.3. Horizontal Pod Autoscaler">Section 8.3, “Horizontal Pod Autoscaler”</a>).</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>Prometheus</strong></span></p><p>Prometheus is an open-source monitoring and trending system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach.
The time series collection happens via a pull mode over HTTP.</p><p>Prometheus consists of multiple components:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Prometheus server: scrapes and stores data to time series database</p></li><li class="listitem "><p><a class="link" href="https://prometheus.io/docs/alerting/alertmanager/" target="_blank">Alertmanager</a> handles client alerts, sanitizes duplicates and noise and routes them to configurable receivers.</p></li><li class="listitem "><p><a class="link" href="https://prometheus.io/docs/practices/pushing/" target="_blank">Pushgateway</a> is an intermediate service which allows you to push metrics from jobs which cannot be scraped.</p></li></ul></div><div id="id-1.10.2.3.4.1.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Deploying Prometheus <a class="link" href="https://prometheus.io/docs/practices/pushing/" target="_blank">Pushgateway</a> is out of the scope of this document.</p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://prometheus.io/docs/instrumenting/exporters/" target="_blank">Exporters</a> are libraries which help to exports existing metrics from 3rd-party system as Prometheus metric.</p></li></ul></div></li><li class="listitem "><p><span class="strong"><strong>Grafana</strong></span></p><p>Grafana is an open-source system for querying, analysing and visualizing metrics.</p></li></ul></div></div><div class="sect2" id="_prerequisites_4"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites_4">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>NGINX Ingress Controller</p><p>Please refer to <a class="xref" href="#nginx-ingress" title="6.8. NGINX Ingress Controller">Section 6.8, “NGINX Ingress Controller”</a> on how to configure ingress in your cluster.
Deploying NGINX Ingress Controller also allows us to provide TLS termination to our services and to provide basic authentication to the Prometheus Expression browser/API.</p></li><li class="listitem "><p>Monitoring namespace</p><p>We will deploy our monitoring stack in its own namespace and therefore create one.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create namespace monitoring</pre></div></li><li class="listitem "><p>Configure Authentication</p><p>We need to create a <code class="literal">basic-auth</code> secret so the NGINX Ingress Controller can perform authentication.</p><p>Install <code class="literal">apache2-utils</code>, which contains <code class="literal">htpasswd</code>, on your local workstation.</p><div class="verbatim-wrap highlight bash"><pre class="screen">zypper in apache2-utils</pre></div><p>Create the secret file <code class="literal">auth</code></p><div class="verbatim-wrap highlight bash"><pre class="screen">htpasswd -c auth admin
New password:
Re-type new password:
Adding password for user admin</pre></div><div id="id-1.10.2.4.2.3.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>It is very important that the filename is <code class="literal">auth</code>.
During creation, a key in the configuration containing the secret is created that is named after the used filename.
The ingress controller will expect a key named <code class="literal">auth</code>. And when you access the monitoring WebUI, you need to enter the username and password.</p></div><p>Create secret in Kubernetes cluster</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create secret generic -n monitoring prometheus-basic-auth --from-file=auth</pre></div></li></ol></div></div><div class="sect2" id="_installation"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation</span> <a title="Permalink" class="permalink" href="#_installation">#</a></h3></div></div></div><p>There will be two different ways of using ingress for accessing the monitoring system.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="xref" href="#installation-for-subdomains" title="8.1.3.1. Installation For Subdomains">Section 8.1.3.1, “Installation For Subdomains”</a>: Using <code class="literal">subdomains</code> for accessing monitoring system such as <code class="literal">prometheus.example.com</code>, <code class="literal">prometheus-alertmanager.example.com</code>, and <code class="literal">grafana.example.com</code>.</p></li><li class="listitem "><p><a class="xref" href="#installation-for-subpaths" title="8.1.3.3. Installation For Subpaths">Section 8.1.3.3, “Installation For Subpaths”</a>: Using <code class="literal">subpaths</code> for accessing monitoring system such as <code class="literal">example.com/prometheus</code>, <code class="literal">example.com/alertmanager</code>, and <code class="literal">example.com/grafana</code>.</p></li></ul></div><div class="sect3" id="installation-for-subdomains"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation For Subdomains</span> <a title="Permalink" class="permalink" href="#installation-for-subdomains">#</a></h4></div></div></div><p>This installation example shows how to install and configure Prometheus and Grafana using subdomains such as <code class="literal">prometheus.example.com</code>, <code class="literal">prometheus-alertmanager.example.com</code>, and <code class="literal">grafana.example.com</code>.</p><div id="id-1.10.2.5.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>In order to provide additional security by using TLS certificates, please make sure you have the <a class="xref" href="#nginx-ingress" title="6.8. NGINX Ingress Controller">Section 6.8, “NGINX Ingress Controller”</a> installed and configured.</p><p>If you don’t need TLS, you may use other methods for exposing these web services as native <code class="literal">LBaaS</code> in OpenStack, haproxy service or k8s native methods as port-forwarding or NodePort but this is out of scope of this document.</p></div></div><div class="sect3" id="_create_dns_entries_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create DNS entries</span> <a title="Permalink" class="permalink" href="#_create_dns_entries_2">#</a></h4></div></div></div><p>In this example, we will use a master node with IP <code class="literal">10.86.4.158</code> in the case of NodePort service of the Ingress Controller.</p><div id="id-1.10.2.5.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You should configure proper DNS names in any production environment.
These values are only for example purposes.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Configure the DNS server</p><div class="verbatim-wrap"><pre class="screen">monitoring.example.com                      IN  A       10.86.4.158
prometheus.example.com                      IN  CNAME   monitoring.example.com
prometheus-alertmanager.example.com         IN  CNAME   monitoring.example.com
grafana.example.com                         IN  CNAME   monitoring.example.com</pre></div></li><li class="listitem "><p>Configure the management workstation <code class="literal">/etc/hosts</code> (optional)</p><div class="verbatim-wrap"><pre class="screen">10.86.4.158 prometheus.example.com prometheus-alertmanager.example.com grafana.example.com</pre></div></li></ol></div><div class="sect4" id="_tls_certificate"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">TLS Certificate</span> <a title="Permalink" class="permalink" href="#_tls_certificate">#</a></h5></div></div></div><p>You must configure your certificates for the components as secrets in the Kubernetes cluster.
Get certificates from your certificate authority.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Individual certificate</p><p>Single-name TLS certificate protects a single sub-domain, and it means each sub-domain owns its private key. From the security perspective, it is recommended to use individual certificates. However, you have to manage the private key and the certificate rotation separately.</p><div id="id-1.10.2.5.5.5.3.1.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Note Down Secret Names For Configuration</h6><p>When you choose to secure each service with an individual certificate, you must repeat the step below for each component and adjust the name for the individual secret each time. Please note down the names of the secrets you have created.</p><p>In this example, the secret name is <code class="literal">monitoring-tls</code>.</p></div></li><li class="listitem "><p>Wildcard certificate</p><p>Wildcard TLS allows you to secure multiple sub-domains with one certificate and it means multiple sub-domains share the same private key. You can then add more sub-domains without having to redeploy the certificate and moreover, save the additional certificate costs.</p></li></ol></div><p>Refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> on how to sign the trusted certificate or refer to <a class="xref" href="#self-signed-server-certificate" title="6.9.9.2.2. Self-signed Server Certificate">Section 6.9.9.2.2, “Self-signed Server Certificate”</a> on how to sign the self-signed certificate. The <code class="literal">server.conf</code> for DNS.1 is <code class="literal">prometheus.example.com</code> and <code class="literal">prometheus-alertmanager.example.com</code> <code class="literal">grafana.example.com</code> for individual certificates separately. The <code class="literal">server.conf</code> for DNS.1 is <code class="literal">*.example.com</code> for a wildcard certificate.</p><p>Then, import your certificate and key pair into the Kubernetes cluster secret name <code class="literal">monitoring-tls</code>. In this example, the certificate and key are <code class="literal">monitoring.crt</code> and <code class="literal">monitoring.key</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create -n monitoring secret tls monitoring-tls  \
--key  ./monitoring.key \
--cert ./monitoring.crt</pre></div></div><div class="sect4" id="_prometheus"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prometheus</span> <a title="Permalink" class="permalink" href="#_prometheus">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a configuration file <code class="literal">prometheus-config-values.yaml</code></p><p>We need to configure the storage for our deployment.
Choose among the options and uncomment the line in the config file.
In production environments you must configure persistent storage.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Use an existing <code class="literal">PersistentVolumeClaim</code></p></li><li class="listitem "><p>Use a <code class="literal">StorageClass</code> (preferred)</p></li></ul></div><div class="verbatim-wrap"><pre class="screen"># Alertmanager configuration
alertmanager:
  enabled: true
  ingress:
    enabled: true
    hosts:
    -  prometheus-alertmanager.example.com
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
    tls:
      - hosts:
        - prometheus-alertmanager.example.com
        secretName: monitoring-tls
  persistentVolume:
    enabled: true
    ## Use a StorageClass
    storageClass: my-storage-class
    ## Create a PersistentVolumeClaim of 2Gi
    size: 2Gi
    ## Use an existing PersistentVolumeClaim (my-pvc)
    #existingClaim: my-pvc

## Alertmanager is configured through alertmanager.yml. This file and any others
## listed in alertmanagerFiles will be mounted into the alertmanager pod.
## See configuration options https://prometheus.io/docs/alerting/configuration/
#alertmanagerFiles:
#  alertmanager.yml:

# Create a specific service account
serviceAccounts:
  nodeExporter:
    name: prometheus-node-exporter

# Node tolerations for node-exporter scheduling to nodes with taints
# Allow scheduling of node-exporter on master nodes
nodeExporter:
  hostNetwork: false
  hostPID: false
  podSecurityPolicy:
    enabled: true
    annotations:
      apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
      apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
      seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

# Disable Pushgateway
pushgateway:
  enabled: false

# Prometheus configuration
server:
  ingress:
    enabled: true
    hosts:
    - prometheus.example.com
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
    tls:
      - hosts:
        - prometheus.example.com
        secretName: monitoring-tls
  persistentVolume:
    enabled: true
    ## Use a StorageClass
    storageClass: my-storage-class
    ## Create a PersistentVolumeClaim of 8Gi
    size: 8Gi
    ## Use an existing PersistentVolumeClaim (my-pvc)
    #existingClaim: my-pvc

## Prometheus is configured through prometheus.yml. This file and any others
## listed in serverFiles will be mounted into the server pod.
## See configuration options
## https://prometheus.io/docs/prometheus/latest/configuration/configuration/
#serverFiles:
#  prometheus.yml:</pre></div></li><li class="listitem "><p>Add SUSE helm charts repository</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com</pre></div></li><li class="listitem "><p>Deploy SUSE <code class="literal">prometheus</code> helm chart and pass our configuration values file.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install --name prometheus suse/prometheus \
--namespace monitoring \
--values prometheus-config-values.yaml</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install prometheus suse/prometheus \
--namespace monitoring \
--values prometheus-config-values.yaml</pre></div><p>There need to be 3 pods running (3 node-exporter pods because we have 3 nodes).</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl -n monitoring get pod | grep prometheus
NAME                                             READY     STATUS    RESTARTS   AGE
prometheus-alertmanager-5487596d54-kcdd6         2/2       Running   0          2m
prometheus-kube-state-metrics-566669df8c-krblx   1/1       Running   0          2m
prometheus-node-exporter-jnc5w                   1/1       Running   0          2m
prometheus-node-exporter-qfwp9                   1/1       Running   0          2m
prometheus-node-exporter-sc4ls                   1/1       Running   0          2m
prometheus-server-6488f6c4cd-5n9w8               2/2       Running   0          2m</pre></div><p>There need to be be 2 ingresses configured</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get ingress -n monitoring
NAME                      HOSTS                                 ADDRESS   PORTS     AGE
prometheus-alertmanager   prometheus-alertmanager.example.com             80, 443   87s
prometheus-server         prometheus.example.com                          80, 443   87s</pre></div></li><li class="listitem "><p>At this stage, the Prometheus Expression browser/API should be accessible, depending on your network configuration</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://prometheus.example.com:32443</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://prometheus.example.com</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://prometheus.example.com</code></p></li></ul></div></li></ol></div></div><div class="sect4" id="alertmanager-configuration-example"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alertmanager Configuration Example</span> <a title="Permalink" class="permalink" href="#alertmanager-configuration-example">#</a></h5></div></div></div><p>The configuration example sets one "receiver" to get notified by email when one of below conditions is met:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Node is unschedulable: severity is <code class="literal">critical</code> because the node cannot accept new pods</p></li><li class="listitem "><p>Node runs out of disk space: severity is <code class="literal">critical</code> because the node cannot accept new pods</p></li><li class="listitem "><p>Node has memory pressure: severity is <code class="literal">warning</code></p></li><li class="listitem "><p>Node has disk pressure: severity is <code class="literal">warning</code></p></li><li class="listitem "><p>Certificates is going to expire in 7 days: severity is <code class="literal">critical</code></p></li><li class="listitem "><p>Certificates is going to expire in 30 days: severity is <code class="literal">warning</code></p></li><li class="listitem "><p>Certificates is going to expire in 3 months: severity is <code class="literal">info</code></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Configure alerting receiver in Alertmanager</p><p>The Alertmanager handles alerts sent by Prometheus server, it takes care of deduplicating, grouping, and routing them to the correct receiver integration such as email. It also takes care of silencing and inhibition of alerts.</p><p>Add the <code class="literal">alertmanagerFiles</code> section to your Prometheus configuration file <code class="literal">prometheus-config-values.yaml</code>.</p><p>For more information on how to configure Alertmanager, refer to <a class="link" href="https://prometheus.io/docs/alerting/configuration" target="_blank">Prometheus: Alerting - Configuration</a>.</p><div class="verbatim-wrap"><pre class="screen">alertmanagerFiles:
  alertmanager.yml:
    global:
      # The smarthost and SMTP sender used for mail notifications.
      smtp_from: alertmanager@example.com
      smtp_smarthost: smtp.example.com:587
      smtp_auth_username: admin@example.com
      smtp_auth_password: &lt;PASSWORD&gt;
      smtp_require_tls: true

    route:
      # The labels by which incoming alerts are grouped together.
      group_by: ['node']

      # When a new group of alerts is created by an incoming alert, wait at
      # least 'group_wait' to send the initial notification.
      # This way ensures that you get multiple alerts for the same group that start
      # firing shortly after another are batched together on the first
      # notification.
      group_wait: 30s

      # When the first notification was sent, wait 'group_interval' to send a batch
      # of new alerts that started firing for that group.
      group_interval: 5m

      # If an alert has successfully been sent, wait 'repeat_interval' to
      # resend them.
      repeat_interval: 3h

      # A default receiver
      receiver: admin-example

    receivers:
    - name: 'admin-example'
      email_configs:
      - to: 'admin@example.com'</pre></div></li><li class="listitem "><p>Configures alerting rules in Prometheus server</p><p>Replace the <code class="literal">serverFiles</code> section of the Prometheus configuration file <code class="literal">prometheus-config-values.yaml</code>.</p><p>For more information on how to configure alerts, refer to: <a class="link" href="https://prometheus.io/docs/alerting/notification_examples/" target="_blank">Prometheus: Alerting - Notification Template Examples</a></p><div class="verbatim-wrap"><pre class="screen">serverFiles:
  alerts: {}
  rules:
    groups:
    - name: caasp.node.rules
      rules:
      - alert: NodeIsNotReady
        expr: kube_node_status_condition{condition="Ready",status="false"} == 1 or kube_node_status_condition{condition="Ready",status="unknown"} == 1
        for: 1m
        labels:
          severity: critical
        annotations:
          description: '{{ $labels.node }} is not ready'
      - alert: NodeIsOutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        labels:
          severity: critical
        annotations:
          description: '{{ $labels.node }} has insufficient free disk space'
      - alert: NodeHasDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.node }} has insufficient available disk space'
      - alert: NodeHasInsufficientMemory
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.node }} has insufficient available memory'
    - name: caasp.certs.rules
      rules:
      - alert: KubernetesCertificateExpiry3Months
        expr: (cert_exporter_cert_expires_in_seconds / 86400) &lt; 90
        labels:
          severity: info
        annotations:
          description: 'The cert for {{ $labels.filename }} on {{ $labels.nodename }} node is going to expire in 3 months'
      - alert: KubernetesCertificateExpiry30Days
        expr: (cert_exporter_cert_expires_in_seconds / 86400) &lt; 30
        labels:
          severity: warning
        annotations:
          description: 'The cert for {{ $labels.filename }} on {{ $labels.nodename }} node is going to expire in 30 days'
      - alert: KubernetesCertificateExpiry7Days
        expr: (cert_exporter_cert_expires_in_seconds / 86400) &lt; 7
        labels:
          severity: critical
        annotations:
          description: 'The cert for {{ $labels.filename }} on {{ $labels.nodename }} node is going to expire in 7 days'
      - alert: KubeconfigCertificateExpiry3Months
        expr: (cert_exporter_kubeconfig_expires_in_seconds / 86400) &lt; 90
        labels:
          severity: info
        annotations:
          description: 'The cert for {{ $labels.filename }} on {{ $labels.nodename }} node is going to expire in 3 months'
      - alert: KubeconfigCertificateExpiry30Days
        expr: (cert_exporter_kubeconfig_expires_in_seconds / 86400) &lt; 30
        labels:
          severity: warning
        annotations:
          description: 'The cert for {{ $labels.filename }} on {{ $labels.nodename }} node is going to expire in 30 days'
      - alert: KubeconfigCertificateExpiry7Days
        expr: (cert_exporter_kubeconfig_expires_in_seconds / 86400) &lt; 7
        labels:
          severity: critical
        annotations:
          description: 'The cert for {{ $labels.filename }} on {{ $labels.nodename }} node is going to expire in 7 days'
      - alert: AddonCertificateExpiry3Months
        expr: (cert_exporter_secret_expires_in_seconds / 86400) &lt; 90
        labels:
          severity: info
        annotations:
          description: 'The cert for {{ $labels.secret_name }} is going to expire in 3 months'
      - alert: AddonCertificateExpiry30Days
        expr: (cert_exporter_secret_expires_in_seconds / 86400) &lt; 30
        labels:
          severity: warning
        annotations:
          description: 'The cert for {{ $labels.secret_name }} is going to expire in 30 days'
      - alert: AddonCertificateExpiry7Days
        expr: (cert_exporter_secret_expires_in_seconds / 86400) &lt; 7
        labels:
          severity: critical
        annotations:
          description: 'The cert for {{ $labels.secret_name }} is going to expire in 7 days'</pre></div></li><li class="listitem "><p>To apply the changed configuration, run:</p><div class="verbatim-wrap"><pre class="screen">helm upgrade prometheus suse/prometheus --namespace monitoring --values prometheus-config-values.yaml</pre></div></li><li class="listitem "><p>You should now be able to see your Alertmanager, depending on your network configuration</p></li></ol></div></li><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://prometheus-alertmanager.example.com:32443</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://prometheus-alertmanager.example.com</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://prometheus-alertmanager.example.com</code></p></li></ul></div></div><div class="sect4" id="recording-rules-configuration-example"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recording Rules Configuration Example</span> <a title="Permalink" class="permalink" href="#recording-rules-configuration-example">#</a></h5></div></div></div><p>Recording rules allow you to precompute frequently needed or computationally
expensive expressions and save their result as a new set of time series.
Querying the precomputed result will then often be much faster than executing
the original expression every time it is needed. This is especially useful for
dashboards, which need to query the same expression repeatedly every time they
refresh. Another common use case is federation where precomputed metrics are
scraped from one Prometheus instance by another.</p><p>For more information on how to configure recording rules, refer to
<a class="link" href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/#recording-rules" target="_blank">Prometheus:Recording Rules - Configuration</a>.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Configuring recording rules</p><p>Add the following group of rules in the <code class="literal">serverFiles</code> section of the <code class="literal">prometheus-config-values.yaml</code> configuration file.</p><div class="verbatim-wrap"><pre class="screen">serverFiles:
  alerts: {}
  rules:
    groups:
    - name: node-exporter.rules
      rules:
      - expr: count by (instance) (count without (mode) (node_cpu_seconds_total{component="node-exporter"}))
        record: instance:node_num_cpu:sum
      - expr: 1 - avg by (instance) (rate(node_cpu_seconds_total{component="node-exporter",mode="idle"}[5m]))
        record: instance:node_cpu_utilisation:rate5m
      - expr: node_load1{component="node-exporter"} / on (instance) instance:node_num_cpu:sum
        record: instance:node_load1_per_cpu:ratio
      - expr: node_memory_MemAvailable_bytes / on (instance) node_memory_MemTotal_bytes
        record: instance:node_memory_utilisation:ratio
      - expr: rate(node_vmstat_pgmajfault{component="node-exporter"}[5m])
        record: instance:node_vmstat_pgmajfault:rate5m
      - expr: rate(node_disk_io_time_seconds_total{component="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[5m])
        record: instance_device:node_disk_io_time_seconds:rate5m
      - expr: rate(node_disk_io_time_weighted_seconds_total{component="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[5m])
        record: instance_device:node_disk_io_time_weighted_seconds:rate5m
      - expr: sum by (instance) (rate(node_network_receive_bytes_total{component="node-exporter", device!="lo"}[5m]))
        record: instance:node_network_receive_bytes_excluding_lo:rate5m
      - expr: sum by (instance) (rate(node_network_transmit_bytes_total{component="node-exporter", device!="lo"}[5m]))
        record: instance:node_network_transmit_bytes_excluding_lo:rate5m
      - expr: sum by (instance) (rate(node_network_receive_drop_total{component="node-exporter", device!="lo"}[5m]))
        record: instance:node_network_receive_drop_excluding_lo:rate5m
      - expr: sum by (instance) (rate(node_network_transmit_drop_total{component="node-exporter", device!="lo"}[5m]))
        record: instance:node_network_transmit_drop_excluding_lo:rate5m</pre></div></li><li class="listitem "><p>To apply the changed configuration, run:</p><div class="verbatim-wrap"><pre class="screen">helm upgrade prometheus suse/prometheus --namespace monitoring --values prometheus-config-values.yaml</pre></div></li><li class="listitem "><p>You should now be able to see your configured rules, depending on your network configuration</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://prometheus.example.com:32443/rules</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://prometheus.example.com/rules</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://prometheus.example.com/rules</code></p></li></ul></div></li></ol></div></div><div class="sect4" id="_grafana"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Grafana</span> <a title="Permalink" class="permalink" href="#_grafana">#</a></h5></div></div></div><p>Starting from Grafana 5.0, it is possible to dynamically provision the data sources and dashboards via files.
In a Kubernetes cluster, these files are provided via the utilization of <code class="literal">ConfigMap</code>, editing a <code class="literal">ConfigMap</code> will result by the modification of the configuration without having to delete/recreate the pod.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Configure Grafana provisioning</p><p>Create the default datasource configuration file <code class="literal">grafana-datasources.yaml</code> which point to our Prometheus server</p><div class="verbatim-wrap"><pre class="screen">kind: ConfigMap
apiVersion: v1
metadata:
  name: grafana-datasources
  namespace: monitoring
  labels:
     grafana_datasource: "1"
data:
  datasource.yaml: |-
    apiVersion: 1
    deleteDatasources:
      - name: Prometheus
        orgId: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-server.monitoring.svc.cluster.local:80
      access: proxy
      orgId: 1
      isDefault: true</pre></div></li><li class="listitem "><p>Create the <code class="literal">ConfigMap</code> in Kubernetes cluster</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create -f grafana-datasources.yaml</pre></div></li><li class="listitem "><p>Configure storage for the deployment</p><p>Choose among the options and uncomment the line in the config file.
In production environments you must configure persistent storage.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Use an existing <code class="literal">PersistentVolumeClaim</code></p></li><li class="listitem "><p>Use a <code class="literal">StorageClass</code> (preferred)</p><p>Create a file <code class="literal">grafana-config-values.yaml</code> with the appropriate values</p><div class="verbatim-wrap"><pre class="screen"># Configure admin password
adminPassword: &lt;PASSWORD&gt;

# Ingress configuration
ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
  hosts:
    - grafana.example.com
  tls:
    - hosts:
      - grafana.example.com
      secretName: monitoring-tls

# Configure persistent storage
persistence:
  enabled: true
  accessModes:
    - ReadWriteOnce
  ## Use a StorageClass
  storageClassName: my-storage-class
  ## Create a PersistentVolumeClaim of 10Gi
  size: 10Gi
  ## Use an existing PersistentVolumeClaim (my-pvc)
  #existingClaim: my-pvc

# Enable sidecar for provisioning
sidecar:
  datasources:
    enabled: true
    label: grafana_datasource
  dashboards:
    enabled: true
    label: grafana_dashboard</pre></div></li></ul></div></li><li class="listitem "><p>Add SUSE helm charts repository</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com</pre></div></li><li class="listitem "><p>Deploy SUSE grafana helm chart and pass our configuration values file</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install --name grafana suse/grafana \
--namespace monitoring \
--values grafana-config-values.yaml</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install grafana suse/grafana \
--namespace monitoring \
--values grafana-config-values.yaml</pre></div></li><li class="listitem "><p>The result should be a running Grafana pod</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl -n monitoring get pod | grep grafana
NAME                                             READY     STATUS    RESTARTS   AGE
grafana-dbf7ddb7d-fxg6d                          3/3       Running   0          2m</pre></div></li><li class="listitem "><p>At this stage, Grafana should be accessible, depending on your network configuration</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://grafana.example.com:32443</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://grafana.example.com</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://grafana.example.com</code></p></li></ul></div></li><li class="listitem "><p>Now you can add Grafana dashboards.</p></li></ol></div></div><div class="sect4" id="adding-grafana-dashboards"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Grafana Dashboards</span> <a title="Permalink" class="permalink" href="#adding-grafana-dashboards">#</a></h5></div></div></div><p>There are three ways to add dashboards to Grafana:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Deploy an existing dashboard from <a class="link" href="https://grafana.com/dashboards" target="_blank">Grafana dashboards</a></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Open the deployed Grafana in your browser and log in.</p></li><li class="listitem "><p>On the home page of Grafana, hover your mousecursor over the + button on the left sidebar and click on the import menuitem.</p></li><li class="listitem "><p>Select an existing dashboard for your purpose from Grafana dashboards. Copy the URL to the clipboard.</p></li><li class="listitem "><p>Paste the URL (for example) <code class="literal">https://grafana.com/dashboards/3131</code> into the first input field to import the "Kubernetes All Nodes" Grafana Dashboard.
After pasting in the url, the view will change to another form.</p></li><li class="listitem "><p>Now select the "Prometheus" datasource in the <code class="literal">prometheus</code> field and click on the import button.</p></li><li class="listitem "><p>The browser will redirect you to your newly created dashboard.</p></li></ol></div></li><li class="listitem "><p>Use our <a class="link" href="https://github.com/SUSE/caasp-monitoring" target="_blank">pre-built dashboards</a> to monitor the SUSE CaaS Platform system</p><div class="verbatim-wrap highlight bash"><pre class="screen"># monitor SUSE CaaS Platform cluster
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-monitoring/master/grafana-dashboards-caasp-cluster.yaml
# monitor SUSE CaaS Platform etcd cluster
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-monitoring/master/grafana-dashboards-caasp-etcd-cluster.yaml
# monitor SUSE CaaS Platform nodes
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-monitoring/master/grafana-dashboards-caasp-nodes.yaml
# monitor SUSE CaaS Platform namespaces
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-monitoring/master/grafana-dashboards-caasp-namespaces.yaml
# monitor SUSE CaaS Platform pods
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-monitoring/master/grafana-dashboards-caasp-pods.yaml
# monitor SUSE CaaS Platform certificates
kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-monitoring/master/grafana-dashboards-caasp-certificates.yaml</pre></div></li><li class="listitem "><p>Build your own dashboard
Deploy your own dashboard by configuration file containing the dashboard definition.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create your dashboard definition file as a <code class="literal">ConfigMap</code>, for example <code class="literal">grafana-dashboards-caasp-cluster.yaml</code>.</p><div class="verbatim-wrap"><pre class="screen">---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards-caasp-cluster
  namespace: monitoring
  labels:
     grafana_dashboard: "1"
data:
  caasp-cluster.json: |-
    {
      "__inputs": [
        {
          "name": "DS_PROMETHEUS",
          "label": "Prometheus",
          "description": "",
          "type": "datasource",
          "pluginId": "prometheus",
          "pluginName": "Prometheus"
        }
      ],
      "__requires": [
        {
          "type": "grafana",
[...]
continues with definition of dashboard JSON
[...]</pre></div></li><li class="listitem "><p>Apply the <code class="literal">ConfigMap</code> to the cluster.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -f grafana-dashboards-caasp-cluster.yaml</pre></div></li></ol></div></li></ul></div></div></div><div class="sect3" id="installation-for-subpaths"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation For Subpaths</span> <a title="Permalink" class="permalink" href="#installation-for-subpaths">#</a></h4></div></div></div><p>This installation example shows how to install and configure Prometheus and Grafana using subpaths such as example.com/prometheus, example.com/alertmanager, and example.com/grafana.</p><div id="id-1.10.2.5.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Overlapped instructions from subdomains will be omitted. Refer to the instruction from subdomains.</p></div></div><div class="sect3" id="_create_dns_entries_3"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create DNS entries</span> <a title="Permalink" class="permalink" href="#_create_dns_entries_3">#</a></h4></div></div></div><p>In this example, we will use a master node with IP <code class="literal">10.86.4.158</code> in the case of NodePort service of the Ingress Controller.</p><div id="id-1.10.2.5.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You should configure proper DNS names in any production environment.
These values are only for example purposes.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Configure the DNS server</p><div class="verbatim-wrap"><pre class="screen">example.com                      IN  A       10.86.4.158</pre></div></li><li class="listitem "><p>Configure the management workstation <code class="literal">/etc/hosts</code> (optional)</p><div class="verbatim-wrap"><pre class="screen">10.86.4.158 example.com</pre></div></li></ol></div><div class="sect4" id="_tls_certificate_2"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">TLS Certificate</span> <a title="Permalink" class="permalink" href="#_tls_certificate_2">#</a></h5></div></div></div><p>You must configure your certificates for the components as secrets in the Kubernetes cluster.
Get certificates from your certificate authority.</p><p>Refer to <a class="xref" href="#trusted-server-certificate" title="6.9.9.1.1. Trusted Server Certificate">Section 6.9.9.1.1, “Trusted Server Certificate”</a> on how to sign the trusted certificate or refer to <a class="xref" href="#self-signed-server-certificate" title="6.9.9.2.2. Self-signed Server Certificate">Section 6.9.9.2.2, “Self-signed Server Certificate”</a> on how to sign the self-signed certificate. The <code class="literal">server.conf</code> for DNS.1 is <code class="literal">example.com</code>.</p><p>Then, import your certificate and key pair into the Kubernetes cluster secret name <code class="literal">monitoring-tls</code>. In this example, the certificate and key are <code class="literal">monitoring.crt</code> and <code class="literal">monitoring.key</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create -n monitoring secret tls monitoring-tls  \
--key  ./monitoring.key \
--cert ./monitoring.crt</pre></div></div><div class="sect4" id="_prometheus_2"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prometheus</span> <a title="Permalink" class="permalink" href="#_prometheus_2">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a configuration file <code class="literal">prometheus-config-values.yaml</code></p><p>We need to configure the storage for our deployment.
Choose among the options and uncomment the line in the config file.
In production environments you must configure persistent storage.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Use an existing <code class="literal">PersistentVolumeClaim</code></p></li><li class="listitem "><p>Use a <code class="literal">StorageClass</code> (preferred)</p></li><li class="listitem "><p>Add the external URL to <code class="literal">baseURL</code> at which the server can be accessed. The <code class="literal">baseURL</code> depends on your network configuration.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>NodePort: <a class="link" href="https://example.com:32443/prometheus" target="_blank">https://example.com:32443/prometheus</a> and <a class="link" href="https://example.com:32443/alertmanager" target="_blank">https://example.com:32443/alertmanager</a></p></li><li class="listitem "><p>External IPs: <a class="link" href="https://example.com/prometheus" target="_blank">https://example.com/prometheus</a> and <a class="link" href="https://example.com/alertmanager" target="_blank">https://example.com/alertmanager</a></p></li><li class="listitem "><p>LoadBalancer: <a class="link" href="https://example.com/prometheus" target="_blank">https://example.com/prometheus</a> and <a class="link" href="https://example.com/alertmanager" target="_blank">https://example.com/alertmanager</a></p></li></ul></div></li></ul></div><div class="verbatim-wrap"><pre class="screen"># Alertmanager configuration
alertmanager:
  enabled: true
  baseURL: https://example.com:32443/alertmanager
  prefixURL: /alertmanager
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
    hosts:
      - example.com/alertmanager
    tls:
      - secretName: monitoring-tls
        hosts:
        - example.com
  persistentVolume:
    enabled: true
    ## Use a StorageClass
    storageClass: my-storage-class
    ## Create a PersistentVolumeClaim of 2Gi
    size: 2Gi
    ## Use an existing PersistentVolumeClaim (my-pvc)
    #existingClaim: my-pvc

## Alertmanager is configured through alertmanager.yml. This file and any others
## listed in alertmanagerFiles will be mounted into the alertmanager pod.
## See configuration options https://prometheus.io/docs/alerting/configuration/
#alertmanagerFiles:
#  alertmanager.yml:

# Create a specific service account
serviceAccounts:
  nodeExporter:
    name: prometheus-node-exporter

# Node tolerations for node-exporter scheduling to nodes with taints
# Allow scheduling of node-exporter on master nodes
nodeExporter:
  hostNetwork: false
  hostPID: false
  podSecurityPolicy:
    enabled: true
    annotations:
      apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
      apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
      seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

# Disable Pushgateway
pushgateway:
  enabled: false

# Prometheus configuration
server:
  baseURL: https://example.com:32443/prometheus
  prefixURL: /prometheus
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
    hosts:
      - example.com/prometheus
    tls:
      - secretName: monitoring-tls
        hosts:
        - example.com
  persistentVolume:
    enabled: true
    ## Use a StorageClass
    storageClass: my-storage-class
    ## Create a PersistentVolumeClaim of 8Gi
    size: 8Gi
    ## Use an existing PersistentVolumeClaim (my-pvc)
    #existingClaim: my-pvc

## Prometheus is configured through prometheus.yml. This file and any others
## listed in serverFiles will be mounted into the server pod.
## See configuration options
## https://prometheus.io/docs/prometheus/latest/configuration/configuration/
#serverFiles:
#  prometheus.yml:</pre></div></li><li class="listitem "><p>Add SUSE helm charts repository</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com</pre></div></li><li class="listitem "><p>Deploy SUSE prometheus helm chart and pass our configuration values file.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install --name prometheus suse/prometheus \
--namespace monitoring \
--values prometheus-config-values.yaml</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install prometheus suse/prometheus \
--namespace monitoring \
--values prometheus-config-values.yaml</pre></div><p>There need to be 3 pods running (3 node-exporter pods because we have 3 nodes).</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl -n monitoring get pod | grep prometheus
NAME                                             READY     STATUS    RESTARTS   AGE
prometheus-alertmanager-5487596d54-kcdd6         2/2       Running   0          2m
prometheus-kube-state-metrics-566669df8c-krblx   1/1       Running   0          2m
prometheus-node-exporter-jnc5w                   1/1       Running   0          2m
prometheus-node-exporter-qfwp9                   1/1       Running   0          2m
prometheus-node-exporter-sc4ls                   1/1       Running   0          2m
prometheus-server-6488f6c4cd-5n9w8               2/2       Running   0          2m</pre></div></li></ol></div></div><div class="sect4" id="_alertmanager_configuration_example"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alertmanager Configuration Example</span> <a title="Permalink" class="permalink" href="#_alertmanager_configuration_example">#</a></h5></div></div></div><p>Refer to <a class="xref" href="#alertmanager-configuration-example" title="8.1.3.2.3. Alertmanager Configuration Example">Section 8.1.3.2.3, “Alertmanager Configuration Example”</a></p></div><div class="sect4" id="_recording_rules_configuration_example"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recording Rules Configuration Example</span> <a title="Permalink" class="permalink" href="#_recording_rules_configuration_example">#</a></h5></div></div></div><p>Refer to <a class="xref" href="#recording-rules-configuration-example" title="8.1.3.2.4. Recording Rules Configuration Example">Section 8.1.3.2.4, “Recording Rules Configuration Example”</a></p></div><div class="sect4" id="_grafana_2"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Grafana</span> <a title="Permalink" class="permalink" href="#_grafana_2">#</a></h5></div></div></div><p>Starting from Grafana 5.0, it is possible to dynamically provision the data sources and dashboards via files.
In Kubernetes cluster, these files are provided via the utilization of <code class="literal">ConfigMap</code>, editing a <code class="literal">ConfigMap</code> will result by the modification of the configuration without having to delete/recreate the pod.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Configure Grafana provisioning</p><p>Create the default datasource configuration file <code class="literal">grafana-datasources.yaml</code> which point to our Prometheus server</p><div class="verbatim-wrap"><pre class="screen">---
kind: ConfigMap
apiVersion: v1
metadata:
  name: grafana-datasources
  namespace: monitoring
  labels:
     grafana_datasource: "1"
data:
  datasource.yaml: |-
    apiVersion: 1
    deleteDatasources:
      - name: Prometheus
        orgId: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-server.monitoring.svc.cluster.local:80
      access: proxy
      orgId: 1
      isDefault: true</pre></div></li><li class="listitem "><p>Create the <code class="literal">ConfigMap</code> in Kubernetes cluster</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create -f grafana-datasources.yaml</pre></div></li><li class="listitem "><p>Configure storage for the deployment</p><p>Choose among the options and uncomment the line in the config file.
In production environments you must configure persistent storage.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Use an existing <code class="literal">PersistentVolumeClaim</code></p></li><li class="listitem "><p>Use a <code class="literal">StorageClass</code> (preferred)</p></li><li class="listitem "><p>Add the external URL to <code class="literal">root_url</code> at which the server can be accessed. The <code class="literal">root_url</code> depends on your network configuration.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>NodePort: <a class="link" href="https://example.com:32443/grafana" target="_blank">https://example.com:32443/grafana</a></p></li><li class="listitem "><p>External IPs: <a class="link" href="https://example.com/grafana" target="_blank">https://example.com/grafana</a></p></li><li class="listitem "><p>LoadBalancer: <a class="link" href="https://example.com/grafana" target="_blank">https://example.com/grafana</a></p></li></ul></div></li></ul></div><p>Create a file <code class="literal">grafana-config-values.yaml</code> with the appropriate values</p><p>+</p><div class="verbatim-wrap"><pre class="screen"># Configure admin password
adminPassword: &lt;PASSWORD&gt;

# Ingress configuration
ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  hosts:
    - example.com
  path: /grafana
  tls:
    - secretName: monitoring-tls
      hosts:
      - example.com

# subpath for grafana
grafana.ini:
  server:
    root_url: https://example.com:32443/grafana

# Configure persistent storage
persistence:
  enabled: true
  accessModes:
    - ReadWriteOnce
  ## Use a StorageClass
  storageClassName: my-storage-class
  ## Create a PersistentVolumeClaim of 10Gi
  size: 10Gi
  ## Use an existing PersistentVolumeClaim (my-pvc)
  #existingClaim: my-pvc

# Enable sidecar for provisioning
sidecar:
  datasources:
    enabled: true
    label: grafana_datasource
  dashboards:
    enabled: true
    label: grafana_dashboard</pre></div></li><li class="listitem "><p>Add SUSE helm charts repository</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com</pre></div></li><li class="listitem "><p>Deploy SUSE grafana helm chart and pass our configuration values file</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install --name grafana suse/grafana \
--namespace monitoring \
--values grafana-config-values.yaml</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install grafana suse/grafana \
--namespace monitoring \
--values grafana-config-values.yaml</pre></div></li><li class="listitem "><p>The result should be a running Grafana pod</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl -n monitoring get pod | grep grafana
NAME                                             READY     STATUS    RESTARTS   AGE
grafana-dbf7ddb7d-fxg6d                          3/3       Running   0          2m</pre></div></li><li class="listitem "><p>Access Prometheus, Alertmanager, and Grafana</p><p>At this stage, the Prometheus Expression browser/API, Alertmanager, and Grafana should be accessible, depending on your network configuration</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Prometheus Expression browser/API</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://example.com:32443/prometheus</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://example.com/prometheus</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://example.com/prometheus</code></p></li></ul></div></li><li class="listitem "><p>Alertmanager</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://example.com:32443/alertmanager</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://example.com/alertmanager</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://example.com/alertmanager</code></p></li></ul></div></li><li class="listitem "><p>Grafana</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>NodePort</strong></span>: <code class="literal">https://example.com:32443/grafana</code></p></li><li class="listitem "><p><span class="strong"><strong>External IPs</strong></span>: <code class="literal">https://example.com/grafana</code></p></li><li class="listitem "><p><span class="strong"><strong>LoadBalancer</strong></span>: <code class="literal">https://example.com/grafana</code></p></li></ul></div></li></ul></div></li><li class="listitem "><p>Now you can add the Grafana dashboards.</p></li></ol></div></div><div class="sect4" id="_adding_grafana_dashboards"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.1.3.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Grafana Dashboards</span> <a title="Permalink" class="permalink" href="#_adding_grafana_dashboards">#</a></h5></div></div></div><p>Refer to <a class="xref" href="#adding-grafana-dashboards" title="8.1.3.2.6. Adding Grafana Dashboards">Section 8.1.3.2.6, “Adding Grafana Dashboards”</a></p></div></div></div><div class="sect2" id="_monitoring_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring</span> <a title="Permalink" class="permalink" href="#_monitoring_2">#</a></h3></div></div></div><div class="sect3" id="_prometheus_jobs"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prometheus Jobs</span> <a title="Permalink" class="permalink" href="#_prometheus_jobs">#</a></h4></div></div></div><p>The Prometheus SUSE helm chart includes the following predefined jobs that will scrape metrics from these jobs using service discovery.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>prometheus: Get metrics from prometheus server</p></li><li class="listitem "><p>kubernetes-apiservers: Get metrics from Kubernetes apiserver</p></li><li class="listitem "><p>kubernetes-nodes: Get metrics from Kubernetes nodes</p></li><li class="listitem "><p>kubernetes-service-endpoints: Get metrics from Services which have annotation <code class="literal">prometheus.io/scrape=true</code> in the metadata</p></li><li class="listitem "><p>kubernetes-pods: Get metrics from Pods which have annotation <code class="literal">prometheus.io/scrape=true</code> in the metadata</p></li></ul></div><p>If you want to monitor new pods and services, you don’t need to change <code class="literal">prometheus.yaml</code> but add annotation <code class="literal">prometheus.io/scrape=true</code>, <code class="literal">prometheus.io/port=&lt;TARGET_PORT&gt;</code> and <code class="literal">prometheus.io/path=&lt;METRIC_ENDPOINT&gt;</code> to your pods and services metadata. Prometheus will automatically scrape the target.</p></div><div class="sect3" id="_etcd_cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ETCD Cluster</span> <a title="Permalink" class="permalink" href="#_etcd_cluster">#</a></h4></div></div></div><p>ETCD server exposes metrics on the <code class="literal">/metrics</code> endpoint. Prometheus jobs do not scrape it by default. Edit the <code class="literal">prometheus.yaml</code> file if you want to monitor the etcd cluster. Since the etcd cluster runs on https, we need to create a certificate to access the endpoint.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a new etcd client certificate signed by etcd CA cert/key pair:</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt; EOF &gt; &lt;CLUSTER_NAME&gt;/pki/etcd/openssl-monitoring-client.conf
[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[v3_req]
keyUsage = digitalSignature,keyEncipherment
extendedKeyUsage = clientAuth

[req_distinguished_name]
O = system:masters
CN = kube-etcd-monitoring-client
EOF

openssl req -nodes -new -newkey rsa:2048 -config &lt;CLUSTER_NAME&gt;/pki/etcd/openssl-monitoring-client.conf -out &lt;CLUSTER_NAME&gt;/pki/etcd/monitoring-client.csr -keyout &lt;CLUSTER_NAME&gt;/pki/etcd/monitoring-client.key
openssl x509 -req -days 365 -CA &lt;CLUSTER_NAME&gt;/pki/etcd/ca.crt -CAkey &lt;CLUSTER_NAME&gt;/pki/etcd/ca.key -CAcreateserial -in &lt;CLUSTER_NAME&gt;/pki/etcd/monitoring-client.csr -out &lt;CLUSTER_NAME&gt;/pki/etcd/monitoring-client.crt -sha256 -extfile &lt;CLUSTER_NAME&gt;/pki/etcd/openssl-monitoring-client.conf -extensions v3_req</pre></div></li><li class="listitem "><p>Create the etcd client certificate to secret in monitoring namespace:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl -n monitoring create secret generic etcd-certs --from-file=&lt;CLUSTER_NAME&gt;/pki/etcd/ca.crt --from-file=&lt;CLUSTER_NAME&gt;/pki/etcd/monitoring-client.crt --from-file=&lt;CLUSTER_NAME&gt;/pki/etcd/monitoring-client.key</pre></div></li><li class="listitem "><p>Get all etcd cluster private IP address:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get pods -n kube-system -l component=etcd -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP             NODE      NOMINATED NODE   READINESS GATES
etcd-master0   1/1     Running   2          21h   192.168.0.6    master0   &lt;none&gt;           &lt;none&gt;
etcd-master1   1/1     Running   2          21h   192.168.0.20   master1   &lt;none&gt;           &lt;none&gt;</pre></div></li><li class="listitem "><p>Edit the configuration file <code class="literal">prometheus-config-values.yaml</code>, add <code class="literal">extraSecretMounts</code> and <code class="literal">extraScrapeConfigs</code> parts, change the extraScrapeConfigs targets IP address(es) as your environment and change the target numbers if you have different etcd cluster members:</p><div class="verbatim-wrap"><pre class="screen"># Prometheus configuration
server:
  ...
  extraSecretMounts:
  - name: etcd-certs
    mountPath: /etc/secrets
    secretName: etcd-certs
    readOnly: true

extraScrapeConfigs: |
  - job_name: etcd
    static_configs:
    - targets: ['192.168.0.32:2379','192.168.0.17:2379','192.168.0.5:2379']
    scheme: https
    tls_config:
      ca_file: /etc/secrets/ca.crt
      cert_file: /etc/secrets/monitoring-client.crt
      key_file: /etc/secrets/monitoring-client.key</pre></div></li><li class="listitem "><p>Upgrade prometheus helm deployment:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm upgrade prometheus suse/prometheus \
--namespace monitoring \
--values prometheus-config-values.yaml</pre></div></li></ol></div></div></div></div><div class="sect1" id="_health_checks"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Health Checks</span> <a title="Permalink" class="permalink" href="#_health_checks">#</a></h2></div></div></div><p>Although Kubernetes cluster takes care of a lot of the traditional deployment
problems on its own, it is good practice to monitor the availability
and health of your services and applications in order to react
to problems should they go beyond the automated measures.</p><p>There are three levels of health checks.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Cluster</p></li><li class="listitem "><p>Node</p></li><li class="listitem "><p>Service / Application</p></li></ul></div><div class="sect2" id="_cluster_health_checks"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Health Checks</span> <a title="Permalink" class="permalink" href="#_cluster_health_checks">#</a></h3></div></div></div><p>The basic check if a cluster is working correctly is based on a few criteria:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Are all services running as expected?</p></li><li class="listitem "><p>Is there at least one Kubernetes master fully working? Even if the deployment is
configured to be highly available, it’s useful to know if
<code class="literal">kube-controller-manager</code> is down on one of the machines.</p></li></ul></div><div id="id-1.10.3.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>For further understanding cluster health information, consider reading
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/</a></p></div><div class="sect3" id="_kubernetes_master"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Kubernetes master</span> <a title="Permalink" class="permalink" href="#_kubernetes_master">#</a></h4></div></div></div><p>All components in Kubernetes cluster expose a <code class="literal">/healthz</code> endpoint. The expected
(healthy) HTTP response status code is <code class="literal">200</code>.</p><p>The minimal services for the master to work properly are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>kube-apiserver:</p><p>The component that receives your requests from <code class="literal">kubectl</code> and from the rest of
the Kubernetes components. The URL is <a class="link" href="https://&lt;CONTROL_PLANE_IP/FQDN&gt;:6443/healthz" target="_blank">https://&lt;CONTROL_PLANE_IP/FQDN&gt;:6443/healthz</a></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Local Check</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -k -i https://localhost:6443/healthz</pre></div></li><li class="listitem "><p>Remote Check</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -k -i https://&lt;CONTROL_PLANE_IP/FQDN&gt;:6443/healthz</pre></div></li></ul></div></li><li class="listitem "><p>kube-controller-manager:</p><p>The component that contains the control loop, driving current state to the
desired state. The URL is <a class="link" href="http://&lt;CONTROL_PLANE_IP/FQDN&gt;:10252/healthz" target="_blank">http://&lt;CONTROL_PLANE_IP/FQDN&gt;:10252/healthz</a></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Local Check</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -i http://localhost:10252/healthz</pre></div></li><li class="listitem "><p>Remote Check</p><p>Make sure firewall allows port <code class="literal">10252</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -i http://&lt;CONTROL_PLANE_IP/FQDN&gt;:10252/healthz</pre></div></li></ul></div></li><li class="listitem "><p>kube-scheduler:</p><p>The component that schedules workloads to nodes. The URL is
<a class="link" href="http://&lt;CONTROL_PLANE_IP/FQDN&gt;:10251/healthz" target="_blank">http://&lt;CONTROL_PLANE_IP/FQDN&gt;:10251/healthz</a></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Local Check</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -i http://localhost:10251/healthz</pre></div></li><li class="listitem "><p>Remote Check</p><p>Make sure firewall allows port <code class="literal">10251</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -i http://&lt;CONTROL_PLANE_IP/FQDN&gt;:10251/healthz</pre></div></li></ul></div></li></ul></div><div id="id-1.10.3.5.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: High-Availability Environments</h6><p>In a HA environment you can monitor <code class="literal">kube-apiserver</code> on
<code class="literal"><a class="link" href="https://&lt;LOAD_BALANCER_IP/FQDN&gt;:6443/healthz" target="_blank">https://&lt;LOAD_BALANCER_IP/FQDN&gt;:6443/healthz</a></code>.</p><p>If any one of the master nodes is running correctly, you will receive a valid response.</p><p>This does, however, not mean that all master nodes necessarily work correctly.
To ensure that all master nodes work properly, the health checks must be
repeated individually for each deployed master node.</p><p>This endpoint will return a successful HTTP response if the cluster is
operational; otherwise it will fail.
It will for example check that it can access <code class="literal">etcd</code>.
This should not be used to infer that the overall cluster health is ideal.
It will return a successful response even when only minimal operational
cluster health exists.</p><p>To probe for full cluster health, you must perform individual health
checking for all machines.</p></div></div><div class="sect3" id="_etcd_cluster_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ETCD Cluster</span> <a title="Permalink" class="permalink" href="#_etcd_cluster_2">#</a></h4></div></div></div><p>The etcd cluster exposes an endpoint <code class="literal">/health</code>. The expected (healthy)
HTTP response body is <code class="literal">{"health":"true"}</code>. The etcd cluster is accessed through
HTTPS only, so be sure to have etcd certificates.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Local Check</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl --cacert /etc/kubernetes/pki/etcd/ca.crt
--cert /etc/kubernetes/pki/etcd/healthcheck-client.crt
--key /etc/kubernetes/pki/etcd/healthcheck-client.key https://localhost:2379/health</pre></div></li><li class="listitem "><p>Remote Check</p><p>Make sure firewall allows port <code class="literal">2379</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl --cacert &lt;ETCD_ROOT_CA_CERT&gt; --cert &lt;ETCD_CLIENT_CERT&gt;
--key &lt;ETCD_CLIENT_KEY&gt; https://&lt;CONTROL_PLANE_IP/FQDN&gt;:2379/health</pre></div></li></ul></div></div></div><div class="sect2" id="_node_health_checks"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Health Checks</span> <a title="Permalink" class="permalink" href="#_node_health_checks">#</a></h3></div></div></div><p>This basic node health check consists of two parts. It checks:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The <span class="strong"><strong>kubelet endpoint</strong></span></p></li><li class="listitem "><p><span class="strong"><strong>CNI (Container Networking Interface) pod state</strong></span></p></li></ol></div><div class="sect3" id="_kubelet"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">kubelet</span> <a title="Permalink" class="permalink" href="#_kubelet">#</a></h4></div></div></div><p>First, determine if kubelet is up and working on the node.</p><p>Kubelet has two ports exposed on all machines:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Port https/10250: exposes kubelet services to the entire cluster and
is available from all nodes through authentication.</p></li><li class="listitem "><p>Port http/10248: is only available on local host.</p></li></ul></div><p>You can send an HTTP request to the endpoint to find out if
kubelet is healthy on that machine. The expected (healthy) HTTP response
status code is <code class="literal">200</code>.</p><div class="sect4" id="_local_check"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Local Check</span> <a title="Permalink" class="permalink" href="#_local_check">#</a></h5></div></div></div><p>If there is an agent running on each node, this agent can simply
fetch the local healthz port:</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -i http://localhost:10248/healthz</pre></div></div><div class="sect4" id="_remote_check"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remote Check</span> <a title="Permalink" class="permalink" href="#_remote_check">#</a></h5></div></div></div><p>There are two ways to fetch endpoints remotely (metrics, healthz, etc.).
Both methods use HTTPS and a token.</p><p><span class="strong"><strong>The first method</strong></span> is executed against the APIServer and mostly used with Prometheus
and Kubernetes discovery <code class="literal">kubernetes_sd_config</code>.
It allows automatic discovery of the nodes and avoids the task of defining monitoring
for each node. For more information see the Kubernetes documentation:
<a class="link" href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config" target="_blank">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config</a></p><p><span class="strong"><strong>The second method</strong></span> directly talks to kubelet and can be used in more traditional
monitoring where one must configure each node to be checked.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>Configuration and Token retrieval:</strong></span></p><p>Create a Service Account (<code class="literal">monitoring</code>) with an associated secondary Token
(<code class="literal">monitoring-secret-token</code>). The token will be used in HTTP requests to authenticate
against the API server.</p><p>This Service Account can only fetch information about nodes and pods.
Best practice is not to use the token that has been created default. Using a secondary
token is also easier for management. Create a file <code class="literal">kubelet.yaml</code> with
the following as content.</p><div class="verbatim-wrap"><pre class="screen">---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: monitoring
  namespace: kube-system
secrets:
- name: monitoring-secret-token
---
apiVersion: v1
kind: Secret
metadata:
  name: monitoring-secret-token
  namespace: kube-system
  annotations:
    kubernetes.io/service-account.name: monitoring
type: kubernetes.io/service-account-token
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-clusterrole
  namespace: kube-system
rules:
- apiGroups: [""]
  resources:
  - nodes/metrics
  - nodes/proxy
  - pods
  verbs: ["get", "list"]
- nonResourceURLs: ["/metrics", "/healthz", "/healthz/*"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: monitoring-clusterrole-binding
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: monitoring-clusterrole
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: monitoring
  namespace: kube-system</pre></div><p>Apply the yaml file:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl apply -f kubelet.yaml</pre></div><p>Export the token to an environment variable:</p><div class="verbatim-wrap highlight bash"><pre class="screen">TOKEN=$(kubectl -n kube-system get secrets monitoring-secret-token
-o jsonpath='{.data.token}' | base64 -d)</pre></div><p>This token can now be passed through the <code class="literal">--header</code> argument as: "Authorization: Bearer $TOKEN".</p><p>Now export important values as environment variables:</p></li><li class="listitem "><p><span class="strong"><strong>Environment Variables Setup</strong></span></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Choose a Kubernetes master node or worker node. The <code class="literal">NODE_IP_FQDN</code> here must
be a node’s IP address or FQDN. The <code class="literal">NODE_NAME</code> here must be a node name in
your Kubernetes cluster. Export the variables <code class="literal">NODE_IP_FQDN</code> and <code class="literal">NODE_NAME</code>
so it can be reused.</p><div class="verbatim-wrap highlight bash"><pre class="screen">NODE_IP_FQDN="10.86.4.158"
NODE_NAME=worker0</pre></div></li><li class="listitem "><p>Retrieve the TOKEN with kubectl.</p><div class="verbatim-wrap highlight bash"><pre class="screen">TOKEN=$(kubectl -n kube-system get secrets monitoring-secret-token
-o jsonpath='{.data.token}' | base64 -d)</pre></div></li><li class="listitem "><p>Get the control plane &lt;IP/FQDN&gt; from the configuration file. You can skip this
step if you only want to use the kubelet endpoint.</p><div class="verbatim-wrap highlight bash"><pre class="screen">CONTROL_PLANE=$(kubectl config view | grep server | cut -f 2- -d ":" | tr -d " ")</pre></div><p>Now the key information to retrieve data from the endpoints should be available
in the environment and you can poll the endpoints.</p></li></ol></div></li><li class="listitem "><p><span class="strong"><strong>Fetching Information from kubelet Endpoint</strong></span></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Make sure firewall allows port <code class="literal">10250</code>.</p></li><li class="listitem "><p>Fetching metrics</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -k https://$NODE_IP_FQDN:10250/metrics --header "Authorization: Bearer $TOKEN"</pre></div></li><li class="listitem "><p>Fetching healthz</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -k https://$NODE_IP_FQDN:10250/healthz --header "Authorization: Bearer $TOKEN"</pre></div></li></ol></div></li><li class="listitem "><p><span class="strong"><strong>Fetching Information from APISERVER Endpoint</strong></span></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Fetching metrics</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -k $CONTROL_PLANE/api/v1/nodes/$NODE_NAME/proxy/metrics --header
"Authorization: Bearer $TOKEN"</pre></div></li><li class="listitem "><p>Fetching healthz</p><div class="verbatim-wrap highlight bash"><pre class="screen">curl -k $CONTROL_PLANE/api/v1/nodes/$NODE_NAME/proxy/healthz --header
"Authorization: Bearer $TOKEN"</pre></div></li></ol></div></li></ul></div></div></div><div class="sect3" id="_cni"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CNI</span> <a title="Permalink" class="permalink" href="#_cni">#</a></h4></div></div></div><p>You can check if the CNI (Container Networking Interface) is working as expected
by check if the <code class="literal">coredns</code> service is running. If CNI has some kind of trouble
<code class="literal">coredns</code> will not be able to start:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get deployments -n kube-system
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
cilium-operator   1/1     1            1           8d
coredns           2/2     2            2           8d
oidc-dex          1/1     1            1           8d
oidc-gangway      1/1     1            1           8d</pre></div><p>If <code class="literal">coredns</code> is running and you are able to create pods then you can be certain
that CNI and your CNI plugin are working correctly.</p><p>There’s also the <a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/" target="_blank">Monitor Node Health</a> check.
This is a <code class="literal">DaemonSet</code> that runs on every node, and reports to the <code class="literal">apiserver</code> back as
<code class="literal">NodeCondition</code> and <code class="literal">Events</code>.</p></div></div><div class="sect2" id="_serviceapplication_health_checks"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service/Application Health Checks</span> <a title="Permalink" class="permalink" href="#_serviceapplication_health_checks">#</a></h3></div></div></div><p>If the deployed services contain a health endpoint, or if they contain an endpoint
that can be used to determine if the service is up, you can use <code class="literal">livenessProbes</code>
and/or <code class="literal">readinessProbes</code>.</p><div id="id-1.10.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Health check endpoints vs. functional endpoints</h6><p>A proper health check is always preferred if designed correctly.</p><p>Despite the fact that any endpoint could potentially be used to infer if your
application is up, it is better to have an endpoint specifically for health in
your application.
Such an endpoint will only respond affirmatively when all your setup code on
the server has finished and the application is running in a desired state.</p></div><p>The <code class="literal">livenessProbes</code> and <code class="literal">readinessProbes</code> share configuration options and probe types.</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.10.3.7.5.1"><span class="term ">initialDelaySeconds</span></dt><dd><p>Number of seconds to wait before performing the very first liveness probe.</p></dd><dt id="id-1.10.3.7.5.2"><span class="term ">periodSeconds</span></dt><dd><p>Number of seconds that the kubelet should wait between liveness probes.</p></dd><dt id="id-1.10.3.7.5.3"><span class="term ">successThreshold</span></dt><dd><p>Number of minimum consecutive successes for the probe to be considered successful (Default: 1).</p></dd><dt id="id-1.10.3.7.5.4"><span class="term ">failureThreshold</span></dt><dd><p>Number of times this probe is allowed to fail in order to assume that the service
is not responding (Default: 3).</p></dd><dt id="id-1.10.3.7.5.5"><span class="term ">timeoutSeconds</span></dt><dd><p>Number of seconds after which the probe times out (Default: 1).</p></dd></dl></div><p>There are different options for the <code class="literal">livenessProbes</code> to check:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.10.3.7.7.1"><span class="term ">Command</span></dt><dd><p>A command executed within a container; a return code of 0 means success.
All other return codes mean failure.</p></dd><dt id="id-1.10.3.7.7.2"><span class="term ">TCP</span></dt><dd><p>If a TCP connection can be established is considered success.</p></dd><dt id="id-1.10.3.7.7.3"><span class="term ">HTTP</span></dt><dd><p>Any HTTP response between <code class="literal">200</code> and <code class="literal">400</code> indicates success.</p></dd></dl></div><div class="sect3" id="_livenessprobe"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">livenessProbe</span> <a title="Permalink" class="permalink" href="#_livenessprobe">#</a></h4></div></div></div><p>livenessProbes are used to detect running but misbehaving pods/a service that might be running
(the process didn’t die), but that is not responding as expected.
You can find out more about livenessProbes here:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></p><p>Probes are executed by each <code class="literal">kubelet</code> against the pods that define them and that
are running in that specific node. When a <code class="literal">livenessProbe</code> fails, Kubernetes will automatically
restart the pod and increase the <code class="literal">RESTARTS</code> count for that pod. These probes will be
executed every <code class="literal">periodSeconds</code> starting from <code class="literal">initialDelaySeconds</code>.</p></div><div class="sect3" id="_readinessprobe"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">readinessProbe</span> <a title="Permalink" class="permalink" href="#_readinessprobe">#</a></h4></div></div></div><p>readinessProbes are used to wait for processes that take some time to start.
Find out more about readinessProbes here: <a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes</a>
Despite the container running, it might be performing some time consuming initialization operations.
During this time, you don’t want Kubernetes to route traffic to that specific pod.
You also don’t want that container to be restarted because it will appear unresponsive.</p><p>These probes will be executed every <code class="literal">periodSeconds</code> starting from <code class="literal">initialDelaySeconds</code>
until the service is ready.</p><p>Both probe types can be used at the same time. If a service is running, but  misbehaving,
the <code class="literal">livenessProbe</code> will ensure that it’s restarted, and the <code class="literal">readinessProbe</code>
will ensure that Kubernetes  won’t route traffic to that specific pod until it’s considered
to be fully functional and running again.</p></div></div><div class="sect2" id="_general_health_checks"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General Health Checks</span> <a title="Permalink" class="permalink" href="#_general_health_checks">#</a></h3></div></div></div><p>We recommend to apply other best practices from system administration to your
monitoring and health checking approach. These steps are not specific to SUSE CaaS Platform
and are beyond the scope of this document.</p></div></div><div class="sect1" id="horizontal-pod-autoscaler"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Horizontal Pod Autoscaler</span> <a title="Permalink" class="permalink" href="#horizontal-pod-autoscaler">#</a></h2></div></div></div><p>Horizontal Pod Autoscaler (HPA) is a tool that automatically increases or decreases the number of pods in a replication controller, deployment, replica set or stateful set, based on metrics collected from pods.</p><p>In order to leverage HPA, <code class="literal">skuba</code> now supports an addon <code class="literal">metrics-server</code>.
The <a class="link" href="https://github.com/kubernetes-sigs/metrics-server" target="_blank">metrics-server</a> addon is first installed into the Kubernetes cluster. After that, HPA fetches metrics from the aggregated API <code class="literal">metrics.k8s.io</code> and according to the user configuration determines whether to increase or decrease the scale of a replication controller, deployment, replica set or stateful set.</p><p>The HPA <code class="literal">metrics.target.type</code> can be one of the following:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>Utilization</strong></span>: the value returned from the metrics server API is calculated as the average resource utilization across all relevant pods and subsequently compared with the <code class="literal">metrics.target.averageUtilization</code>.</p></li><li class="listitem "><p><span class="strong"><strong>AverageValue</strong></span>: the value returned from the metrics server API is divided by the number of all relevant pods, then compared to the <code class="literal">metrics.target.averageValue</code>.</p></li><li class="listitem "><p><span class="strong"><strong>Value</strong></span>: the value returned from the metrics server API is directly compared to the <code class="literal">metrics.target.value</code>.</p></li></ul></div><div id="id-1.10.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The metrics supported by <code class="literal">metrics-server</code> are the <span class="strong"><strong>CPU</strong></span> and <span class="strong"><strong>memory</strong></span> of a pod or node.</p></div><div id="id-1.10.4.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>API versions supported by the HPA:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>CPU metric: <code class="literal">autoscaling/v1</code>,<code class="literal">autoscaling/v2beta2</code></p></li><li class="listitem "><p>Memory metric: <code class="literal">autoscaling/v2beta2</code>.</p></li></ul></div></div><div class="sect2" id="_usage"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage</span> <a title="Permalink" class="permalink" href="#_usage">#</a></h3></div></div></div><p>It is useful to first find out about the available resources of your cluster.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>To display resource (CPU/Memory) usage for nodes, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl top node</pre></div><p>the expected output should look like the following:</p><div class="verbatim-wrap highlight bash"><pre class="screen">NAME        CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
master000   207m         10%    1756Mi          45%
worker000   100m         10%    602Mi           31%</pre></div></li><li class="listitem "><p>To display resource (CPU/Memory) usage for pods, run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl top pod</pre></div><p>the expected output should look like the following:</p><div class="verbatim-wrap highlight bash"><pre class="screen">NAME                                CPU(cores)   MEMORY(bytes)
cilium-9fjw2                        32m          216Mi
cilium-cqnq5                        43m          227Mi
cilium-operator-7d6ddddbf5-2jwgr    1m           46Mi
coredns-69c4947958-2br4b            2m           11Mi
coredns-69c4947958-kb6dq            3m           11Mi
etcd-master000                      21m          584Mi
kube-apiserver-master000            20m          325Mi
kube-controller-manager-master000   6m           105Mi
kube-proxy-x2965                    0m           24Mi
kube-proxy-x9zlv                    0m           19Mi
kube-scheduler-master000            2m           46Mi
kured-45rc2                         1m           25Mi
kured-cptk4                         0m           25Mi
metrics-server-79b8658cd7-gjvhs     1m           21Mi
oidc-dex-55fc689dc-f6cfg            1m           20Mi
oidc-gangway-7b7fbbdbdf-85p6t       1m           18Mi</pre></div><div id="id-1.10.4.8.3.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The option flag <code class="literal">--sort-by=cpu</code>/<code class="literal">--sort-by=memory</code> has an sorting issue at the moment. It will be fixed in the future.</p></div></li></ul></div><div class="sect3" id="_using_horizontal_pod_autoscaler_hpa"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Horizontal Pod Autoscaler (HPA)</span> <a title="Permalink" class="permalink" href="#_using_horizontal_pod_autoscaler_hpa">#</a></h4></div></div></div><p>You can set the HPA to scale according to various metrics.
These include <span class="strong"><strong>average CPU utilization</strong></span>, <span class="strong"><strong>average CPU value</strong></span>, <span class="strong"><strong>average memory utilization</strong></span> and <span class="strong"><strong>average memory value</strong></span>. The following sections show the recommended configuration for each of the aforementioned options.</p><div class="sect4" id="_creating_an_hpa_using_average_cpu_utilization"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.3.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an HPA Using Average CPU Utilization</span> <a title="Permalink" class="permalink" href="#_creating_an_hpa_using_average_cpu_utilization">#</a></h5></div></div></div><p>The following code is an example of what this type of HPA can look like.
You will have to run the code on your admin node or user local machine.
Note that you need a kubeconfig file with RBAC permission that allow setting up autoscale rules into your Kubernetes cluster.</p><div class="verbatim-wrap"><pre class="screen"># deployment
kubectl autoscale deployment &lt;DEPLOYMENT_NAME&gt; \
    --min=&lt;MIN_REPLICAS_NUMBER&gt; \
    --max=&lt;MAX_REPLICAS_NUMBER&gt; \
    --cpu-percent=&lt;PERCENT&gt;

# replication controller
kubectl autoscale replicationcontrollers &lt;REPLICATIONCONTROLLERS_NAME&gt; \
    --min=&lt;MIN_REPLICAS_NUMBER&gt; \
    --max=&lt;MAX_REPLICAS_NUMBER&gt; \
    --cpu-percent=&lt;PERCENT&gt;</pre></div><p>You could for example use the following values:</p><div class="verbatim-wrap"><pre class="screen">kubectl autoscale deployment oidc-dex \
    --name=avg-cpu-util \
    --min=1 \
    --max=10 \
    --cpu-percent=50</pre></div><p>The example output below shows autoscaling works in case of the oidc-dex deployment.
The HPA increases the minimum number of pods to 1 and will increase the pods up to 10, if the average CPU utilization of the pods reaches 50%. For more details about the inner workings of the scaling, refer to <a class="link" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details" target="_blank">The Kubernetes documentation on the horizontal pod autoscale algorithm</a>.</p><p>To check the current status of the HPA run:</p><div class="verbatim-wrap"><pre class="screen">kubectl get hpa</pre></div><p>Example output:</p><div class="verbatim-wrap"><pre class="screen">NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
oidc-dex   Deployment/oidc-dex   0%/50%          1         10        3          115s</pre></div><div id="id-1.10.4.8.4.3.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>To calculate pod CPU utilization HPA divides the total CPU usage of all containers by the total number of CPU requests:</p><p>POD CPU UTILIZATION = TOTAL CPU USAGE OF ALL CONTAINERS / NUMBER OF CPU REQUESTS</p><p>For example:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Container1 requests 0.5 CPU and uses 0 CPU.</p></li><li class="listitem "><p>Container2 requests 1 CPU and uses 2 CPU.</p></li></ul></div><p>The CPU utilization will be (0+2)/(0.5+1)*100 (%)=133 (%)</p><p>If a replication controller, deployment, replica set or stateful set does not specify the CPU request, the output of <code class="literal">kubectl get hpa</code> TARGETS will be unknown.</p></div></div><div class="sect4" id="_creating_an_hpa_using_the_average_cpu_value"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.3.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an HPA Using the Average CPU Value</span> <a title="Permalink" class="permalink" href="#_creating_an_hpa_using_the_average_cpu_value">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a yaml manifest file <code class="literal">hpa-avg-cpu-value.yaml</code> with the following content:</p><div class="verbatim-wrap"><pre class="screen">apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: avg-cpu-value <span id="CO28-1"></span><span class="callout">1</span>
  namespace: kube-system <span id="CO28-2"></span><span class="callout">2</span>
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment <span id="CO28-3"></span><span class="callout">3</span>
    name: example <span id="CO28-4"></span><span class="callout">4</span>
  minReplicas: 1 <span id="CO28-5"></span><span class="callout">5</span>
  maxReplicas: 10 <span id="CO28-6"></span><span class="callout">6</span>
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: AverageValue
        averageValue: 500Mi <span id="CO28-7"></span><span class="callout">7</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO28-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Name of the HPA.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO28-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Namespace of the HPA.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO28-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Specifies the kind of object to scale (a replication controller, deployment, replica set or stateful set).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO28-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Specifies the name of the object to scale.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO28-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Specifies the minimum number of replicas.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO28-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Specifies the maximum number of replicas.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO28-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>The average value of the requested CPU that each pod uses.</p></td></tr></table></div></li><li class="listitem "><p>Apply the yaml manifest by running:</p><div class="verbatim-wrap"><pre class="screen">kubectl apply -f hpa-avg-cpu-value.yaml</pre></div></li><li class="listitem "><p>Check the current status of the HPA:</p><div class="verbatim-wrap"><pre class="screen">kubectl get hpa

NAME            REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
avg-cpu-value   Deployment/php-apache   1m/500Mi   1         10        1          39s</pre></div></li></ol></div></div><div class="sect4" id="_creating_an_hpa_using_average_memory_utilization"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.3.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an HPA Using Average Memory Utilization</span> <a title="Permalink" class="permalink" href="#_creating_an_hpa_using_average_memory_utilization">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a yaml manifest file <code class="literal">hpa-avg-memory-util.yaml</code> with the following content:</p><div class="verbatim-wrap"><pre class="screen">apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: avg-memory-util <span id="CO29-1"></span><span class="callout">1</span>
  namespace: kube-system <span id="CO29-2"></span><span class="callout">2</span>
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment <span id="CO29-3"></span><span class="callout">3</span>
    name: example <span id="CO29-4"></span><span class="callout">4</span>
  minReplicas: 1 <span id="CO29-5"></span><span class="callout">5</span>
  maxReplicas: 10 <span id="CO29-6"></span><span class="callout">6</span>
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 50 <span id="CO29-7"></span><span class="callout">7</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO29-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Name of the HPA.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO29-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Namespace of the HPA.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO29-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Specifies the kind of object to scale (a replication controller, deployment, replica set or stateful set).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO29-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Specifies the name of the object to scale.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO29-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Specifies the minimum number of replicas.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO29-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Specifies the maximum number of replicas.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO29-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>The average utilization of the requested memory that each pod uses.</p></td></tr></table></div></li><li class="listitem "><p>Apply the yaml manifest by running:</p><div class="verbatim-wrap"><pre class="screen">kubectl apply -f hpa-avg-memory-util.yaml</pre></div></li><li class="listitem "><p>Check the current status of the HPA:</p><div class="verbatim-wrap"><pre class="screen">kubectl get hpa

NAME              REFERENCE            TARGETS          MINPODS   MAXPODS   REPLICAS   AGE
avg-memory-util   Deployment/example   5%/50%           1         10        1          4m54s</pre></div><div id="id-1.10.4.8.4.5.2.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>HPA calculates pod memory utilization as: total memory usage of all containers / total memory requests.
If a deployment or replication controller does not specify the memory request, the ouput of <code class="literal">kubectl get hpa</code> TARGETS is &lt;unknown&gt;.</p></div></li></ol></div></div><div class="sect4" id="_creating_an_hpa_using_average_memory_value"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.3.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an HPA Using Average Memory Value</span> <a title="Permalink" class="permalink" href="#_creating_an_hpa_using_average_memory_value">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a yaml manifest file <code class="literal">hpa-avg-memory-value.yaml</code> with the following content:</p><div class="verbatim-wrap"><pre class="screen">apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: avg-memory-value <span id="CO30-1"></span><span class="callout">1</span>
  namespace: kube-system <span id="CO30-2"></span><span class="callout">2</span>
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment <span id="CO30-3"></span><span class="callout">3</span>
    name: example <span id="CO30-4"></span><span class="callout">4</span>
  minReplicas: 1 <span id="CO30-5"></span><span class="callout">5</span>
  maxReplicas: 10 <span id="CO30-6"></span><span class="callout">6</span>
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 500Mi <span id="CO30-7"></span><span class="callout">7</span></pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO30-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Name of the HPA.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO30-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>Namespace of the HPA.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO30-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>Specifies the kind of object to scale (a replication controller, deployment, replica set or stateful set).</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO30-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>Specifies the name of the object to scale.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO30-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>Specifies the minimum number of replicas.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO30-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>Specifies the maximum number of replicas.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO30-7"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>The average value of the requested memory that each pod uses.</p></td></tr></table></div></li><li class="listitem "><p>Apply the yaml manifest by running:</p><div class="verbatim-wrap"><pre class="screen">kubectl apply -f hpa-avg-memory-value.yaml</pre></div></li><li class="listitem "><p>Check the current status of the HPA:</p><div class="verbatim-wrap"><pre class="screen">kubectl get hpa

NAME                     REFERENCE            TARGETS          MINPODS   MAXPODS   REPLICAS   AGE
avg-memory-value         Deployment/example   11603968/500Mi   1         10        1          6m24s</pre></div></li></ol></div></div></div></div></div><div class="sect1" id="_stratos_web_console"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stratos Web Console</span> <a title="Permalink" class="permalink" href="#_stratos_web_console">#</a></h2></div></div></div><div id="id-1.10.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>This feature is offered as a "tech preview".</p><p>We release this as a tech-preview in order to get early feedback from our customers.
Tech previews are largely untested, unsupported, and thus not ready for production use.</p><p>That said, we strongly believe this technology is useful at this stage in order to make the right improvements based on your feedback.
A fully supported, production-ready release is planned for a later point in time.</p></div><div id="id-1.10.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If you plan to deploy SUSE Cloud Application Platform on your SUSE CaaS Platform
cluster please skip this section of the documentation and refer
to the official SUSE Cloud Application Platform instructions. This will include Stratos.</p><p><a class="link" href="https://documentation.suse.com/suse-cap/1.5.2/single-html/cap-guides/#cha-cap-depl-caasp" target="_blank">https://documentation.suse.com/suse-cap/1.5.2/single-html/cap-guides/#cha-cap-depl-caasp</a></p></div><div class="sect2" id="_introduction_4"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction_4">#</a></h3></div></div></div><p>The Stratos user interface (UI) is a modern web-based management application for
Kubernetes and for Cloud Foundry distributions based on Kubernetes like SUSE Cloud Application Platform.</p><p>Stratos provides a graphical management console for both developers and system
administrators.</p><p>A single Stratos instance can be used to monitor multiple Kubernetes clusters
as long as it is granted access to their Kubernetes API endpoint.</p><p>This document aims to describe how to install Stratos in a SUSE CaaS Platform cluster
that doesn’t plan to run any SUSE Cloud Application Platform components.</p><p>The Stratos stack is deployed using helm charts and consists of its web
UI POD and a MariaDB one that is used to store configuration values.</p></div><div class="sect2" id="_prerequisites_5"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites_5">#</a></h3></div></div></div><div class="sect3" id="_helm"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Helm</span> <a title="Permalink" class="permalink" href="#_helm">#</a></h4></div></div></div><p>The deployment of Stratos is performed using a helm chart. Your remote
administration machine must have Helm installed.</p><p>When using a Helm release earlier than 3.0, Helm’s Tiller component has to be
installed and active on your SUSE CaaS Platform cluster.</p></div><div class="sect3" id="_persistent_storage"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persistent Storage</span> <a title="Permalink" class="permalink" href="#_persistent_storage">#</a></h4></div></div></div><p>The MariaDB instance used by Stratos requires a persistent storage to store
its data.</p><p>The cluster must have a Kubernetes Storage Class defined.</p></div></div><div class="sect2" id="_installation_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation</span> <a title="Permalink" class="permalink" href="#_installation_2">#</a></h3></div></div></div><div class="sect3" id="_adding_helm_chart_repository_and_default_values"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding helm chart repository and default values</span> <a title="Permalink" class="permalink" href="#_adding_helm_chart_repository_and_default_values">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Add SUSE helm charts repository</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com</pre></div></li><li class="listitem "><p>Obtain the default <code class="literal">values.yaml</code> file of the helm chart</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm inspect values suse/console &gt; stratos-values.yaml</pre></div></li><li class="listitem "><p>Create the <code class="literal">stratos</code> namespace</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl create namespace stratos</pre></div></li></ol></div></div><div class="sect3" id="_define_admin_user_password"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Define <code class="literal">admin</code> user password</span> <a title="Permalink" class="permalink" href="#_define_admin_user_password">#</a></h4></div></div></div><p>Create a secure password for your admin user and write that into the
<code class="literal">stratos-values.yaml</code> as value of the <code class="literal">console.localAdminPassword</code> key.</p><div id="id-1.10.5.6.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>This step is required to allow the installation of Stratos without
having any SUSE Cloud Application Platform components deployed on the cluster.</p></div></div><div class="sect3" id="_define_the_storage_class_to_be_used"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Define the Storage Class to be used</span> <a title="Permalink" class="permalink" href="#_define_the_storage_class_to_be_used">#</a></h4></div></div></div><p>If your cluster does not have a default storage class configured, or you want
to use a different one, follow these instructions.</p><p>Open the <code class="literal">stratos-values.yaml</code> file and look for the <code class="literal">storageClass</code> entry
defined at the global level, uncomment the line and provide the name of your
Storage Class.</p><p>The values file will have something like that:</p><div class="verbatim-wrap highlight yaml"><pre class="screen"># Specify which storage class should be used for PVCs
storageClass: default</pre></div><div id="id-1.10.5.6.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The file has other <code class="literal">storageClass</code> keys defined inside of some of
its resources. These can be left empty to rely on the global Storage Class that
has just been defined.</p></div></div><div class="sect3" id="_exposing_the_web_ui"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Exposing the Web UI</span> <a title="Permalink" class="permalink" href="#_exposing_the_web_ui">#</a></h4></div></div></div><p>The web interface of Stratos can be exposed either via a Ingress resource or
by using a Service of type <code class="literal">LoadBalancer</code> or even both at the same time.</p><p>An Ingress controller must be deployed on the cluster to be able to expose
the service using an Ingress resource.</p><p>The cluster must be deployed on a platform that can handle <code class="literal">LoadBalancer</code>
objects and must have the Cloud Provider Integration (CPI) enabled. This
can be achieved, for example, when deploying SUSE CaaS Platform on top of OpenStack.</p><p>The behavior is defined inside of the <code class="literal">console.service</code> stanza of the yaml file:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">console:
  service:
    annotations: []
    externalIPs: []
    loadBalancerIP:
    loadBalancerSourceRanges: []
    servicePort: 443
    # nodePort: 30000
    type: ClusterIP
    externalName:
    ingress:
      ## If true, Ingress will be created
      enabled: false

      ## Additional annotations
      annotations: {}

      ## Additional labels
      extraLabels: {}

      ## Host for the ingress
      # Defaults to console.[env.Domain] if env.Domain is set and host is not
      host:

      # Name of secret containing TLS certificate
      secretName:

      # crt and key for TLS Certificate (this chart will create the secret based on these)
      tls:
        crt:
        key:</pre></div><div class="sect4" id="_expose_the_web_ui_using_a_loadbalancer"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.4.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Expose the web UI using a LoadBalancer</span> <a title="Permalink" class="permalink" href="#_expose_the_web_ui_using_a_loadbalancer">#</a></h5></div></div></div><p>The service can be exposes as a <code class="literal">LoadBalancer</code> one by setting the value of
<code class="literal">console.service.type</code> to be <code class="literal">LoadBalancer</code>.</p><p>The <code class="literal">LoadBalancer</code> resource can be tuned by changing the values of the other
<code class="literal">loadBalancer*</code> params specified inside of the <code class="literal">console.service</code> stanza.</p></div><div class="sect4" id="_expose_the_web_ui_using_an_ingress"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.4.3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Expose the web UI using an Ingress</span> <a title="Permalink" class="permalink" href="#_expose_the_web_ui_using_an_ingress">#</a></h5></div></div></div><p>The Ingress resource can be created by setting
<code class="literal">console.service.ingress.enabled</code> to be <code class="literal">true</code>.</p><p>Stratos is exposed by the Ingress using a dedicated host rule. Hence
you must specify the FQDN of the host as a value of the
<code class="literal">console.service.ingress.host</code> key.</p><p>The behavior of the Ingress object can be fine tuned by using the
other keys inside of the <code class="literal">console.service.ingress</code> stanza.</p></div></div><div class="sect3" id="_securing_stratos"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Securing Stratos</span> <a title="Permalink" class="permalink" href="#_securing_stratos">#</a></h4></div></div></div><p>It’s highly recommended to secure Stratos' web interface using TLS encryption.</p><p>This can be done by creating a TLS certificate for Stratos.</p><div class="sect4" id="_secure_stratos_web_ui"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.4.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Secure Stratos web UI</span> <a title="Permalink" class="permalink" href="#_secure_stratos_web_ui">#</a></h5></div></div></div><p>It’s highly recommended to secure the web interface of Stratos by using TLS
encryption. This can be easily done when exposing the web interface using an
Ingress resource.</p><p>Inside of the <code class="literal">console.service.ingress</code> stanza ensure the Ingress resource is
enabled and then specify values for <code class="literal">console.service.ingress.tls.crt</code> and
<code class="literal">console.service.ingress.tls.key</code>. These keys hold the base64 encoded TLS
certificate and key.</p><p>The TLS certificate and key can be base64 encoded by using the following command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">base64 tls.crt
base64 tls.key</pre></div><p>The output produced by the two commands has to be copied into the
<code class="literal">stratos-values.yaml</code> file, resulting in something like that:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">console:
  service:
    ingress:
      enabled: true
      tls: |
        &lt;output of base64 tls.crt&gt;
      key: |
        &lt;output of base64 tls.key&gt;</pre></div></div><div class="sect4" id="_change_mariadb_password"><div class="titlepage"><div><div><h5 class="title"><span class="number">8.4.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change MariaDB password</span> <a title="Permalink" class="permalink" href="#_change_mariadb_password">#</a></h5></div></div></div><p>The helm chart provisions the MariaDB database with a default weak password.
A stronger password can be specified by altering the value of <code class="literal">mariadb.mariadbPassword</code>.</p></div></div><div class="sect3" id="_enable_tech_preview_features"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable tech preview features</span> <a title="Permalink" class="permalink" href="#_enable_tech_preview_features">#</a></h4></div></div></div><p>You can enable tech preview features of Stratos by changing the value of
<code class="literal">console.techPreview</code> from <code class="literal">false</code> to <code class="literal">true</code>.</p></div><div class="sect3" id="_deploying_stratos"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.4.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Stratos</span> <a title="Permalink" class="permalink" href="#_deploying_stratos">#</a></h4></div></div></div><p>Now Stratos can be deployed using helm and the values specified inside of the
<code class="literal">stratos-values.yaml</code> file:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install suse/console \
  --name stratos-console \
  --namespace stratos \
  --values stratos-values.yaml</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install stratos-console suse/console \
  --namespace stratos \
  --values stratos-values.yaml</pre></div><p>You can monitor the status of your Stratos deployment with the watch command:</p><div class="verbatim-wrap highlight bash"><pre class="screen">watch --color 'kubectl get pods --namespace stratos'</pre></div><p>When Stratos is successfully deployed, the following is observed:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>For the volume-migration pod, the STATUS is Completed and the READY column is at 0/1.</p></li><li class="listitem "><p>All other pods have a Running STATUS and a READY value of n/n.</p></li></ul></div><p>Press <code class="literal">Ctrl–C</code> to exit the watch command.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>At this stage Stratos web UI should be accessible. You can log into that using
the <code class="literal">admin</code> user and the password you specified inside of your <code class="literal">stratos-values.yaml</code>
file.</p></li></ol></div></div></div><div class="sect2" id="_stratos_configuration"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stratos configuration</span> <a title="Permalink" class="permalink" href="#_stratos_configuration">#</a></h3></div></div></div><p>Now that Stratos is up and running you can log into it and configure it to
connect to your Kubernetes cluster(s).</p><p>Please refer to the <a class="link" href="https://documentation.suse.com/suse-cap/1.5.2/single-html/cap-guides/#book-cap-guides" target="_blank">SUSE Cloud Application Platform documentation</a> for more information.</p></div></div></div><div class="chapter " id="_storage"><div class="titlepage"><div><div><h1 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage</span> <a title="Permalink" class="permalink" href="#_storage">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_azure_storage"><span class="number">9.1 </span><span class="name">Azure Storage</span></a></span></dt><dt><span class="section"><a href="#_vsphere_storage"><span class="number">9.2 </span><span class="name">vSphere Storage</span></a></span></dt></dl></div></div><div class="sect1" id="_azure_storage"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Azure Storage</span> <a title="Permalink" class="permalink" href="#_azure_storage">#</a></h2></div></div></div><p>The Azure cloud provider can be enabled with SUSE CaaS Platform to allow Kubernetes pods to use Azure disks or Azure files as persistent volumes.</p><p>This chapter provides the volume usage and description.</p><p>Please refer to <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-deployment/#bootstrap" target="_blank">Cluster Bootstrap</a> on how to setup Azure cloud provider enabled cluster.</p><div class="sect2" id="_azure_disk"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Azure Disk</span> <a title="Permalink" class="permalink" href="#_azure_disk">#</a></h3></div></div></div><div class="sect3" id="create_azure_disk"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Disk</span> <a title="Permalink" class="permalink" href="#create_azure_disk">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install Azure CLI on local machine.</p><p>For installation instructions, refer to <a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank">Install the Azure CLI</a>.</p></li><li class="listitem "><p>Create Azure disk. The <code class="literal">DISK_NAME</code> and <code class="literal">DISK_URI</code> will be used as <code class="literal">diskName</code> and <code class="literal">diskURI</code> in Kubernetes manifests for volume binding.</p><div class="verbatim-wrap"><pre class="screen">RESOURCE_GROUP="&lt;RESOURCE_GROUP_NAME&gt;"
DISK_NAME="&lt;VOLUME_NAME&gt;"
VOLUME_TYPE="&lt;VOLUME_TYPE&gt;"
VOLUME_SIZE="&lt;VOLUME_SIZE&gt;"

DISK_URI=`az disk create --resource-group ${RESOURCE_GROUP} --name ${DISK_NAME} --sku ${VOLUME_TYPE} --size-gb ${VOLUME_SIZE} --query id | cut -d'"' -f2`</pre></div><p>&lt;RESOURCE_GROUP_NAME&gt; The Azure resource group name, for example my-cluster-resource-group.</p><p>&lt;VOLUME_NAME&gt; The Azure disk name, for example my-disk.</p><p>&lt;VOLUME_TYPE&gt; The Azure storage SKU. Default value = Premium_LRS. Accepted values are Premium_LRS, StandardSSD_LRS, Standard_LRS, UltraSSD_LRS.</p><p>&lt;VOLUME_SIZE&gt; The Azure disk size to create in GB, for example 10.</p></li></ol></div></div><div class="sect3" id="_in_line_volume"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">In-line Volume</span> <a title="Permalink" class="permalink" href="#_in_line_volume">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create new Azure disk as described in <a class="xref" href="#create_azure_disk" title="9.1.1.1. Create Disk">Section 9.1.1.1, “Create Disk”</a>. The disk used in pod must exist before the resource is created.</p></li><li class="listitem "><p>Create deployement - sample-disk-inline-deployment.yaml.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-disk-inline-deployment.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-disk-inline-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        azureDisk:
          kind: Managed
          diskName: <span id="CO31-1"></span><span class="callout">1</span>
          diskURI: <span id="CO31-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO31-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The disk name, for example my-disk.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO31-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The disk resource ID.</p></td></tr></table></div></li><li class="listitem "><p>Check pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pod
...
NAME                                             READY   STATUS    RESTARTS   AGE
sample-disk-inline-deployment-549dc77d76-pwdqw   1/1     Running   0          3m42s</pre></div></li></ol></div></div><div class="sect3" id="_persistent_volume"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persistent Volume</span> <a title="Permalink" class="permalink" href="#_persistent_volume">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create new Azure disk as described in <a class="xref" href="#create_azure_disk" title="9.1.1.1. Create Disk">Section 9.1.1.1, “Create Disk”</a>. The disk used in pod must exist before the resource is created.</p></li><li class="listitem "><p>Create persistent volume.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-disk-pv.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
  name: sample-static-pv
spec:
  capacity:
    storage: 10Gi <span id="CO32-1"></span><span class="callout">1</span>
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  azureDisk:
    kind: Managed
    diskName: my-disk <span id="CO32-2"></span><span class="callout">2</span>
    diskURI: /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/resource-group-name/providers/Microsoft.Compute/disks/my-disk <span id="CO32-3"></span><span class="callout">3</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO32-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The disk size available.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO32-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The disk name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO32-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The disk resource ID.</p></td></tr></table></div></li><li class="listitem "><p>Create persistent volume claim.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-disk-pv-pvc.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sample-disk-pv-pvc
  labels:
    app: sample
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi <span id="CO33-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO33-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The required volume size.</p></td></tr></table></div></li><li class="listitem "><p>Create deployement.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-disk-pv-deployment.yaml.</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-disk-pv-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data <span id="CO34-1"></span><span class="callout">1</span>
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        persistentVolumeClaim:
          claimName: sample-disk-pv-pvc <span id="CO34-2"></span><span class="callout">2</span>
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO34-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path in deployed pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO34-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The requested persistent volume claim name.</p></td></tr></table></div></li><li class="listitem "><p>Check persistent volume claim is bonded and pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pvc
...
NAME                STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sample-disk-pv-pvc   Bound    sample-disk-pv   10Gi        RWO                           55s

kubectl get pod
...
NAME                                        READY   STATUS    RESTARTS   AGE
sample-disk-pv-deployment-549dc77d76-pwdqw   1/1     Running   0          3m42s</pre></div></li></ol></div></div><div class="sect3" id="_storage_class"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Class</span> <a title="Permalink" class="permalink" href="#_storage_class">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create storage class.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-disk-sc.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: sample-disk-sc
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/azure-disk
parameters:
  skuName: Standard_LRS <span id="CO35-1"></span><span class="callout">1</span>
  location: eastasia <span id="CO35-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO35-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The Azure storage SKU tier. Default is empty.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO35-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Azure storage location. Default is empty.</p></td></tr></table></div></li><li class="listitem "><p>Create persistent volume claim - sample-dynamic-pvc.yaml.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-disk-sc-pvc.yaml.</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sample-disk-sc-pvc
  labels:
    app: sample
spec:
  storageClassName: sample-disk-sc <span id="CO36-1"></span><span class="callout">1</span>
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi <span id="CO36-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO36-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The name of the storage class created.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO36-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The required volume size.</p></td></tr></table></div></li><li class="listitem "><p>Create deployment.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-disk-sc-deployment.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-disk-sc-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data <span id="CO37-1"></span><span class="callout">1</span>
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        persistentVolumeClaim:
          claimName: sample-disk-sc-pvc <span id="CO37-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO37-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path in deployed pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO37-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The requested persistent volume claim name.</p></td></tr></table></div></li><li class="listitem "><p>Check persistent volume claim is bonded and pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pvc
...
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
sample-disk-sc-pvc   Bound    pvc-0ca694b5-0084-4e36-bef1-5b2354158d79   10Gi       RWO            sample-disk-sc      70s

kubectl get pod
...
NAME                                         READY   STATUS    RESTARTS   AGE
sample-disk-sc-deployment-687765d5b5-67vnh   1/1     Running   0          20s</pre></div></li></ol></div></div></div><div class="sect2" id="_azure_file"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Azure File</span> <a title="Permalink" class="permalink" href="#_azure_file">#</a></h3></div></div></div><div id="id-1.11.2.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">cifs-utils</code> package must be installed on all cluster nodes to successfully mount the Azure file share.</p><div class="verbatim-wrap"><pre class="screen">sudo zypper in cifs-utils</pre></div></div><div class="sect3" id="create_azure_fileshare_and_secret"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create File Share and Secret</span> <a title="Permalink" class="permalink" href="#create_azure_fileshare_and_secret">#</a></h4></div></div></div><div class="orderedlist " id="create_azure_fileshare"><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share?tabs=azure-portal" target="_blank">Create an Azure file share</a>.</p></li><li class="listitem "><p>Create Kubernetes file share secret.</p><div class="verbatim-wrap"><pre class="screen">RESOURCE_GROUP="&lt;RESOURCE_GROUP_NAME&gt;"
STORAGE_ACCOUNT="&lt;STORAGE_ACCOUNT_NAME&gt;"
SECRET="&lt;SECRET_NAME&gt;"

# get storage key
STORAGE_KEY=`az storage account keys list --resource-group ${RESOURCE_GROUP} --account-name ${STORAGE_ACCOUNT} --query "[0].value" -o tsv`
echo "Storage account name: "${STORAGE_ACCOUNT}
echo "Storage account key: "${STORAGE_KEY}

# create secret
kubectl create secret generic ${SECRET} --from-literal=azurestorageaccountname=${STORAGE_ACCOUNT} --from-literal=azurestorageaccountkey=${STORAGE_KEY}</pre></div><p>&lt;RESOURCE_GROUP_NAME&gt; The Azure resource group name. Must be same value as in <a class="xref" href="#create_azure_fileshare">???TITLE???</a>.</p><p>&lt;STORAGE_ACCOUNT_NAME&gt; The Azure storage account name. Must be same value as in <a class="xref" href="#create_azure_fileshare">???TITLE???</a>.</p><p>&lt;SECRET_NAME&gt; The Kubernetes file share secret name, for example azure-file-secret.</p></li></ol></div></div><div class="sect3" id="_in_line_volume_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">In-line Volume</span> <a title="Permalink" class="permalink" href="#_in_line_volume_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create new Azure file share and secret as described in <a class="xref" href="#create_azure_fileshare_and_secret" title="9.1.2.1. Create File Share and Secret">Section 9.1.2.1, “Create File Share and Secret”</a>. The file share and secret used in pod must exist before the resource is created.</p></li><li class="listitem "><p>Create deployement.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-inline-deployment.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-file-inline-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        azureFile:
          secretName: azure-file-secret <span id="CO38-1"></span><span class="callout">1</span>
          shareName: my-cluster-file-share <span id="CO38-2"></span><span class="callout">2</span>
          readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes file share secret name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO38-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Azure file share name.</p></td></tr></table></div></li><li class="listitem "><p>Check pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pod
...
NAME                                             READY   STATUS    RESTARTS   AGE
sample-file-inline-deployment-549dc77d76-pwdqw   1/1     Running   0          3m42s</pre></div></li></ol></div></div><div class="sect3" id="_persistent_volume_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persistent Volume</span> <a title="Permalink" class="permalink" href="#_persistent_volume_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create new Azure file share and secret as described in <a class="xref" href="#create_azure_fileshare_and_secret" title="9.1.2.1. Create File Share and Secret">Section 9.1.2.1, “Create File Share and Secret”</a>. The file share and secret used in pod must exist before the resource is created.</p></li><li class="listitem "><p>Create persistent volume.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-pv.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
  name: sample-file-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  azureFile:
      secretName: azure-file-secret <span id="CO39-1"></span><span class="callout">1</span>
      shareName: my-cluster-file-share <span id="CO39-2"></span><span class="callout">2</span>
      readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO39-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes file share secret name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO39-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Azure file share name.</p></td></tr></table></div></li><li class="listitem "><p>Create persistent volume claim.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-pv-pvc.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sample-file-pv-pvc
  labels:
    app: sample
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi <span id="CO40-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO40-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The required volume size.</p></td></tr></table></div></li><li class="listitem "><p>Create deployement.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-pv-deployment.yaml.</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-file-pv-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data <span id="CO41-1"></span><span class="callout">1</span>
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        persistentVolumeClaim:
          claimName: sample-file-pv-pvc <span id="CO41-2"></span><span class="callout">2</span>
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO41-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path in deployed pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO41-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The requested persistent volume claim name.</p></td></tr></table></div></li><li class="listitem "><p>Check persistent volume claim is bonded and pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pvc
...
NAME                STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sample-file-pv-pvc   Bound    sample-file-pv   10Gi        RWO                           55s

kubectl get pod
...
NAME                                        READY   STATUS    RESTARTS   AGE
sample-file-pv-deployment-549dc77d76-pwdqw   1/1     Running   0          3m42s</pre></div></li></ol></div></div><div class="sect3" id="_storage_class_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Class</span> <a title="Permalink" class="permalink" href="#_storage_class_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Creat RBAC for Azure file share.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-rbac.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:azure-cloud-provider
rules:
- apiGroups: ['']
  resources: ['secrets']
  verbs:     ['get','create']
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:azure-cloud-provider
roleRef:
  kind: ClusterRole
  apiGroup: rbac.authorization.k8s.io
  name: system:azure-cloud-provider
subjects:
- kind: ServiceAccount
  name: persistent-volume-binder
  namespace: kube-system</pre></div></div></li><li class="listitem "><p>Create storage class.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-sc.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: sample-file-sc
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/azure-file
parameters:
  skuName: Standard_LRS <span id="CO42-1"></span><span class="callout">1</span>
  location: eastasia <span id="CO42-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO42-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The Azure storage SKU tier. Default is empty.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO42-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Azure storage location. Default is empty.</p></td></tr></table></div></li><li class="listitem "><p>Create persistent volume claim.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-sc-pvc.yaml.</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sample-file-sc-pvc
  labels:
    app: sample
spec:
  storageClassName: sample-file-sc <span id="CO43-1"></span><span class="callout">1</span>
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi <span id="CO43-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO43-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The name of the storage class created.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO43-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The required volume size.</p></td></tr></table></div></li><li class="listitem "><p>Create deployment.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-file-sc-deployment.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data <span id="CO44-1"></span><span class="callout">1</span>
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        persistentVolumeClaim:
          claimName: sample-file-sc-pvc <span id="CO44-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path in deployed pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO44-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The requested persistent volume claim name.</p></td></tr></table></div></li><li class="listitem "><p>Check persistent volume claim is bonded and pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pvc
...
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
sample-file-sc-pvc   Bound    pvc-0ca694b5-0084-4e36-bef1-5b2354158d79   10Gi       RWO            sample-file-sc      70s

kubectl get pod
...
NAME                                         READY   STATUS    RESTARTS   AGE
sample-file-sc-deployment-687765d5b5-67vnh   1/1     Running   0          20s</pre></div></li></ol></div></div></div></div><div class="sect1" id="_vsphere_storage"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">vSphere Storage</span> <a title="Permalink" class="permalink" href="#_vsphere_storage">#</a></h2></div></div></div><p>The vSphere cloud provider can be enabled with SUSE CaaS Platform to allow Kubernetes pods to use VMWare vSphere Virtual Machine Disk (VMDK) volumes as persistent volumes.</p><p>This chapter provides the two types of persistent volume usage and description.</p><p>Please refer to <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/html/caasp-deployment/bootstrap.html" target="_blank">Cluster Bootstrap</a> on how to setup vSphere cloud provider enabled cluster.</p><div class="sect2" id="_node_meta"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Meta</span> <a title="Permalink" class="permalink" href="#_node_meta">#</a></h3></div></div></div><p>Extra node meta could be found when region and zone was added to <code class="literal">vsphere.conf</code> before bootstrap cluster node.</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">[Labels]
region = "&lt;VC_DATACENTER_TAG&gt;"
zone = "&lt;VC_CLUSTER_TAG&gt;"</pre></div></div><p><code class="literal">Region</code> refers to the datacenter and zones refers to the cluster grouping of hosts within the datacenter.
Adding region and zone makes Kubernetes persistent volume created with zone and region labels.
With such an environment, Kubernetes pod scheduler is set to be locational aware for the persistent volume.
For more information refer to: <a class="link" href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/zones.html" target="_blank">https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/zones.html</a>.</p><p>You can view the cloudprovider associated node meta with command.</p><div class="verbatim-wrap"><pre class="screen">kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\tregion: "}{.metadata.labels.failure-domain\.beta\.kubernetes\.io/region}{"\tzone: "}{.metadata.labels.failure-domain\.beta\.kubernetes\.io/zone}{"\n"}{end}'
...
010084072206    region: vcp-provo       zone: vcp-cluster-jazz
010084073045    region: vcp-provo       zone: vcp-cluster-jazz</pre></div></div><div class="sect2" id="_persistent_volume_3"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persistent Volume</span> <a title="Permalink" class="permalink" href="#_persistent_volume_3">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create new VMDK volume in datastore. The VMDK volume used in persistent volume must exist before the resource is created.</p><p>You can use <code class="literal">govc</code> to automate the task.</p><p>For installation instructions, refer to: <a class="link" href="https://github.com/vmware/govmomi/tree/master/govc" target="_blank">https://github.com/vmware/govmomi/tree/master/govc</a>.</p><div class="verbatim-wrap"><pre class="screen">govc datastore.disk.create -dc &lt;DATA_CENTER&gt; -ds &lt;DATA_STORE&gt; -size &lt;DISK_SIZE&gt; &lt;DISK_NAME&gt;</pre></div><p>&lt;DATA_CENTER&gt; The datacenter name in vCenter where Kubernetes nodes reside.</p><p>&lt;DATA_STORE&gt; The datastore in vCenter where volume should be created.</p><p>&lt;DISK_SIZE&gt; The volume size to create, for example 1G.</p><p>&lt;DISK_NAME&gt; The VMDK volume name. for example my-disk.vmdk, or &lt;CLUSTER_NAME&gt;-folder/my-disk.vmdk.</p></li><li class="listitem "><p>Create persistent volume - sample-static-pv.yaml.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-static-pv.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
  name: sample-static-pv <span id="CO45-1"></span><span class="callout">1</span>
spec:
  capacity:
    storage: 1Gi <span id="CO45-2"></span><span class="callout">2</span>
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete <span id="CO45-3"></span><span class="callout">3</span>
  vsphereVolume:
    volumePath: "[datastore] volume/path" <span id="CO45-4"></span><span class="callout">4</span>
    fsType: ext4 <span id="CO45-5"></span><span class="callout">5</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The name of persistent volume resource.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The disk size available.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The policy for how persistent volume should be handled when it is released.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The path to VMDK volume. This path must exist.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The file system type to mount.</p></td></tr></table></div></li><li class="listitem "><p>Create persistent volume claim - sample-static-pvc.yaml.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-static-pvc.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sample-static-pvc
  labels:
    app: sample
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi <span id="CO46-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The required volume size.</p></td></tr></table></div></li><li class="listitem "><p>Create deployement - sample-static-deployment.yaml.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-static-deployment.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-static-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data <span id="CO47-1"></span><span class="callout">1</span>
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        persistentVolumeClaim:
          claimName: sample-static-pvc <span id="CO47-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path in deployed pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The requested persistent volume claim name.</p></td></tr></table></div></li><li class="listitem "><p>Check persistent volume claim is bonded and pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pvc
...
NAME                STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sample-static-pvc   Bound    sample-static-pv   1Gi        RWO                           55s

kubectl get pod
...
NAME                                        READY   STATUS    RESTARTS   AGE
sample-static-deployment-549dc77d76-pwdqw   1/1     Running   0          3m42s</pre></div></li></ol></div></div><div class="sect2" id="_storage_class_3"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Class</span> <a title="Permalink" class="permalink" href="#_storage_class_3">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create storage class - sample-sc.yaml.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-sc.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: sample-sc
  annotations:
    storageclass.kubernetes.io/is-default-class: "true" <span id="CO48-1"></span><span class="callout">1</span>
provisioner: kubernetes.io/vsphere-volume
parameters:
  datastore: "datastore" <span id="CO48-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Set as the default storage class.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The datastore name in vCenter where volume should be created.</p></td></tr></table></div></li><li class="listitem "><p>Create persistent volume claim - sample-dynamic-pvc.yaml.</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-dynamic-pvc.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sample-dynamic-pvc
  annotations:
    volume.beta.kubernetes.io/storage-class: sample-sc <span id="CO49-1"></span><span class="callout">1</span>
  labels:
    app: sample
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi <span id="CO49-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>Annotate with storage class name to use the storage class created.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The required volume size.</p></td></tr></table></div></li><li class="listitem "><p>Create deployment - sample-deployment.yaml</p><div class="verbatim-wrap"><pre class="screen">kubectl create -f sample-deployment.yaml</pre></div><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-dynamic-deployment
  labels:
    app: sample
    tier: sample
spec:
  selector:
    matchLabels:
      app: sample
      tier: sample
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: sample
        tier: sample
    spec:
      containers:
      - image: busybox
        name: sample
        volumeMounts:
        - name: sample-volume
          mountPath: /data <span id="CO50-1"></span><span class="callout">1</span>
        command: [ "sleep", "infinity" ]
      volumes:
      - name: sample-volume
        persistentVolumeClaim:
          claimName: sample-dynamic-pvc <span id="CO50-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path in deployed pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The requested persistent volume claim name.</p></td></tr></table></div></li><li class="listitem "><p>Check persistent volume claim is bonded and pod is running.</p><div class="verbatim-wrap"><pre class="screen">kubectl get pvc
...
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sample-dynamic-pvc   Bound    pvc-0ca694b5-0084-4e36-bef1-5b2354158d79   1Gi        RWO            sample-sc      70s

kubectl get pod
...
NAME                                         READY   STATUS    RESTARTS   AGE
sample-dynamic-deployment-687765d5b5-67vnh   1/1     Running   0          20s</pre></div></li></ol></div></div></div></div><div class="chapter " id="_integration"><div class="titlepage"><div><div><h1 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integration</span> <a title="Permalink" class="permalink" href="#_integration">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#ses-integration"><span class="number">10.1 </span><span class="name">SUSE Enterprise Storage Integration</span></a></span></dt><dt><span class="section"><a href="#_suse_cloud_application_platform_integration"><span class="number">10.2 </span><span class="name">SUSE Cloud Application Platform Integration</span></a></span></dt></dl></div></div><div id="id-1.12.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Integration with external systems might require you to install additional packages to the base OS.
Please refer to <a class="xref" href="#software-installation" title="3.1. Software Installation">Section 3.1, “Software Installation”</a>.</p></div><div class="sect1" id="ses-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage Integration</span> <a title="Permalink" class="permalink" href="#ses-integration">#</a></h2></div></div></div><p>SUSE CaaS Platform offers SUSE Enterprise Storage as a storage solution for its containers.
This chapter describes the steps required for successful integration.</p><div class="sect2" id="_prerequisites_6"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites_6">#</a></h3></div></div></div><p>Before you start with integrating SUSE Enterprise Storage, you need to ensure the following:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The SUSE CaaS Platform cluster must have <code class="literal">ceph-common</code> and <code class="literal">xfsprogs</code> installed on all nodes.
You can check this by running <code class="literal">rpm -q ceph-common</code> and <code class="literal">rpm -q xfsprogs</code>.</p></li><li class="listitem "><p>The SUSE CaaS Platform cluster can communicate with all of the following SUSE Enterprise Storage nodes:
master, monitoring nodes, OSD nodes and the metadata server (in case you need a shared file system).
For more details refer to the SUSE Enterprise Storage documentation:
<a class="link" href="https://documentation.suse.com/ses/6/" target="_blank">https://documentation.suse.com/ses/6/</a>.</p></li></ul></div></div><div class="sect2" id="_procedures_according_to_type_of_integration"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedures According to Type of Integration</span> <a title="Permalink" class="permalink" href="#_procedures_according_to_type_of_integration">#</a></h3></div></div></div><p>The steps will differ in small details depending on whether you are using RBD or
CephFS.</p><div class="sect3" id="_using_rbd_in_pods"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using RBD in Pods</span> <a title="Permalink" class="permalink" href="#_using_rbd_in_pods">#</a></h4></div></div></div><p>RBD, also known as the Ceph Block Device or RADOS Block Device,
facilitates the storage of block-based data in the Ceph distributed storage system.
The procedure below describes steps to take when you need to use a RADOS Block Device in a Kubernetes Pod.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool" target="_blank">Create a Ceph Pool</a>:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create myPool 64 64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-pool" target="_blank">Create a Block Device Pool</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd pool init myPool</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#creating-a-block-device-image" target="_blank">Create a Block Device Image</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd create -s 2G myPool/image</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a>, and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolUser mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=myPool" | tr -d '\n' | base64</pre></div></li><li class="listitem "><p>Create the Secret containing <code class="literal">client.myPoolUser</code> key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
name: ceph-user
namespace: default
type: kubernetes.io/rbd
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1Jreplace== <span id="CO51-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO51-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: ceph-rbd-inline
spec:
  containers:
  - name: ceph-rbd-inline
    image: opensuse/leap
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/ceph_rbd <span id="CO52-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    rbd:
      monitors:
      - 10.244.2.136:6789 <span id="CO52-2"></span><span class="callout">2</span>
      - 10.244.3.123:6789
      - 10.244.4.7:6789
      pool: myPool <span id="CO52-3"></span><span class="callout">3</span>
      image: image <span id="CO52-4"></span><span class="callout">4</span>
      user: myPoolUser <span id="CO52-5"></span><span class="callout">5</span>
      secretRef:
        name: ceph-user <span id="CO52-6"></span><span class="callout">6</span>
      fsType: ext4
      readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Ceph volume image.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool user.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool user key.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/ceph-rbd-inline -- df -k | grep rbd
  Filesystem     1K-blocks    Used Available Use% Mounted on
  /dev/rbd0        1998672    6144   1976144   1% /mnt/ceph_rbd</pre></div></li></ol></div></div><div class="sect3" id="_using_rbd_in_persistent_volumes"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using RBD in Persistent Volumes</span> <a title="Permalink" class="permalink" href="#_using_rbd_in_persistent_volumes">#</a></h4></div></div></div><p>The following procedure describes how to use RBD in a Persistent Volume:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool" target="_blank">Create a Ceph pool</a>:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create myPool 64 64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-pool" target="_blank">Create a Block Device Pool</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd pool init myPool</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#creating-a-block-device-image" target="_blank">Create a Block Device Image</a>:</p><div class="verbatim-wrap"><pre class="screen">rbd create -s 2G myPool/image</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a>, and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolUser mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=myPool" | tr -d '\n' | base64</pre></div></li><li class="listitem "><p>Create the Secret containing <code class="literal">client.myPoolUser</code> key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
name: ceph-user
namespace: default
type: kubernetes.io/rbd
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1Jreplace== <span id="CO53-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
  name: ceph-rbd-pv
spec:
  capacity:
    storage: 2Gi <span id="CO54-1"></span><span class="callout">1</span>
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - 172.28.0.25:6789 <span id="CO54-2"></span><span class="callout">2</span>
    - 172.28.0.21:6789
    - 172.28.0.6:6789
    pool: myPool  <span id="CO54-3"></span><span class="callout">3</span>
    image: image <span id="CO54-4"></span><span class="callout">4</span>
    user: myPoolUser  <span id="CO54-5"></span><span class="callout">5</span>
    secretRef:
      name: ceph-user <span id="CO54-6"></span><span class="callout">6</span>
    fsType: ext4
    readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The size of the volume image. Reference to <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage" target="_blank">Setting requests and limits for local ephemeral storage</a> to see supported suffixes.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Ceph volume image name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool user.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool user key.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume Claim:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-rbd-pv
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  volumeName: ceph-rbd-pv</pre></div></div><div id="id-1.12.3.4.4.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Deleting Persistent Volume Claim does not remove RBD volume in the Ceph cluster.</p></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: ceph-rbd-pv
spec:
  containers:
  - name: ceph-rbd-pv
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/ceph_rbd <span id="CO55-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    persistentVolumeClaim:
      claimName: ceph-rbd-pv <span id="CO55-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Persistent Volume Claim name.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/ceph-rbd-pv -- df -k | grep rbd
  Filesystem     1K-blocks    Used Available Use% Mounted on
  /dev/rbd0        1998672    6144   1976144   1% /mnt/ceph_rbd</pre></div></li></ol></div></div><div class="sect3" id="_using_rbd_in_storage_classes"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using RBD in Storage Classes</span> <a title="Permalink" class="permalink" href="#_using_rbd_in_storage_classes">#</a></h4></div></div></div><p>The following procedure describes how use RBD in Storage Class:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool" target="_blank">Create a Ceph pool</a>:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create myPool 64 64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as pool admin and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolAdmin mds 'allow *' mgr 'allow *' mon 'allow *' osd 'allow * pool=myPool'  | tr -d '\n' | base64</pre></div></li><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as pool user and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myPoolUser mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=myPool" | tr -d '\n' | base64</pre></div></li><li class="listitem "><p>Create the Secret containing the block device pool admin key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
 name: ceph-admin
type: kubernetes.io/rbd
data:
  key: QVFCa0ZJVmZBQUFBQUJBQUp2VzdLbnNIOU1yYll1R0p6T2Zreplace== <span id="CO56-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device pool admin key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Secret containing the block device pool user key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
 name: ceph-user
type: kubernetes.io/rbd
data:
  key: QVFCa0ZJVmZBQUFBQUJBQUp2VzdLbnNIOU1yYll1R0p6T2Zreplace== <span id="CO57-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO57-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The block device pool user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Storage Class:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: ceph-rbd-sc
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.28.0.19:6789, 172.28.0.5:6789, 172.218:6789 <span id="CO58-1"></span><span class="callout">1</span>
  adminId: myPoolAdmin <span id="CO58-2"></span><span class="callout">2</span>
  adminSecretName: ceph-admin <span id="CO58-3"></span><span class="callout">3</span>
  adminSecretNamespace: default
  pool: myPool <span id="CO58-4"></span><span class="callout">4</span>
  userId: myPoolUser <span id="CO58-5"></span><span class="callout">5</span>
  userSecretName: ceph-user <span id="CO58-6"></span><span class="callout">6</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitory nodes IP and port separate by <code class="literal">,</code>. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool admin name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool admin key.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-5"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>The Ceph pool user name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-6"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the Ceph pool user key.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume Claim:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-rbd-sc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi <span id="CO59-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The request volume size. Reference to <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage" target="_blank">Setting requests and limits for local ephemeral storage</a> to see supported suffixes.</p></td></tr></table></div><div id="id-1.12.3.4.5.3.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Deleting Persistent Volume Claim does not remove RBD volume in the Ceph cluster.</p></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: ceph-rbd-sc
spec:
  containers:
  - name:  ceph-rbd-sc
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/ceph_rbd <span id="CO60-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    persistentVolumeClaim:
      claimName: ceph-rbd-sc <span id="CO60-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Persistent Volume Claim name.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/ceph-rbd-sc -- df -k | grep rbd
  Filesystem     1K-blocks    Used Available Use% Mounted on
  /dev/rbd0        1998672    6144   1976144   1% /mnt/ceph_rbd</pre></div></li></ol></div></div><div class="sect3" id="_using_cephfs_in_pods"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using CephFS in Pods</span> <a title="Permalink" class="permalink" href="#_using_cephfs_in_pods">#</a></h4></div></div></div><p>The procedure below describes how to use CephFS in Pod.</p><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Procedure: Using CephFS In Pods </span><a title="Permalink" class="permalink" href="#id-1.12.3.4.6.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as CephFS user and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myCephFSUser mds 'allow *' mgr 'allow *' mon 'allow r' osd 'allow rw pool=cephfs_metadata,allow rwx pool=cephfs_data'  | tr -d '\n' | base64</pre></div><div id="id-1.12.3.4.6.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The <code class="literal">cephfs_data</code> pool should be pre-existed with SES deployment, if not you can create and initialize with:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create cephfs_data 256 256
ceph osd pool create cephfs_metadata 64 64
ceph fs new cephfs cephfs_metadata cephfs_data</pre></div></div><div id="id-1.12.3.4.6.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p><a class="link" href="https://docs.ceph.com/en/latest/cephfs/experimental-features/#multiple-file-systems-within-a-ceph-cluster" target="_blank">Multiple Filesystems Within a Ceph Cluster</a> is still an experimental feature, and disabled by default, to setup more than one filesystem requires to have this feature enabled.
See <a class="link" href="https://docs.ceph.com/en/latest/cephfs/createfs/#create-a-ceph-file-system" target="_blank">Create a Ceph File System</a> on how to create more filesystems.</p></div><div id="id-1.12.3.4.6.3.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Reference to <a class="link" href="https://docs.ceph.com/en/latest/cephfs/client-auth/#cephfs-client-capabilities" target="_blank">CephFS Client Capabilities</a> to see how to further restrict user authority.</p></div></li><li class="listitem "><p>Create the Secret containing the CephFS admin key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: ceph-user
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1J4ZStreplace== <span id="CO61-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The CephFS user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: cephfs-inline
spec:
  containers:
  - name: cephfs-inline
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/cephfs <span id="CO62-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    cephfs:
      monitors:
      - 172.28.0.19:6789 <span id="CO62-2"></span><span class="callout">2</span>
      - 172.28.0.5:6789
      - 172.28.0.18:6789
      user: myCephFSUser <span id="CO62-3"></span><span class="callout">3</span>
      secretRef:
        name: ceph-user <span id="CO62-4"></span><span class="callout">4</span>
      readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The CephFS user name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the CephFS user key.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the volume is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/cephfs-inline -- df -k | grep cephfs
  Filesystem   1K-blocks    Used Available Use% Mounted on
  172.28.0.19:6789,172.28.0.5:6789,172.28.0.18:6789:/
                79245312       0  79245312   0% /mnt/cephfs</pre></div></li></ol></div></div><div class="sect3" id="_using_cephfs_in_persistent_volumes"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using CephFS in Persistent Volumes</span> <a title="Permalink" class="permalink" href="#_using_cephfs_in_persistent_volumes">#</a></h4></div></div></div><p>The following procedure describes how to attach a CephFS static persistent volume to a pod:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p><a class="link" href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-user" target="_blank">Create a Block Device User</a> to use as CephFS user and record the key:</p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.myCephFSUser mds 'allow *' mgr 'allow *' mon 'allow r' osd 'allow rw pool=cephfs_metadata,allow rwx pool=cephfs_data'  | tr -d '\n' | base64</pre></div><div id="id-1.12.3.4.7.3.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The <code class="literal">cephfs_data</code> pool should be pre-existed with SES deployment, if not you can create and initialize with:</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create cephfs_data 256 256
ceph osd pool create cephfs_metadata 64 64
ceph fs new cephfs cephfs_metadata cephfs_data</pre></div></div><div id="id-1.12.3.4.7.3.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p><a class="link" href="https://docs.ceph.com/en/latest/cephfs/experimental-features/#multiple-file-systems-within-a-ceph-cluster" target="_blank">Multiple Filesystems Within a Ceph Cluster</a> is still an experimental feature, and disabled by default, to setup more than one filesystem requires to have this feature enabled.
See <a class="link" href="https://docs.ceph.com/en/latest/cephfs/createfs/#create-a-ceph-file-system" target="_blank">Create a Ceph File System</a> on how to create more filesystem.</p></div><div id="id-1.12.3.4.7.3.1.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Reference to <a class="link" href="https://docs.ceph.com/en/latest/cephfs/client-auth/#cephfs-client-capabilities" target="_blank">CephFS Client Capabilities</a> to see how to further restrict user authority.</p></div></li><li class="listitem "><p>Create the Secret that contains the created CephFS admin key:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: ceph-user
data:
  key: QVFESE1rbGRBQUFBQUJBQWxnSmpZalBEeGlXYS9Qb1J4ZStreplace== <span id="CO63-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO63-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The CephFS user key from the Ceph cluster.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolume
metadata:
  name: cephfs-pv
spec:
  capacity:
    storage: 2Gi <span id="CO64-1"></span><span class="callout">1</span>
  accessModes:
    - ReadWriteOnce
  cephfs:
    monitors:
      - 172.28.0.19:6789 <span id="CO64-2"></span><span class="callout">2</span>
      - 172.28.0.5:6789
      - 172.28.0.18:6789
    user: myCephFSUser <span id="CO64-3"></span><span class="callout">3</span>
    secretRef:
      name: ceph-user <span id="CO64-4"></span><span class="callout">4</span>
    readOnly: false</pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The desired volume size. Reference to <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage" target="_blank">Setting requests and limits for local ephemeral storage</a> to see supported suffixes.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>A list of Ceph monitor nodes IP and port. The default port is <span class="strong"><strong>6789</strong></span>.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-3"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>The CephFS user name.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-4"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>The Kubernetes Secret name contains the CephFS user key.</p></td></tr></table></div></li><li class="listitem "><p>Create the Persistent Volume Claim:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs-pv
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi <span id="CO65-1"></span><span class="callout">1</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The request volume size.</p></td></tr></table></div><div id="id-1.12.3.4.7.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Deleting Persistent Volume Claim does not remove CephFS volume in the Ceph cluster.</p></div></li><li class="listitem "><p>Create the Pod:</p><div class="informalexample"><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: cephfs-pv
spec:
  containers:
  - name: cephfs-pv
    image: busybox
    command: ["sleep", "infinity"]
    volumeMounts:
    - mountPath: /mnt/cephfs <span id="CO66-1"></span><span class="callout">1</span>
      name: volume
  volumes:
  - name: volume
    persistentVolumeClaim:
      claimName: cephfs-pv <span id="CO66-2"></span><span class="callout">2</span></pre></div></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO66-1"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>The volume mount path inside the Pod.</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO66-2"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>The Persistent Volume Claim name.</p></td></tr></table></div></li><li class="listitem "><p>Once the pod is running, check the CephFS is mounted:</p><div class="verbatim-wrap"><pre class="screen">kubectl exec -it pod/cephfs-pv -- df -k | grep cephfs
  Filesystem   1K-blocks    Used Available Use% Mounted on
  172.28.0.19:6789,172.28.0.5:6789,172.28.0.18:6789:/
                79245312       0  79245312   0% /mnt/cephfs</pre></div></li></ol></div></div></div></div><div class="sect1" id="_suse_cloud_application_platform_integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Cloud Application Platform Integration</span> <a title="Permalink" class="permalink" href="#_suse_cloud_application_platform_integration">#</a></h2></div></div></div><p>For integration with SUSE Cloud Application Platform, refer to: <a class="link" href="https://documentation.suse.com/suse-cap/1.5.2/single-html/cap-guides/#cha-cap-depl-caasp" target="_blank">Deploying SUSE Cloud Application Platform on SUSE CaaS Platform</a>.</p></div></div><div class="chapter " id="_gpu_dependent_workloads"><div class="titlepage"><div><div><h1 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">GPU-Dependent Workloads</span> <a title="Permalink" class="permalink" href="#_gpu_dependent_workloads">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#gpus"><span class="number">11.1 </span><span class="name">NVIDIA GPUs</span></a></span></dt></dl></div></div><div class="sect1" id="gpus"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NVIDIA GPUs</span> <a title="Permalink" class="permalink" href="#gpus">#</a></h2></div></div></div><div id="id-1.13.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>This feature is offered as a "tech preview".</p><p>We release this as a tech-preview in order to get early feedback from our customers.
Tech previews are largely untested, unsupported, and thus not ready for production use.</p><p>That said, we strongly believe this technology is useful at this stage in order to make the right improvements based on your feedback.
A fully supported, production-ready release is planned for a later point in time.</p></div><p>Graphics Processing Units (GPUs) provide a powerful way to run compute-intensive workloads such as machine learning pipelines. SUSE’s CaaS Platform supports scheduling GPU-dependent workloads on NVIDIA GPUs as a technical preview. This section illustrates how to prepare your host machine to expose GPU devices to your containers, and how to configure Kubernetes to schedule GPU-dependent workloads.</p><div class="sect2" id="_prepare_the_host_machine"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare the host machine</span> <a title="Permalink" class="permalink" href="#_prepare_the_host_machine">#</a></h3></div></div></div><div class="sect3" id="_install_the_gpu_drivers"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the GPU drivers</span> <a title="Permalink" class="permalink" href="#_install_the_gpu_drivers">#</a></h4></div></div></div><p>Not every worker node in the cluster need have a GPU device present. On the nodes that do have one or more NVIDIA GPUs, install the drivers from NVIDIA’s repository.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># zypper addrepo --refresh https://download.nvidia.com/suse/sle15sp2/ nvidia
# zypper refresh
# zypper install x11-video-nvidiaG05</pre></div><div id="id-1.13.2.4.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>For most modern NVIDIA GPUs, the G05 driver will support your device. Some older devices may require the x11-video-nvidiaG04 driver package instead. Check NVIDIA’s documentation for your GPU device model.</p></div></div><div class="sect3" id="_install_the_oci_hooks"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the OCI hooks</span> <a title="Permalink" class="permalink" href="#_install_the_oci_hooks">#</a></h4></div></div></div><p>OCI hooks are a way for vendors or projects to inject executable actions into the lifecycle of a container managed by the container runtime (runc). SUSE provides an OCI hook for NVIDIA that enable the container runtime and therefor the kubelet and the Kubernetes scheduler to query the host system for the presence of a GPU device and access it directly. Install the hook on the worker nodes with GPUs:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># zypper install nvidia-container-toolkit</pre></div></div><div class="sect3" id="_test_a_gpu_container_image"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test a GPU Container Image</span> <a title="Permalink" class="permalink" href="#_test_a_gpu_container_image">#</a></h4></div></div></div><p>At this point, you should be able to run a container image that requires a GPU and directly access the device from the running container, for example using Podman:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># podman run docker.io/nvidia/cuda nvidia-smi</pre></div></div><div class="sect3" id="_troubleshooting"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#_troubleshooting">#</a></h4></div></div></div><p>At this point, you should be able to run a container image using a GPU. If that is not working, check the following:</p><p>Ensure your GPU is visible from the host system:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># lspci | grep -i nvidia
# nvidia-smi</pre></div><p>Ensure the kernel modules are loaded:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># lsmod | grep nvidia</pre></div><p>If they are not, try loading them explicitly and check dmesg for an error indicating why they are missing:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># nvidia-modprobe
# dmesg | tail</pre></div></div></div><div class="sect2" id="_configure_kubernetes"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Kubernetes</span> <a title="Permalink" class="permalink" href="#_configure_kubernetes">#</a></h3></div></div></div><div class="sect3" id="_install_the_device_plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the Device Plugin</span> <a title="Permalink" class="permalink" href="#_install_the_device_plugin">#</a></h4></div></div></div><p>The Kubernetes device plugin framework allows the kubelet to advertise system hardware resources that the Kubernetes scheduler can then use as hints to schedule workloads that require such devices. The Kubernetesdevice plugin from NVIDIA allows the kubelet to advertise NVIDIA GPUs it finds present on the worker node. Install the device plugin using kubectl:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl create -f https://raw.githubusercontent.com/{nvidia}/k8s-device-plugin/1.0.0-beta6/nvidia-device-plugin.yml</pre></div></div><div class="sect3" id="_taint_gpu_workers"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Taint GPU Workers</span> <a title="Permalink" class="permalink" href="#_taint_gpu_workers">#</a></h4></div></div></div><p>In a heterogeneous cluster, it may be preferable to prevent scheduling pods that do not require a GPU on nodes with a GPU in order to ensure that GPU workloads are not competing for time on the hardware they need to run. To accomplish this, add a taint to the worker nodes that have GPUs:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl taint nodes worker0 nvidia.com/gpu=:PreferNoSchedule</pre></div><p>or</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl taint nodes worker0 nvidia.com/gpu=:NoSchedule</pre></div><p>See the Kubernetes documentation on <a class="link" href="https://v1-18.docs.kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taints and tolerations</a> for a discussion on the considerations for using the NoSchedule or PreferNoSchedule effects. If you use the NoSchedule effect, you must also add the appropriate toleration to infrastructure-critical Daemonsets that must run on all nodes, such as the kured, kube-proxy, and cilium Daemonsets.</p><div id="id-1.13.2.5.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The <a class="link" href="https://v1-18.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration" target="_blank">ExtendedResourceToleration admission controller</a> is enabled on SUSE CaaS Platform v5 by default. This is a mutating admission controller that reviews all pod requests and adds tolerations to any pod that requests an extended resource advertised by a device plugin. For the NVIDIA GPU device plugin, it will automatically add the nvidia.com/gpu toleration to pods that request the nvidia.com/gpu resource, so you will not need to add this toleration explicitly for every GPU workload.</p></div></div><div class="sect3" id="_test_a_gpu_workload"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test a GPU Workload</span> <a title="Permalink" class="permalink" href="#_test_a_gpu_workload">#</a></h4></div></div></div><p>To test your installation you can create a pod that requests GPU devices:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: cuda-container
      image: nvidia/cuda:9.0-devel
      resources:
        limits:
          nvidia.com/gpu: 1 # requesting 1 GPU
    - name: digits-container
      image: nvidia/digits:6.0
      resources:
        limits:
          nvidia.com/gpu: 1 # requesting 1 GPU
EOF</pre></div><p>This example requests a total of two GPUs for two containers. If two GPUs are available on a worker in your cluster, this pod will be scheduled to that worker.</p></div><div class="sect3" id="_troubleshooting_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#_troubleshooting_2">#</a></h4></div></div></div><p>At this point, after a few moments your pod should transition to state "running". If it is not, check the following:</p><p>Examine the pod events for an indication of why it is not being scheduled:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl describe pod gpu-pod</pre></div><p>Examine the events for the device plugin daemonset for any issues:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl describe daemonset nvidia-device-plugin-daemonset --namespace kube-system</pre></div><p>Check the logs of each pod in the daemonset running on a worker that has a GPU:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl logs -l name=nvidia-device-plugin-ds --namespace kube-system</pre></div><p>Check the kubelet log on the worker node that has a GPU. This may indicate errors the container runtime had executing the OCI hook command:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># journalctl -u kubelet</pre></div></div></div><div class="sect2" id="_monitoring_3"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring</span> <a title="Permalink" class="permalink" href="#_monitoring_3">#</a></h3></div></div></div><p>If you have configured <a class="link" href="https://documentation.suse.com/suse-caasp/4.5//html/caasp-admin/_monitoring.html" target="_blank">Monitoring</a> for your cluster, you may want to use NVIDIA’s <a class="link" href="https://developer.nvidia.com/dcgm" target="_blank">Data Center GPU Manager (DCGM) to monitor your GPUs</a>. DCGM integrates with the Prometheus and Grafana services configured for your cluster. Follow the steps below to configure the Prometheus exporter and Grafana dashboard for your NVIDIA GPUs.</p><div class="sect3" id="_configure_a_pod_security_policy"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure a Pod Security Policy</span> <a title="Permalink" class="permalink" href="#_configure_a_pod_security_policy">#</a></h4></div></div></div><p>The DCGM requires use of the hostPath volume type to access the kubelet socket on the host worker node. Create an appropriate Pod Security Policy and RBAC configuration to allow this:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl apply -f - &lt;&lt;EOF
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: nvidia.dcgm
spec:
  privileged: false
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  allowedHostPaths:
    - pathPrefix: /var/lib/kubelet/pod-resources
  volumes:
  - hostPath
  - configMap
  - secret
  - emptyDir
  - downwardAPI
  - projected
  - persistentVolumeClaim
  - nfs
  - rbd
  - cephFS
  - glusterfs
  - fc
  - iscsi
  - cinder
  - gcePersistentDisk
  - awsElasticBlockStore
  - azureDisk
  - azureFile
  - vsphereVolume
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nvidia:dcgm
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - nvidia.dcgm
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nvidia:dcgm
roleRef:
  kind: ClusterRole
  name: nvidia:dcgm
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: Group
  name: system:serviceaccounts:dcgm
EOF</pre></div></div><div class="sect3" id="_create_the_dcgm_exporter"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create the DCGM Exporter</span> <a title="Permalink" class="permalink" href="#_create_the_dcgm_exporter">#</a></h4></div></div></div><p>The DCGM exporter monitors GPUs on each worker node and exposes metrics that can be queried.</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl create namespace dcgm
$ kubectl create --namespace dcgm -f https://raw.githubusercontent.com/NVIDIA/gpu-monitoring-tools/master/dcgm-exporter.yaml</pre></div><p>Check that the metrics are being collected:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ NAME=$(kubectl get pods --namespace dcgm -l "app.kubernetes.io/name=dcgm-exporter" -o "jsonpath={ .items[0].metadata.name}")
$ kubectl port-forward $NAME 8080:9400
$ # in another terminal
$ curl http://127.0.0.1:8080/metrics</pre></div></div><div class="sect3" id="_configure_prometheus"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Prometheus</span> <a title="Permalink" class="permalink" href="#_configure_prometheus">#</a></h4></div></div></div><p>After deploying Prometheus as explained in <a class="link" href="https://documentation.suse.com/suse-caasp/4.5//html/caasp-admin/_monitoring.html" target="_blank">Monitoring</a>, configure Prometheus to monitor the DCGM pods. Gather the cluster IPs of the pods to monitor:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl get pods --namespace dcgm -l "app.kubernetes.io/name=dcgm-exporter" -o "jsonpath={ .items[*].status.podIP}"
10.244.1.10 10.244.2.68</pre></div><p>Add the DCGM pods to Prometheus’s scrape configuration. Edit the Prometheus configmap:</p><div class="verbatim-wrap highlight bash"><pre class="screen">$ kubectl edit --namespace monitoring configmap prometheus-server</pre></div><p>Under the <code class="literal">scrape_configs</code> section add a new job, using the pod IPs found above:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">scrape_configs:
...
- job_name: dcgm
  static_configs:
  - targets: ['10.244.1.10:9400', '10.244.2.68:9400']
...</pre></div><p>Prometheus will automatically reload the new configuration.</p></div><div class="sect3" id="_add_the_grafana_dashboard"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.1.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add the Grafana Dashboard</span> <a title="Permalink" class="permalink" href="#_add_the_grafana_dashboard">#</a></h4></div></div></div><p>Import the DCGM Exporter dashboard into Grafana.</p><p>In the Grafana web interface, navigate to <span class="guimenu ">Manage Dashboards</span> › <span class="guimenu ">Import</span>. In the field <span class="emphasis"><em>_Import via grafana.com</em></span>, enter the dashboard ID <code class="literal">12219</code>, and click <span class="guimenu ">Load</span>.</p><p>Alternatively, download the <a class="link" href="https://raw.githubusercontent.com/NVIDIA/gpu-monitoring-tools/2.0.0-rc.11/grafana/dcgm-exporter-dashboard.json" target="_blank">dashboard JSON definition</a>, and upload it with the <span class="guimenu ">Upload .json file</span> button.</p><p>On the next page, in the dropdown menu <span class="guimenu ">Prometheus</span> › <span class="guimenu ">]</span> › <span class="guimenu ">select <span class="emphasis"><em>Prometheus</em></span> as the data source. Customize the dashboard name and UID to your preference. Then click menu:Import[</span>.</p><p>The new dashboard will appear in the Grafana web interface.</p></div></div></div></div><div class="chapter " id="_cluster_disaster_recovery"><div class="titlepage"><div><div><h1 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Disaster Recovery</span> <a title="Permalink" class="permalink" href="#_cluster_disaster_recovery">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_backing_up_etcd_cluster_data"><span class="number">12.1 </span><span class="name">Backing Up etcd Cluster Data</span></a></span></dt><dt><span class="section"><a href="#_recovering_master_nodes"><span class="number">12.2 </span><span class="name">Recovering Master Nodes</span></a></span></dt></dl></div></div><p>Etcd is a crucial component of Kubernetes - the <span class="strong"><strong>etcd cluster</strong></span> stores the entire Kubernetes cluster state, which means critical configuration data, specifications, as well as the statuses of the running workloads. It also serves as the backend for service discovery. <a class="xref" href="#backup-and-restore-with-velero" title="Chapter 13. Backup and Restore with Velero">Chapter 13, <em>Backup and Restore with Velero</em></a> explains how to use Velero to backup, restore and migrate data. However, the Kubernetes cluster needs to be accessible for Velero to operate. And since the Kubernetes cluster can become inaccessible for many reasons, for example when all of its master nodes are lost, <span class="strong"><strong>it is important to periodically backup etcd cluster data</strong></span>.</p><div class="sect1" id="_backing_up_etcd_cluster_data"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backing Up etcd Cluster Data</span> <a title="Permalink" class="permalink" href="#_backing_up_etcd_cluster_data">#</a></h2></div></div></div><p>This chapter describes the backup of <code class="literal">etcd</code> cluster data running on master nodes of SUSE CaaS Platform.</p><div class="sect2" id="_data_to_backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Data To Backup</span> <a title="Permalink" class="permalink" href="#_data_to_backup">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create backup directories on external storage.</p><div class="verbatim-wrap highlight bash"><pre class="screen">BACKUP_DIR=CaaSP_Backup_`date +%Y%m%d%H%M%S`
mkdir /${BACKUP_DIR}</pre></div></li><li class="listitem "><p>Copy the following files/folders into the backup directory:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The <code class="literal">skuba</code> command-line binary: for the running cluster. Used to replace nodes from cluster.</p></li><li class="listitem "><p>The cluster definition folder: Directory created during bootstrap holding the cluster certificates and configuration.</p></li><li class="listitem "><p>The <code class="literal">etcd</code> cluster database: Holds all non-persistent cluster data.
Can be used to recover master nodes. Please refer to the next section for steps to create an <code class="literal">etcd</code> cluster database backup.</p></li></ul></div></li><li class="listitem "><p>(Optional) Make backup directory into a compressed file, and remove the original backup directory.</p><div class="verbatim-wrap highlight bash"><pre class="screen">tar cfv ${BACKUP_DIR}.tgz /${BACKUP_DIR}
rm -rf /${BACKUP_DIR}</pre></div></li></ol></div></div><div class="sect2" id="_creating_an_etcd_cluster_database_backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an etcd Cluster Database Backup</span> <a title="Permalink" class="permalink" href="#_creating_an_etcd_cluster_database_backup">#</a></h3></div></div></div><div class="sect3" id="_procedure"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="#_procedure">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Mount external storage device to all master nodes.
This is only required if the following step is using local hostpath as volume storage.</p></li><li class="listitem "><p>Create backup.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Find the size of the database to be backed up</p><div class="verbatim-wrap highlight bash"><pre class="screen">ls -sh /var/lib/etcd/member/snap/db</pre></div><div id="id-1.14.3.4.2.2.2.2.1.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The backup size depends on the cluster. Ensure each of the backups has sufficient space.
The available size should be more than the database snapshot file.</p><p>You should also have a rotation method to clean up the unneeded snapshots over time.</p><p>When there is insufficient space available during backup, pods will fail to be in <code class="literal">Running</code> state and <code class="literal">no space left on device</code> errors will show in pod logs.</p><p>The below example manifest shows a binding to a local <code class="literal">hostPath</code>.
We strongly recommend using other storage methods instead.</p></div></li><li class="listitem "><p>Modify the script example</p><p>Replace <code class="literal">&lt;STORAGE_MOUNT_POINT&gt;</code> with the directory in which to store the backup.
The directory must exist on every node in cluster.</p><p>Replace <code class="literal">&lt;IN_CLUSTER_ETCD_IMAGE&gt;</code> with the <code class="literal">etcd</code> image used in the cluster.
This can be retrieved by accessing any one of the nodes in the cluster and running:</p><div class="verbatim-wrap"><pre class="screen">grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'</pre></div></li><li class="listitem "><p>Create a backup deployment</p><p>Run the following script:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ETCD_SNAPSHOT="&lt;STORAGE_MOUNT_POINT&gt;/etcd_snapshot"
ETCD_IMAGE="&lt;IN_CLUSTER_ETCD_IMAGE&gt;"
MANIFEST="etcd-backup.yaml"

cat &lt;&lt; *EOF* &gt; ${MANIFEST}
apiVersion: batch/v1
kind: Job
metadata:
  name: etcd-backup
  namespace: kube-system
  labels:
    jobgroup: backup
spec:
  template:
    metadata:
      name: etcd-backup
      labels:
        jobgroup: backup
    spec:
      containers:
      - name: etcd-backup
        image: ${ETCD_IMAGE}
        env:
        - name: ETCDCTL_API
          value: "3"
        command: ["/bin/sh"]
        args: ["-c", "etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-\$(date +%Y-%m-%d_%H:%M:%S_%Z).db"]
        volumeMounts:
        - mountPath: /etc/kubernetes/pki/etcd
          name: etcd-certs
          readOnly: true
        - mountPath: /backup
          name: etcd-backup
      restartPolicy: OnFailure
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
      - effect: NoSchedule
        operator: Exists
      hostNetwork: true
      volumes:
      - name: etcd-certs
        hostPath:
          path: /etc/kubernetes/pki/etcd
          type: DirectoryOrCreate
      - name: etcd-backup
        hostPath:
          path: ${ETCD_SNAPSHOT}
          type: Directory
*EOF*

kubectl create -f ${MANIFEST}</pre></div><p>If you are using local <code class="literal">hostPath</code> and not using a shared storage device, the <code class="literal">etcd</code> backup will be created to any one of the master nodes.
To find the node associated with each <code class="literal">etcd</code> backup run:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get pods --namespace kube-system --selector=job-name=etcd-backup -o wide</pre></div></li></ol></div></li></ol></div></div></div><div class="sect2" id="_scheduling_etcd_cluster_backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scheduling etcd Cluster Backup</span> <a title="Permalink" class="permalink" href="#_scheduling_etcd_cluster_backup">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Mount external storage device to all master nodes.
This is only required if the following step is using local <code class="literal">hostPath</code> as volume storage.</p></li><li class="listitem "><p>Create Cronjob.</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Find the size of the database to be backed up</p><div id="id-1.14.3.5.2.2.2.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The backup size depends on the cluster. Ensure each of the backups has sufficient space.
The available size should be more than the database snapshot file.</p><p>You should also have a rotation method to clean up the unneeded snapshots over time.</p><p>When there is insufficient space available during backup, pods will fail to be in <code class="literal">Running</code> state and <code class="literal">no space left on device</code> errors will show in pod logs.</p><p>The below example manifest shows a binding to a local <code class="literal">hostPath</code>.
We strongly recommend using other storage methods instead.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">ls -sh /var/lib/etcd/member/snap/db</pre></div></li><li class="listitem "><p>Modify the script example</p><p>Replace <code class="literal">&lt;STORAGE_MOUNT_POINT&gt;</code> with directory to store for backup. The directory must exist on every node in cluster.</p><p>Replace <code class="literal">&lt;IN_CLUSTER_ETCD_IMAGE&gt;</code> with etcd image used in cluster.
This can be retrieved by accessing any one of the nodes in the cluster and running:</p><div class="verbatim-wrap"><pre class="screen">grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'</pre></div></li><li class="listitem "><p>Create a backup schedule deployment</p><p>Run the following script:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ETCD_SNAPSHOT="&lt;STORAGE_MOUNT_POINT&gt;/etcd_snapshot"
ETCD_IMAGE="&lt;IN_CLUSTER_ETCD_IMAGE&gt;"

# SCHEDULE in Cron format. https://crontab.guru/
SCHEDULE="0 1 * * *"

# *_HISTORY_LIMIT is the number of maximum history keep in the cluster.
SUCCESS_HISTORY_LIMIT="3"
FAILED_HISTORY_LIMIT="3"

MANIFEST="etcd-backup.yaml"

cat &lt;&lt; *EOF* &gt; ${MANIFEST}
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  startingDeadlineSeconds: 100
  schedule: "${SCHEDULE}"
  successfulJobsHistoryLimit: ${SUCCESS_HISTORY_LIMIT}
  failedJobsHistoryLimit: ${FAILED_HISTORY_LIMIT}
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: etcd-backup
            image: ${ETCD_IMAGE}
            env:
            - name: ETCDCTL_API
              value: "3"
            command: ["/bin/sh"]
            args: ["-c", "etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-\$(date +%Y-%m-%d_%H:%M:%S_%Z).db"]
            volumeMounts:
            - mountPath: /etc/kubernetes/pki/etcd
              name: etcd-certs
              readOnly: true
            - mountPath: /backup
              name: etcd-backup
          restartPolicy: OnFailure
          nodeSelector:
            node-role.kubernetes.io/master: ""
          tolerations:
          - effect: NoSchedule
            operator: Exists
          hostNetwork: true
          volumes:
          - name: etcd-certs
            hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
          - name: etcd-backup
            # hostPath is only one of the types of persistent volume. Suggest to setup external storage instead.
            hostPath:
              path: ${ETCD_SNAPSHOT}
              type: Directory
*EOF*

kubectl create -f ${MANIFEST}</pre></div></li></ol></div></li></ol></div></div></div><div class="sect1" id="_recovering_master_nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering Master Nodes</span> <a title="Permalink" class="permalink" href="#_recovering_master_nodes">#</a></h2></div></div></div><p>This chapter describes how to recover SUSE CaaS Platform master nodes.</p><div class="sect2" id="_replacing_a_single_master_node"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Single Master Node</span> <a title="Permalink" class="permalink" href="#_replacing_a_single_master_node">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Remove the failed master node with skuba.</p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with failed cluster master node name.</p><div class="verbatim-wrap"><pre class="screen">skuba node remove &lt;NODE_NAME&gt;</pre></div></li><li class="listitem "><p>Delete failed master node from <code class="literal">known_hosts</code>.</p><p>Replace` &lt;NODE_IP&gt;` with failed master node IP address.</p><div class="verbatim-wrap"><pre class="screen">sed -i "/&lt;NODE_IP&gt;/d" known_hosts</pre></div></li><li class="listitem "><p>Prepare a new instance.</p></li><li class="listitem "><p>Use <code class="literal">skuba</code> to join master node from step 3.</p><p>Replace <code class="literal">&lt;NODE_IP&gt;</code> with the new master node ip address.</p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with the new master node name.</p><p>Replace <code class="literal">&lt;USER_NAME&gt;</code> with user name.</p><div class="verbatim-wrap"><pre class="screen">skuba node join --role=master --user=&lt;USER_NAME&gt; --sudo --target &lt;NODE_IP&gt; &lt;NODE_NAME&gt;</pre></div></li></ol></div></div><div class="sect2" id="_recovering_all_master_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering All Master Nodes</span> <a title="Permalink" class="permalink" href="#_recovering_all_master_nodes">#</a></h3></div></div></div><p>Ensure cluster version for backup/restore should be the same. Cross-version restoration in any domain is likely to  encounter data/API compatibility issues.</p><div class="sect3" id="_prerequisites_7"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites_7">#</a></h4></div></div></div><p>You will only need to restore database on one of the master node (<code class="literal">master-0</code>) to regain control-plane access.
etcd will sync the database to all master nodes in the cluster once restored.
This does not mean, however, that the nodes will automatically be added back to the cluster.
You must join one master node to the cluster, restore the database and then continue adding your remaining master nodes (which then will sync automatically).</p><p>Do the following on <code class="literal">master-0</code>. Remote restore is not supported.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install one of the required software packages (<code class="literal">etcdctl</code>, Docker or Podman).</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper install etcdctl</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper install docker
sudo systemctl start docker

ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

sudo docker pull ${ETCD_IMAGE}</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper install podman

ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

sudo podman pull ${ETCD_IMAGE}</pre></div></li></ul></div></li><li class="listitem "><p>Have access to <code class="literal">etcd</code> snapshot from backup device.</p></li></ol></div></div><div class="sect3" id="_procedure_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="#_procedure_2">#</a></h4></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Stop <code class="literal">etcd</code> on all master nodes.</p><div class="verbatim-wrap"><pre class="screen">mv /etc/kubernetes/manifests/etcd.yaml /tmp/</pre></div><p>You can check <code class="literal">etcd</code> container does not exist with <code class="literal">crictl ps | grep etcd</code></p></li><li class="listitem "><p>Purge <code class="literal">etcd</code> data on all master nodes.</p><div class="verbatim-wrap"><pre class="screen">sudo rm -rf /var/lib/etcd</pre></div></li><li class="listitem "><p>Login to <code class="literal">master-0</code> via SSH.</p></li><li class="listitem "><p>Restore <code class="literal">etcd</code> data.</p><p>Replace <code class="literal">&lt;SNAPSHOT_DIR&gt;</code> with directory to the etcd snapshot,
for example: <code class="literal">/share/backup/etcd_snapshot</code></p><p>Replace <code class="literal">&lt;SNAPSHOT&gt;</code> with the name of the <code class="literal">etcd</code> snapshot,
for example: <code class="literal">etcd-snapshot-2019-11-08_05:19:20_GMT.db</code></p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with <code class="literal">master-0</code> cluster node name,
for example: <code class="literal">skuba-master-1</code></p><p>Replace <code class="literal">&lt;NODE_IP&gt;</code> with <code class="literal">master-0</code> cluster node IP address.</p><div id="id-1.14.4.4.4.2.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_IP&gt;</code> must be visible from inside the node.</p><div class="verbatim-wrap highlight bash"><pre class="screen">ip addr | grep &lt;NODE_IP&gt;</pre></div></div><div id="id-1.14.4.4.4.2.4.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_NAME&gt;</code> and <code class="literal">&lt;NODE_IP&gt;</code> must exist after <code class="literal">--initial-cluster</code> in <code class="literal">/etc/kubernetes/manifests/etcd.yaml</code></p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">SNAPSHOT="&lt;SNAPSHOT_DIR&gt;/&lt;SNAPSHOT&gt;"
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo ETCDCTL_API=3 etcdctl snapshot restore ${SNAPSHOT}\
 --data-dir /var/lib/etcd\
 --name ${NODE_NAME}\
 --initial-cluster ${NODE_NAME}=https://${NODE_IP}:2380\
 --initial-advertise-peer-urls https://${NODE_IP}:2380</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">SNAPSHOT="&lt;SNAPSHOT&gt;"
SNAPSHOT_DIR="&lt;SNAPSHOT_DIR&gt;"
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo docker run\
 -v ${SNAPSHOT_DIR}:/etcd_snapshot\
 -v /var/lib:/var/lib\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl snapshot restore /etcd_snapshot/${SNAPSHOT}\
 --data-dir /var/lib/etcd\
 --name ${NODE_NAME}\
 --initial-cluster ${NODE_NAME}=https://${NODE_IP}:2380\
 --initial-advertise-peer-urls https://${NODE_IP}:2380"</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">SNAPSHOT="&lt;SNAPSHOT&gt;"
SNAPSHOT_DIR="&lt;SNAPSHOT_DIR&gt;"
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo podman run\
 -v ${SNAPSHOT_DIR}:/etcd_snapshot\
 -v /var/lib:/var/lib\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl snapshot restore /etcd_snapshot/${SNAPSHOT}\
 --data-dir /var/lib/etcd\
 --name ${NODE_NAME}\
 --initial-cluster ${NODE_NAME}=https://${NODE_IP}:2380\
 --initial-advertise-peer-urls https://${NODE_IP}:2380"</pre></div></li></ul></div></li><li class="listitem "><p>Start <code class="literal">etcd</code> on <code class="literal">master-0</code>.</p><div class="verbatim-wrap"><pre class="screen">mv /tmp/etcd.yaml /etc/kubernetes/manifests/</pre></div></li><li class="listitem "><p>You should be able to see <code class="literal">master-0</code> joined to the <code class="literal">etcd</code> cluster member list.</p><p>Replace <code class="literal">&lt;ENDPOINT_IP&gt;</code> with <code class="literal">master-0</code> cluster node IP address.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">sudo ETCDCTL_API=3 etcdctl\
 --endpoints=https://127.0.0.1:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo docker run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo podman run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"</pre></div></li></ul></div></li><li class="listitem "><p>Add another master node to the etcd cluster member list.</p><p>Replace <code class="literal">&lt;NODE_NAME&gt;</code> with cluster node name,
for example: <code class="literal">skuba-master-1</code></p><p>Replace <code class="literal">&lt;ENDPOINT_IP&gt;</code> with <code class="literal">master-0</code> cluster node IP address.</p><p>Replace <code class="literal">&lt;NODE_IP&gt;</code> with cluster node IP address.</p><div id="id-1.14.4.4.4.2.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_IP&gt;</code> must be visible from inside the node.</p><div class="verbatim-wrap highlight bash"><pre class="screen">ip addr | grep &lt;NODE_IP&gt;</pre></div></div><div id="id-1.14.4.4.4.2.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The <code class="literal">&lt;NODE_NAME&gt;</code> and <code class="literal">&lt;NODE_IP&gt;</code> must exist after <code class="literal">--initial-cluster</code> in <code class="literal">/etc/kubernetes/manifests/etcd.yaml</code></p></div><div id="id-1.14.4.4.4.2.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Nodes must be restored in sequence.</p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo ETCDCTL_API=3 etcdctl\
 --endpoints=https://127.0.0.1:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\
 member add ${NODE_NAME} --peer-urls=https://${NODE_IP}:2380</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo docker run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\
 member add ${NODE_NAME} --peer-urls=https://${NODE_IP}:2380"</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`
ENDPOINT=&lt;ENDPOINT_IP&gt;
NODE_NAME="&lt;NODE_NAME&gt;"
NODE_IP="&lt;NODE_IP&gt;"

sudo podman run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\
 member add ${NODE_NAME} --peer-urls=https://${NODE_IP}:2380"</pre></div></li></ul></div></li><li class="listitem "><p>Login to the node in step 7 via SSH.</p></li><li class="listitem "><p>Start <code class="literal">etcd</code>.</p><div class="verbatim-wrap"><pre class="screen">cp /tmp/etcd.yaml /etc/kubernetes/manifests/</pre></div></li><li class="listitem "><p>Repeat step 7, 8, 9 to recover all remaining master nodes.</p></li></ol></div></div><div class="sect3" id="_confirming_the_restoration"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Confirming the Restoration</span> <a title="Permalink" class="permalink" href="#_confirming_the_restoration">#</a></h4></div></div></div><p>After restoring, execute the below command to confirm the procedure. A successful restoration will show master nodes in <code class="literal">etcd</code> member list <code class="literal">started</code>, and all Kubernetes nodes in <code class="literal">STATUS Ready</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Etcdctl:</p><div class="verbatim-wrap"><pre class="screen">sudo ETCDCTL_API=3 etcdctl\
 --endpoints=https://127.0.0.1:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list

# EXAMPLE
116c1458aef748bc, started, caasp-master-cluster-2, https://172.28.0.20:2380, https://172.28.0.20:2379
3d124d6ad11cf3dd, started, caasp-master-cluster-0, https://172.28.0.26:2380, https://172.28.0.26:2379
43d2c8b1d5179c01, started, caasp-master-cluster-1, https://172.28.0.6:2380, https://172.28.0.6:2379</pre></div></li><li class="listitem "><p>Docker:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

# Replace &lt;ENDPOINT_IP&gt; with `master-0` cluster node IP address.
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo docker run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"

# EXAMPLE
116c1458aef748bc, started, caasp-master-cluster-2, https://172.28.0.20:2380, https://172.28.0.20:2379
3d124d6ad11cf3dd, started, caasp-master-cluster-0, https://172.28.0.26:2380, https://172.28.0.26:2379
43d2c8b1d5179c01, started, caasp-master-cluster-1, https://172.28.0.6:2380, https://172.28.0.6:2379</pre></div></li><li class="listitem "><p>Podman:</p><div class="verbatim-wrap"><pre class="screen">ETCD_IMAGE=`grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'`

# Replace &lt;ENDPOINT_IP&gt; with `master-0` cluster node IP address.
ENDPOINT=&lt;ENDPOINT_IP&gt;

sudo podman run\
 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd\
 --network host\
 --entrypoint "" ${ETCD_IMAGE} /bin/bash -c "\
ETCDCTL_API=3 etcdctl\
 --endpoints=https://${ENDPOINT}:2379\
 --cacert=/etc/kubernetes/pki/etcd/ca.crt\
 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt\
 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list"

# EXAMPLE
116c1458aef748bc, started, caasp-master-cluster-2, https://172.28.0.20:2380, https://172.28.0.20:2379
3d124d6ad11cf3dd, started, caasp-master-cluster-0, https://172.28.0.26:2380, https://172.28.0.26:2379
43d2c8b1d5179c01, started, caasp-master-cluster-1, https://172.28.0.6:2380, https://172.28.0.6:2379</pre></div></li><li class="listitem "><p>Kubectl:</p><div class="verbatim-wrap"><pre class="screen">kubectl get nodes

# EXAMPLE
NAME                          STATUS   ROLES    AGE      VERSION
caasp-master-cluster-0        Ready    master   28m      v1.16.2
caasp-master-cluster-1        Ready    master   20m      v1.16.2
caasp-master-cluster-2        Ready    master   12m      v1.16.2
caasp-worker-cluster-0        Ready    &lt;none&gt;   36m36s   v1.16.2</pre></div></li></ul></div></div></div></div></div><div class="chapter " id="backup-and-restore-with-velero"><div class="titlepage"><div><div><h1 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup and Restore with Velero</span> <a title="Permalink" class="permalink" href="#backup-and-restore-with-velero">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_limitations_2"><span class="number">13.1 </span><span class="name">Limitations</span></a></span></dt><dt><span class="section"><a href="#_prerequisites_8"><span class="number">13.2 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#_known_issues"><span class="number">13.3 </span><span class="name">Known Issues</span></a></span></dt><dt><span class="section"><a href="#_deployment_2"><span class="number">13.4 </span><span class="name">Deployment</span></a></span></dt><dt><span class="section"><a href="#_operations"><span class="number">13.5 </span><span class="name">Operations</span></a></span></dt><dt><span class="section"><a href="#_backup"><span class="number">13.6 </span><span class="name">Backup</span></a></span></dt><dt><span class="section"><a href="#_restore"><span class="number">13.7 </span><span class="name">Restore</span></a></span></dt><dt><span class="section"><a href="#_use_cases"><span class="number">13.8 </span><span class="name">Use Cases</span></a></span></dt><dt><span class="section"><a href="#_uninstall"><span class="number">13.9 </span><span class="name">Uninstall</span></a></span></dt></dl></div></div><p><a class="link" href="https://velero.io/" target="_blank">Velero</a> is a solution for supporting Kubernetes cluster disaster recovery,
data migration, and data protection by backing up Kubernetes cluster resources and persistent volumes to externally supported storage backend on-demand or by schedule.</p><p>The major functions include:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Backup Kubernetes resources and persistent volumes for supported storage providers.</p></li><li class="listitem "><p>Restore Kubernetes resources and persistent volumes for supported storage providers.</p></li><li class="listitem "><p>When backing up persistent volumes w/o supported storage provider, Velero leverages <a class="link" href="https://github.com/restic/restic" target="_blank">restic</a> as an agnostic solution to back up this sort of persistent volumes under some known limitations.</p></li></ul></div><p>User can leverage these fundamental functions to achieve user stories:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Backup whole Kubernetes cluster resources then restore if any Kubernetes resources loss.</p></li><li class="listitem "><p>Backup selected Kubernetes resources then restore if the selected Kubernetes resources loss.</p></li><li class="listitem "><p>Backup selected Kubernetes resources and persistent volumes then restore if the Kubernetes selected Kubernetes resources loss or data loss.</p></li><li class="listitem "><p>Replicate or migrate a cluster for any purpose, for example replicating a production cluster to a development cluster for testing.</p></li></ul></div><p>Velero consists of below components:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>A Velero server that runs on your Kubernetes cluster.</p></li><li class="listitem "><p>A <code class="literal">restic</code> deployed on each worker nodes that run on your Kubernetes cluster (optional).</p></li><li class="listitem "><p>A command-line client that runs locally.</p></li></ul></div><div class="sect1" id="_limitations_2"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#_limitations_2">#</a></h2></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Velero doesn’t overwrite objects in-cluster if they already exist.</p></li><li class="listitem "><p>Velero supports a single set of credentials <span class="emphasis"><em>per provider</em></span>.
It’s not yet possible to use different credentials for different object storage locations for the same provider.</p></li><li class="listitem "><p>Volume snapshots are limited by where your provider allows you to create snapshots.
For example, AWS and Azure do not allow you to create a volume snapshot in a different region than where the volume is located.
If you try to take a Velero backup using a volume snapshot location with a different region than where your cluster’s volume is, the backup will fail.</p></li><li class="listitem "><p>It is not yet possible to send a single Velero backup to multiple backup storage locations simultaneously, or a single volume snapshot to multiple locations simultaneously.
However, you can set up multiple backups manually or scheduled that differ only in the storage locations.</p></li><li class="listitem "><p>Cross-provider snapshots are not supported. If you have a cluster with more than one type of volume (e.g. NFS and Ceph), but you only have a volume snapshot location configured for NFS, then Velero will <span class="emphasis"><em>only</em></span> snapshot the NFS volumes.</p></li><li class="listitem "><p><code class="literal">Restic</code> data is stored under a prefix/subdirectory of the main Velero bucket and will go into the bucket corresponding backup storage location selected by the user at backup creation time.</p></li><li class="listitem "><p>When performing cluster migration, the new cluster number of nodes should be equal or greater than the original cluster.</p></li></ol></div><p>For more information about storage and snapshot locations, refer to <a class="link" href="https://velero.io/docs/v1.4/locations/" target="_blank">Velero: Backup Storage Locations and Volume Snapshot Locations</a></p></div><div class="sect1" id="_prerequisites_8"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#_prerequisites_8">#</a></h2></div></div></div><div class="sect2" id="_helm_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Helm</span> <a title="Permalink" class="permalink" href="#_helm_2">#</a></h3></div></div></div><p>To successfully use Velero to backup and restore the Kubernetes cluster, you first need to install Helm and Tiller.
Refer to <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>.</p><p>Add SUSE helm chart repository URL:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm repo add suse https://kubernetes-charts.suse.com</pre></div></div><div class="sect2" id="_object_storage_and_its_credentials"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage And Its Credentials</span> <a title="Permalink" class="permalink" href="#_object_storage_and_its_credentials">#</a></h3></div></div></div><p>Velero uses object storage to store backups and associated artifacts.
It can also optionally create snapshots of persistent volumes and store them in object storage via <code class="literal">restic</code> if there is no supported volume snapshot provider.</p><p>Choose one of the object storage providers, which fits your environment, from the list below for backing up and restoring the Kubernetes cluster.</p><p>The object storage server checks access permission, so it is vital to have credentials ready. Provide the credentials file <code class="literal">credentials-velero</code> to the velero server, so that it has the permission to write or read the backup data from the object storage.</p><div id="id-1.15.10.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Make sure the object storage is created before you install Velero. Otherwise, the Velero server won’t be able to start successfully. This is because the Velero server checks that the object storage exists and needs to have the permission to access it during server boot.</p></div><div class="sect3" id="_public_cloud_providers"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Public Cloud Providers</span> <a title="Permalink" class="permalink" href="#_public_cloud_providers">#</a></h4></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Provider</th><th align="left" valign="top">Object Storage</th><th align="left" valign="top">Plugin Provider Repo</th></tr></thead><tbody><tr><td align="left" valign="top"><p>Amazon Web Services (AWS)</p></td><td align="left" valign="top"><p>AWS S3</p></td><td align="left" valign="top"><p><a class="link" href="https://github.com/vmware-tanzu/velero-plugin-for-aws" target="_blank">Velero plugin for AWS</a></p></td></tr><tr><td align="left" valign="top"><p>Google Cloud Platform (GCP)</p></td><td align="left" valign="top"><p>Google Cloud Storage</p></td><td align="left" valign="top"><p><a class="link" href="https://github.com/vmware-tanzu/velero-plugin-for-gcp" target="_blank">Velero plugin for GCP</a></p></td></tr><tr><td align="left" valign="top"><p>Microsoft Azure</p></td><td align="left" valign="top"><p>Azure Blob Storage</p></td><td align="left" valign="top"><p><a class="link" href="https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure" target="_blank">Velero plugin for Microsoft Azure</a></p></td></tr></tbody></table></div><div class="sect4" id="_aws_s3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">AWS S3</span> <a title="Permalink" class="permalink" href="#_aws_s3">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>AWS CLI</p><p>Install <code class="literal">aws</code> CLI locally, follow the <a class="link" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html" target="_blank">doc</a> to install.</p></li><li class="listitem "><p>AWS S3 bucket</p><p>Create an AWS S3 bucket to store backup data and restore data from the S3 bucket.</p><div class="verbatim-wrap highlight bash"><pre class="screen">aws s3api create-bucket \
    --bucket &lt;BUCKET_NAME&gt; \
    --region &lt;REGION&gt; \
    --create-bucket-configuration LocationConstraint=&lt;REGION&gt;</pre></div></li><li class="listitem "><p>Create the credential file <code class="literal">credentials-velero</code> in the local machine</p><div class="verbatim-wrap"><pre class="screen">[default]
aws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;
aws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;</pre></div><p>For details, please refer to <a class="link" href="https://github.com/vmware-tanzu/velero-plugin-for-aws/tree/v1.1.0" target="_blank">Velero Plugin For AWS</a>.</p></li></ol></div></div><div class="sect4" id="_google_cloud_storage"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Google Cloud Storage</span> <a title="Permalink" class="permalink" href="#_google_cloud_storage">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>GCP CLIs</p><p>Install <code class="literal">gcloud</code> and <code class="literal">gsutil</code> CLIs locally, follow the <a class="link" href="https://cloud.google.com/sdk/docs/" target="_blank">doc</a> to install.</p></li><li class="listitem "><p>Create GCS bucket</p><div class="verbatim-wrap highlight bash"><pre class="screen">gsutil mb gs://&lt;BUCKET_NAME&gt;/</pre></div></li><li class="listitem "><p>Create the service account</p><div class="verbatim-wrap highlight bash"><pre class="screen"># View current config settings
gcloud config list

# Store the project value to PROJECT_ID environment variable
PROJECT_ID=$(gcloud config get-value project)

# Create a service account
gcloud iam service-accounts create velero \
    --display-name "Velero service account"

# List all accounts
gcloud iam service-accounts list

# Set the SERVICE_ACCOUNT_EMAIL environment variable
SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list \
  --filter="displayName:Velero service account" \
  --format 'value(email)')

# Attach policies to give velero the necessary permissions
ROLE_PERMISSIONS=(
    compute.disks.get
    compute.disks.create
    compute.disks.createSnapshot
    compute.snapshots.get
    compute.snapshots.create
    compute.snapshots.useReadOnly
    compute.snapshots.delete
    compute.zones.get
)

# Create iam roles
gcloud iam roles create velero.server \
    --project $PROJECT_ID \
    --title "Velero Server" \
    --permissions "$(IFS=","; echo "${ROLE_PERMISSIONS[*]}")"

# Bind iam policy to project
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member serviceAccount:$SERVICE_ACCOUNT_EMAIL \
    --role projects/$PROJECT_ID/roles/velero.server

gsutil iam ch serviceAccount:$SERVICE_ACCOUNT_EMAIL:objectAdmin gs://&lt;BUCKET_NAME&gt;</pre></div></li><li class="listitem "><p>Create the credential file <code class="literal">credentials-velero</code> in the local machine</p><div class="verbatim-wrap highlight bash"><pre class="screen">gcloud iam service-accounts keys create credentials-velero \
    --iam-account $SERVICE_ACCOUNT_EMAIL</pre></div><p>For details, please refer to <a class="link" href="https://github.com/vmware-tanzu/velero-plugin-for-gcp/tree/v1.1.0" target="_blank">Velero Plugin For GCP</a>.</p></li></ol></div></div><div class="sect4" id="_azure_blob_storage"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Azure Blob Storage</span> <a title="Permalink" class="permalink" href="#_azure_blob_storage">#</a></h5></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Azure CLI</p><p>Install <code class="literal">az</code> CLI locally, follow the <a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli" target="_blank">doc</a> to install.</p></li><li class="listitem "><p>Create a resource group for the backups storage account</p><p>Create the resource group named Velero_Backups, change the resource group name and location as needed.</p><div class="verbatim-wrap highlight bash"><pre class="screen">AZURE_RESOURCE_GROUP=Velero_Backups
az group create -n $AZURE_RESOURCE_GROUP --location &lt;location&gt;</pre></div></li><li class="listitem "><p>Create the storage account</p><div class="verbatim-wrap highlight bash"><pre class="screen">az storage account create \
    --name $AZURE_STORAGE_ACCOUNT_ID \
    --resource-group $AZURE_RESOURCE_GROUP \
    --sku Standard_GRS \
    --encryption-services blob \
    --https-only true \
    --kind BlobStorage \
    --access-tier Hot</pre></div></li><li class="listitem "><p>Create a blob container</p><p>Create a blob container named velero. Change the name as needed.</p><div class="verbatim-wrap highlight bash"><pre class="screen">BLOB_CONTAINER=velero
az storage container create -n $BLOB_CONTAINER --public-access off --account-name $AZURE_STORAGE_ACCOUNT_ID</pre></div></li><li class="listitem "><p>Create the credential file <code class="literal">credentials-velero</code> in the local machine</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Obtain your Azure Account Subscription ID
AZURE_SUBSCRIPTION_ID=`az account list --query '[?isDefault].id' -o tsv`

# Obtain your Azure Account Tenant ID
AZURE_TENANT_ID=`az account list --query '[?isDefault].tenantId' -o tsv`

# Generate client secret
AZURE_CLIENT_SECRET=`az ad sp create-for-rbac --name "velero" --role "Contributor" --query 'password' -o tsv`

# Generate client ID
AZURE_CLIENT_ID=`az ad sp list --display-name "velero" --query '[0].appId' -o tsv`

cat &lt;&lt; EOF  &gt; ./credentials-velero
AZURE_SUBSCRIPTION_ID=${AZURE_SUBSCRIPTION_ID}
AZURE_TENANT_ID=${AZURE_TENANT_ID}
AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
AZURE_RESOURCE_GROUP=${AZURE_RESOURCE_GROUP}
EOF</pre></div><p>For details, please refer to <a class="link" href="https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/tree/v1.1.0" target="_blank">Velero Plugin For Azure</a>.</p></li></ol></div></div></div><div class="sect3" id="_on_premise_s3_compatible_providers"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">On-Premise (S3-Compatible Providers)</span> <a title="Permalink" class="permalink" href="#_on_premise_s3_compatible_providers">#</a></h4></div></div></div><div class="sect4" id="_suse_enterprise_storage_6_ceph_object_gateway_radosgw"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage 6 Ceph Object Gateway (<code class="literal">radosgw</code>)</span> <a title="Permalink" class="permalink" href="#_suse_enterprise_storage_6_ceph_object_gateway_radosgw">#</a></h5></div></div></div><p>SUSE supports the SUSE Enterprise Storage 6 Ceph Object Gateway (<code class="literal">radosgw</code>) as an S3-compatible object storage provider.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Installation
Refer to the <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-additional-software-installation.html" target="_blank">SES 6 Object Gateway Manual Installation</a> on how to install it.</p></li><li class="listitem "><p>Create the credential file <code class="literal">credentials-velero</code> in the local machine</p><div class="verbatim-wrap"><pre class="screen">[default]
aws_access_key_id=&lt;SES_STORAGE_ACCESS_KEY_ID&gt;
aws_secret_access_key=&lt;SES_STORAGE_SECRET_ACCESS_KEY&gt;</pre></div></li></ol></div></div><div class="sect4" id="_minio"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Minio</span> <a title="Permalink" class="permalink" href="#_minio">#</a></h5></div></div></div><p>Besides SUSE Enterprise Storage, there is an alternative open-source S3-compatible object storage provider <a class="link" href="https://min.io/" target="_blank">minio</a>.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Prepare an external host and install Minio on the host</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Download Minio server
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio

# Expose Minio access_key and secret_key
export MINIO_ACCESS_KEY=&lt;access_key&gt;
export MINIO_SECRET_KEY=&lt;secret_key&gt;

# Start Minio server
mkdir -p bucket
./minio server bucket &amp;

# Download Minio client
wget https://dl.min.io/client/mc/release/linux-amd64/mc
chmod +x mc

# Setup Minio server
./mc config host add Velero http://localhost:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

# Create bucket on Minio server
./mc mb -p velero/velero</pre></div></li><li class="listitem "><p>Create the credential file <code class="literal">credentials-velero</code> in the local machine</p><div class="verbatim-wrap"><pre class="screen">[default]
aws_access_key_id=&lt;MINIO_STORAGE_ACCESS_KEY_ID&gt;
aws_secret_access_key=&lt;MINIO_STORAGE_SECRET_ACCESS_KEY&gt;</pre></div><p>For the rest of the S3-compatible storage providers, refer to <a class="link" href="https://velero.io/docs/v1.4/supported-providers/" target="_blank">Velero: Supported Providers</a>.</p></li></ol></div></div></div></div><div class="sect2" id="_volume_snapshotter"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Volume Snapshotter</span> <a title="Permalink" class="permalink" href="#_volume_snapshotter">#</a></h3></div></div></div><p>A volume snapshotter can snapshot its persistent volumes if its volume driver supports volume snapshot and corresponding API.</p><p>If a volume provider does not support volume snapshot or volume snapshot API or does not have Velero supported storage plugin, Velero leverages <code class="literal">restic</code> as an agnostic solution to backup and restore this sort of persistent volumes.</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Provider</th><th align="left" valign="top">Volume Snapshotter</th><th align="left" valign="top">Plugin Provider Repo</th></tr></thead><tbody><tr><td align="left" valign="top"><p>Amazon Web Services (AWS)</p></td><td align="left" valign="top"><p>AWS EBS</p></td><td align="left" valign="top"><p><a class="link" href="https://github.com/vmware-tanzu/velero-plugin-for-aws" target="_blank">Velero plugin for AWS</a></p></td></tr></tbody></table></div><p>For the other <code class="literal">snapshotter</code> providers refer to <a class="link" href="https://velero.io/docs/v1.4/supported-providers/" target="_blank">Velero: Supported Providers</a>.</p></div><div class="sect2" id="_velero_cli"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Velero CLI</span> <a title="Permalink" class="permalink" href="#_velero_cli">#</a></h3></div></div></div><p>Install Velero CLI to interact with Velero server.</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo zypper install velero</pre></div></div></div><div class="sect1" id="_known_issues"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Known Issues</span> <a title="Permalink" class="permalink" href="#_known_issues">#</a></h2></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>When restoring <code class="literal">dex</code> and <code class="literal">gangway</code>, Velero reports <code class="literal">NodePort</code> cannot be restored since <code class="literal">dex</code> and <code class="literal">gangway</code> are deployed by an addon already and the same <code class="literal">NodePort</code> has been registered.
However, this does not break the <code class="literal">dex</code> and <code class="literal">gangway</code> service access from outside.</p><div id="id-1.15.11.2.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>You can add a label to services <code class="literal">oidc-dex</code> and <code class="literal">oidc-gangway</code> to skip Velero backup.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl label -n kube-system services/oidc-dex velero.io/exclude-from-backup=true

kubectl label -n kube-system services/oidc-gangway velero.io/exclude-from-backup=true</pre></div></div></li></ol></div></div><div class="sect1" id="_deployment_2"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment</span> <a title="Permalink" class="permalink" href="#_deployment_2">#</a></h2></div></div></div><p>Use Helm CLI to install Velero deployment and <code class="literal">restic</code> (<span class="emphasis"><em>optional</em></span>) if the storage does not provide volume snapshot API.</p><div id="id-1.15.12.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>If Velero installed other than default namespace <code class="literal">velero</code>, setup velero config to the Velero installed namespace.</p><div class="verbatim-wrap"><pre class="screen">velero client config set namespace=&lt;NAMESPACE&gt;</pre></div></div><div id="id-1.15.12.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>For troubleshooting a velero deployment, refer to <a class="link" href="https://velero.io/docs/v1.4/debugging-install/" target="_blank">Velero: Debugging Installation Issues</a></p></div><div class="sect2" id="_backup_kubernetes_cluster_objects_only"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup Kubernetes Cluster Objects Only</span> <a title="Permalink" class="permalink" href="#_backup_kubernetes_cluster_objects_only">#</a></h3></div></div></div><p>For the cases that the Kubernetes cluster do not use external storage <span class="emphasis"><em>or</em></span> the external storage would handle take volume snapshot by itself, it does not need Velero to backup persistent volume.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Backup To A Public Cloud Provider</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Amazon Web Services (AWS)</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in AWS S3 object storage)</p></li><li class="listitem "><p>The backup region name <span class="emphasis"><em>REGION_NAME</em></span>. (The region name for the AWS S3 object storage. For example, <code class="literal">us-east-1</code> for AWS US East (N. Virginia))</p></li><li class="listitem "><p>The Velero installed namespace <span class="emphasis"><em>NAMESPACE</em></span>, the default namespace is <code class="literal">velero</code>. (optional)</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup locations point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=aws \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt; \
    --region=&lt;REGION_NAME&gt;</pre></div></li></ol></div></li><li class="listitem "><p>Google Cloud Platform (GCP)</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in Google Cloud Storage object storage)</p></li><li class="listitem "><p>The Velero installed namespace <span class="emphasis"><em>NAMESPACE</em></span>, the default namespace is <code class="literal">velero</code>. (optional)</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=gcp \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-gcp \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-gcp:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=gcp \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-gcp \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-gcp:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup locations point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=gcp \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt;</pre></div></li></ol></div></li><li class="listitem "><p>Microsoft Azure</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in Azure Blob Storage	 object storage)</p></li><li class="listitem "><p>The resource group name <span class="emphasis"><em>AZURE_RESOURCE_GROUP</em></span>. (The Azure resource group name)</p></li><li class="listitem "><p>The storage account ID <span class="emphasis"><em>AZURE_STORAGE_ACCOUNT_ID</em></span>. (The Azure storage account ID)</p></li><li class="listitem "><p>The Velero installed namespace <span class="emphasis"><em>NAMESPACE</em></span>, the default namespace is <code class="literal">velero</code>. (optional)</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=azure \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.resourceGroup=&lt;AZURE_RESOURCE_GROUP&gt; \
	--set configuration.backupStorageLocation.config.storageAccount=&lt;AZURE_STORAGE_ACCOUNT_ID&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-microsoft-azure \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-microsoft-azure:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=azure \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.resourceGroup=&lt;AZURE_RESOURCE_GROUP&gt; \
    --set configuration.backupStorageLocation.config.storageAccount=&lt;AZURE_STORAGE_ACCOUNT_ID&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-microsoft-azure \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-microsoft-azure:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup locations point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=azure \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt; \
    --region resourceGroup=&lt;AZURE_RESOURCE_GROUP&gt;,storageAccount=&lt;AZURE_STORAGE_ACCOUNT_ID&gt;</pre></div></li></ol></div></li></ul></div></li><li class="listitem "><p>Backup To A S3-Compatible Provider</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in S3-compatible object storage)</p></li><li class="listitem "><p>The backup region name <span class="emphasis"><em>REGION_NAME</em></span>. (The region name for the S3-compatible object storage. For example, radosgw <span class="emphasis"><em>or</em></span> default/secondary if you have an HA backup servers)</p></li><li class="listitem "><p>The S3-compatible object storage simulates the S3-compatible object storage. Therefore, the configuration for S3-compatible object storage have to setup additional configurations.</p><div class="verbatim-wrap highlight bash"><pre class="screen">--set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
--set configuration.backupStorageLocation.config.s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt; \</pre></div></li><li class="listitem "><p>If the S3-Compatible storage server is secured with a self-signed certificate, add the below command when helm install and pass <code class="literal">--cacert</code> flag when using Velero CLI, refer to <a class="link" href="https://velero.io/docs/v1.4/self-signed-certificates/" target="_blank">Velero: Self Signed Certificates</a>. (optional)</p><div class="verbatim-wrap highlight bash"><pre class="screen">--set configuration.backupStorageLocation.caCert=`cat &lt;PATH_TO_THE_SELF_SIGNED_CA_CERTIFICATE&gt; | base64 -w 0 &amp;&amp; echo` \</pre></div></li><li class="listitem "><p>Install Velero Deployment.</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
    --set configuration.backupStorageLocation.config.s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
    --set configuration.backupStorageLocation.config.s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt; \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup location point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=aws \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt; \
    --config region=secondary,s3ForcePathStyle=true,s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt;</pre></div></li></ol></div></li></ul></div></div><div class="sect2" id="_backup_kubernetes_cluster"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup Kubernetes Cluster</span> <a title="Permalink" class="permalink" href="#_backup_kubernetes_cluster">#</a></h3></div></div></div><p>For the case that the Kubernetes cluster uses external storage <span class="emphasis"><em>and</em></span> the external storage would not handle volume snapshot by itself (either external storage does not support volume snapshot <span class="emphasis"><em>or</em></span> administrator want use velero to take volume snapshot when velero do cluster backup).</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Backup To A Public Cloud Provider</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Amazon Web Services (AWS)</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in AWS S3 object storage)</p></li><li class="listitem "><p>The backup region name <span class="emphasis"><em>REGION_NAME</em></span>. (The region name for the AWS S3 object storage. For example, <code class="literal">us-east-1</code> for AWS US East (N. Virginia))</p></li><li class="listitem "><p>The Velero installed namespace <span class="emphasis"><em>NAMESPACE</em></span>, the default namespace is <code class="literal">velero</code>. (optional)</p><div id="id-1.15.12.6.3.1.2.1.2.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If the Kubernetes cluster in AWS and uses AWS EBS as storage, please remove the</p><div class="verbatim-wrap"><pre class="screen">--set deployRestic=true \</pre></div><p>at below to use AWS EBS volume snapshot API to take volume snapshot.
Otherwise, it would install restic and velero server will use restic to take a volume snapshot and the volume data will store to AWS S3 bucket.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set configuration.volumeSnapshotLocation.config.region=&lt;REGION_NAME&gt; \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set configuration.volumeSnapshotLocation.config.region=&lt;REGION_NAME&gt; \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup locations point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=aws \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt; \
    --config region=&lt;REGION_NAME&gt;</pre></div></li></ol></div></li><li class="listitem "><p>Google Cloud Platform (GCP)</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in Google Cloud Storage object storage)</p></li><li class="listitem "><p>The Velero installed namespace <span class="emphasis"><em>NAMESPACE</em></span>, the default namespace is <code class="literal">velero</code>. (optional)</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=gcp \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set initContainers[0].name=velero-plugin-for-gcp \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-gcp:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=gcp \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set initContainers[0].name=velero-plugin-for-gcp \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-gcp:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup locations point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=gcp \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt;</pre></div></li></ol></div></li><li class="listitem "><p>Microsoft Azure</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in Azure Blob Storage object storage)</p></li><li class="listitem "><p>The resource group name <span class="emphasis"><em>AZURE_RESOURCE_GROUP</em></span>. (The Azure resource group name)</p></li><li class="listitem "><p>The storage account ID <span class="emphasis"><em>AZURE_STORAGE_ACCOUNT_ID</em></span>. (The Azure storage account ID)</p></li><li class="listitem "><p>The Velero installed namespace <span class="emphasis"><em>NAMESPACE</em></span>, the default namespace is <code class="literal">velero</code>. (optional)</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=azure \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.resourceGroup=&lt;AZURE_RESOURCE_GROUP&gt; \
	--set configuration.backupStorageLocation.config.storageAccount=&lt;AZURE_STORAGE_ACCOUNT_ID&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set initContainers[0].name=velero-plugin-for-microsoft-azure \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-microsoft-azure:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=azure \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.resourceGroup=&lt;AZURE_RESOURCE_GROUP&gt; \
    --set configuration.backupStorageLocation.config.storageAccount=&lt;AZURE_STORAGE_ACCOUNT_ID&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set initContainers[0].name=velero-plugin-for-microsoft-azure \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-microsoft-azure:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup locations point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=azure \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt; \
    --region resourceGroup=&lt;AZURE_RESOURCE_GROUP&gt;,storageAccount=&lt;AZURE_STORAGE_ACCOUNT_ID&gt;</pre></div></li></ol></div></li></ul></div></li><li class="listitem "><p>Backup To A S3-Compatible Provider</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>The backup bucket name <span class="emphasis"><em>BUCKET_NAME</em></span>. (The bucket name in S3-compatible object storage)</p></li><li class="listitem "><p>The backup region name <span class="emphasis"><em>REGION_NAME</em></span>. (The region name for the S3-compatible object storage. For example, radosgw <span class="emphasis"><em>or</em></span> default/secondary if you have an HA backup servers)</p></li><li class="listitem "><p>The S3-compatible object storage simulates the S3-compatible object storage. Therefore, the configuration for S3-compatible object storage have to setup additional configurations.</p><div class="verbatim-wrap highlight bash"><pre class="screen">--set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
--set configuration.backupStorageLocation.config.s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt; \</pre></div></li><li class="listitem "><p>If the S3-Compatible storage server is secured with a self-signed certificate, add the below command when helm install and pass <code class="literal">--cacert</code> flag when using Velero CLI, refer to <a class="link" href="https://velero.io/docs/v1.4/self-signed-certificates/" target="_blank">Velero: Self Signed Certificates</a>. (optional)</p><div class="verbatim-wrap highlight bash"><pre class="screen">--set configuration.backupStorageLocation.caCert=`cat &lt;PATH_TO_THE_SELF_SIGNED_CA_CERTIFICATE&gt; | base64 -w 0 &amp;&amp; echo` \</pre></div></li><li class="listitem "><p>Install Velero Deployment and restic DaemonSet.</p><div id="id-1.15.12.6.3.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Mostly the on-premise persistent volume does not support volume snapshot API or does not have community-supported snapshotter providers. Therefore, we <span class="emphasis"><em>have to</em></span> deploy the <code class="literal">restic</code> DaemonSet.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
    --name=velero \
    --namespace=&lt;NAMESPACE&gt; \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
    --set configuration.backupStorageLocation.config.s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set configuration.volumeSnapshotLocation.config.region=&lt;REGION_NAME&gt; \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install velero \
    --namespace=&lt;NAMESPACE&gt; \
    --create-namespace \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.bucket=&lt;BUCKET_NAME&gt; \
    --set configuration.backupStorageLocation.config.region=&lt;REGION_NAME&gt; \
    --set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
    --set configuration.backupStorageLocation.config.s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt; \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set configuration.volumeSnapshotLocation.config.region=minio \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4.5/velero-plugin-for-aws:1.1.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero</pre></div></li><li class="listitem "><p>Then, suggest creating at least one additional backup locations point to the different object storage server to prevent object storage server single point of failure.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location create secondary \
    --provider=aws \
    --bucket=&lt;SECONDARY_BUCKET_NAME&gt; \
    --config region=secondary,s3ForcePathStyle=true,s3Url=&lt;S3_COMPATIBLE_STORAGE_SERVER_URL&gt;</pre></div></li></ol></div></li></ul></div></div></div><div class="sect1" id="_operations"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations</span> <a title="Permalink" class="permalink" href="#_operations">#</a></h2></div></div></div></div><div class="sect1" id="_backup"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup</span> <a title="Permalink" class="permalink" href="#_backup">#</a></h2></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Annotate Persistent Volume (<span class="emphasis"><em>optional</em></span>)</p><p>If the persistent volume in the supported volume <code class="literal">snapshotter</code> provider, skip this procedure.</p><p>However, if we deploy the <code class="literal">restic</code> DaemonSet and want to backup the persistent volume by <code class="literal">restic</code>, we have to add annotation <code class="literal">backup.velero.io/backup-volumes=&lt;VOLUME_NAME_1&gt;,&lt;VOLUME_NAME_2&gt;,…​</code> to the pods which have mounted the volume manually.</p><p>For example, we deploy an Elasticsearch cluster and want to backup the Elasticsearch cluster’s data. Add the annotation to the Elasticsearch cluster pods:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl annotate pod/elasticsearch-master-0 backup.velero.io/backup-volumes=elasticsearch-master
kubectl annotate pod/elasticsearch-master-1 backup.velero.io/backup-volumes=elasticsearch-master
kubectl annotate pod/elasticsearch-master-2 backup.velero.io/backup-volumes=elasticsearch-master</pre></div><div id="id-1.15.14.2.1.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Velero currently does not provide a mechanism to detect persistent volume claims that are missing the <code class="literal">restic</code> backup annotation.
To solve this, there is a community provided controller <a class="link" href="https://github.com/bitsbeats/velero-pvc-watcher" target="_blank">velero-pvc-watcher</a> which integrates Prometheus to generate alerts for volumes that are not in the backup or backup-exclusion annotation.</p></div></li><li class="listitem "><p>Manual Backup</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup create &lt;BACKUP_NAME&gt;</pre></div></li><li class="listitem "><p>Scheduled Backup</p><p>The schedule template in cron notation, using UTC time. The schedule can also be expressed using <code class="literal">@every &lt;duration&gt;</code> syntax.
The duration can be specified using a combination of seconds (s), minutes (m), and hours (h), for example: <code class="literal">@every 2h30m</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create schedule template
# Create a backup every 6 hours
velero schedule create &lt;SCHEDULE_NAME&gt; --schedule="0 */6 * * *"

# Create a backup every 6 hours with the @every notation
velero schedule create &lt;SCHEDULE_NAME&gt; --schedule="@every 6h"

# Create a daily backup of the web namespace
velero schedule create &lt;SCHEDULE_NAME&gt; --schedule="@every 24h" --include-namespaces web

# Create a weekly backup, each living for 90 days (2160 hours)
velero schedule create &lt;SCHEDULE_NAME&gt; --schedule="@every 168h" --ttl 2160h0m0s</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Character Position</th><th align="left" valign="top">Character Period</th><th align="left" valign="top">Acceptable Values</th></tr></thead><tbody><tr><td align="left" valign="top"><p>1</p></td><td align="left" valign="top"><p>Minute</p></td><td align="left" valign="top"><p><code class="literal">0-59,*</code></p></td></tr><tr><td align="left" valign="top"><p>2</p></td><td align="left" valign="top"><p>Hour</p></td><td align="left" valign="top"><p><code class="literal">0-23,*</code></p></td></tr><tr><td align="left" valign="top"><p>3</p></td><td align="left" valign="top"><p>Day of Month</p></td><td align="left" valign="top"><p><code class="literal">1-31,*</code></p></td></tr><tr><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>Month</p></td><td align="left" valign="top"><p><code class="literal">1-12,*</code></p></td></tr><tr><td align="left" valign="top"><p>5</p></td><td align="left" valign="top"><p>Day of Week</p></td><td align="left" valign="top"><p><code class="literal">0-7,*</code></p></td></tr></tbody></table></div><div id="id-1.15.14.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>When creating multiple backups to different backup locations closely, you might hit the object storage server API rate limit issues. Now, the velero does not have a mechanism on retry backups when the rate limit occurred. Consider shifting the time to create multiple backups.</p></div></li><li class="listitem "><p>Optional Flags</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Granularity</p><p>Without passing extra flags to <code class="literal">velero backup create</code>, Velero will backup the whole Kubernetes cluster.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Namespace</p><p>Pass flag <code class="literal">--include-namespaces</code> or <code class="literal">--exclude-namespaces</code> to specify which namespaces to include/exclude when backing up.</p><p>For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create a backup including the nginx and default namespaces
velero backup create backup-1 --include-namespaces nginx,default

# Create a backup excluding the kube-system and default namespaces
velero backup create backup-1 --exclude-namespaces kube-system,default</pre></div></li><li class="listitem "><p>Resources</p><p>Pass flag <code class="literal">--include-resources</code> or <code class="literal">--exclude-resources</code> to specifies which resources to include/exclude when backing up.</p><p>For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create a backup including storageclass resource only
velero backup create backup-1 --include-resources storageclasses</pre></div><div id="id-1.15.14.2.4.2.1.3.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Use <code class="literal">kubectl api-resources</code> to lists all API resources on the server.</p></div></li><li class="listitem "><p>Label Selector</p><p>Pass <code class="literal">--selector</code> to only back up resources matching the label selector.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create a backup for the elasticsearch cluster only
velero backup create backup-1 --selector app=elasticsearch-master</pre></div></li></ul></div></li><li class="listitem "><p>Location</p><p>Pass <code class="literal">--storage-location</code> to specify where to store the backup.
For example, if we have an HA object storage server called default and secondary respectively.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create a backup to the default storage server
velero backup create backup2default --storage-location default

# Create a backup to the secondary storage server
velero backup create backup2secondary --storage-location secondary</pre></div></li><li class="listitem "><p>Garbage Collection</p><p>Pass <code class="literal">--ttl</code> to specify how long the backup should be kept. After the specified time the backup will be deleted.
The default time for a backup before deletion is 720 hours (30 days).</p></li><li class="listitem "><p>Exclude Specific Items from Backup</p><p>You can exclude individual items from being backed up, even if they match the resource/namespace/label selectors defined in the backup spec. To do this, label the item as follows:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl label -n &lt;ITEM_NAMESPACE&gt; &lt;RESOURCE&gt;/&lt;NAME&gt; velero.io/exclude-from-backup=true</pre></div></li></ul></div></li></ul></div><div class="sect2" id="_backup_troubleshooting"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup Troubleshooting</span> <a title="Permalink" class="permalink" href="#_backup_troubleshooting">#</a></h3></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>List Backups</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup get</pre></div></li><li class="listitem "><p>Describe Backups</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup describe &lt;BACKUP_NAME_1&gt; &lt;BACKUP_NAME_2&gt; &lt;BACKUP_NAME_3&gt;</pre></div></li><li class="listitem "><p>Retrieve Backup Logs</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup logs &lt;BACKUP_NAME&gt;</pre></div></li></ul></div></div></div><div class="sect1" id="_restore"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore</span> <a title="Permalink" class="permalink" href="#_restore">#</a></h2></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Manual Restore</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore create &lt;RESTORE_NAME&gt; --from-backup &lt;BACKUP_NAME&gt;</pre></div><p>For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create a restore named "restore-1" from backup "backup-1"
velero restore create restore-1 --from-backup backup-1

# Create a restore with a default name ("backup-1-&lt;timestamp&gt;") from backup "backup-1"
velero restore create --from-backup backup-1</pre></div></li><li class="listitem "><p>Scheduled Backup</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore create &lt;RESTORE_NAME&gt; --from-schedule &lt;SCHEDULE_NAME&gt;</pre></div><p>For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create a restore from the latest successful backup triggered by schedule "schedule-1"
velero restore create --from-schedule schedule-1

# Create a restore from the latest successful OR partially-failed backup triggered by schedule "schedule-1"
velero restore create --from-schedule schedule-1 --allow-partially-failed</pre></div></li><li class="listitem "><p>Optional Flags</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Granularity</p><p>Without passing extra flags to <code class="literal">velero restore create</code>, Velero will restore whole resources from the backup or the schedule.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Namespace</p><p>Pass flag <code class="literal">--include-namespaces</code> or <code class="literal">--exclude-namespaces</code> to <code class="literal">velero restore create</code> to specifies which namespaces to include/exclude when restoring.</p><p>For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Create a restore including the nginx and default namespaces
velero restore create --from-backup backup-1 --include-namespaces nginx,default

# Create a restore excluding the kube-system and default namespaces
velero restore create --from-backup backup-1 --exclude-namespaces kube-system,default</pre></div></li><li class="listitem "><p>Resources</p><p>Pass flag <code class="literal">--include-resources</code> or <code class="literal">--exclude-resources</code> to <code class="literal">velero restore create</code> to specifies which resources to include/exclude when restoring.</p><p>For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># create a restore for only persistentvolumeclaims and persistentvolumes within a backup
velero restore create --from-backup backup-1 --include-resources persistentvolumeclaims,persistentvolumes</pre></div><div id="id-1.15.15.2.3.2.1.3.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Use <code class="literal">kubectl api-resources</code> to lists all API resources on the server.</p></div></li><li class="listitem "><p>Label Selector</p><p>Pass <code class="literal">--selector</code> to only restore the resources matching the label selector.</p><p>For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># create a restore for only the elasticsearch cluster within a backup
velero restore create --from-backup backup-1 --selector app=elasticsearch-master</pre></div></li></ul></div></li></ul></div></li></ul></div><div class="sect2" id="_restore_troubleshooting"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore Troubleshooting</span> <a title="Permalink" class="permalink" href="#_restore_troubleshooting">#</a></h3></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Retrieve restores</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore get</pre></div></li><li class="listitem "><p>Describe restores</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore describe &lt;RESTORE_NAME_1&gt; &lt;RESTORE_NAME_2&gt; &lt;RESTORE_NAME_3&gt;</pre></div></li><li class="listitem "><p>Retrieve restore logs</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore logs &lt;RESTORE_NAME&gt;</pre></div></li></ul></div><div id="id-1.15.15.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>For troubleshooting velero restore, refer to <a class="link" href="https://velero.io/docs/v1.4/debugging-restores/" target="_blank">Velero: Debugging Restores</a></p></div></div></div><div class="sect1" id="_use_cases"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Cases</span> <a title="Permalink" class="permalink" href="#_use_cases">#</a></h2></div></div></div><div class="sect2" id="_disaster_recovery"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disaster Recovery</span> <a title="Permalink" class="permalink" href="#_disaster_recovery">#</a></h3></div></div></div><p>Use the scheduled backup function for periodical backups. When the Kubernetes cluster runs into an unexpected state, recover from the most recent scheduled backup.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Backup</p><p>Run the scheduled backup, this creates a backup file with the name <code class="literal">&lt;SCHEDULE_NAME&gt;-&lt;TIMESTAMP&gt;</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero schedule create &lt;SCHEDULE_NAME&gt; --schedule="@daily"</pre></div></li><li class="listitem "><p>Restore</p><p>When a disaster happens, make sure the Velero server and <code class="literal">restic</code> DaemonSet exists (<span class="emphasis"><em>optional</em></span>). If not, reinstall from the helm chart.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Update the backup storage location to <span class="emphasis"><em>read-only</em></span> mode (it prevents the backup file from being created or deleted in the backup storage location during the restore process):</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl patch backupstoragelocation &lt;STORAGE_LOCATION_NAME&gt; \
    --namespace &lt;NAMESPACE&gt; \
    --type merge \
    --patch '{"spec":{"accessMode":"ReadOnly"}}'</pre></div></li><li class="listitem "><p>Create a restore from the most recent backup file:</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore create --from-backup &lt;SCHEDULE_NAME&gt;-&lt;TIMESTAMP&gt;</pre></div></li><li class="listitem "><p>After restoring finished, change the backup storage location back to read-write mode:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl patch backupstoragelocation &lt;STORAGE_LOCATION_NAME&gt; \
    --namespace &lt;NAMESPACE&gt; \
    --type merge \
    --patch '{"spec":{"accessMode":"ReadWrite"}}'</pre></div></li></ol></div></li></ul></div></div><div class="sect2" id="_cluster_migration"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Migration</span> <a title="Permalink" class="permalink" href="#_cluster_migration">#</a></h3></div></div></div><p>Migrate the Kubernetes cluster from <code class="literal">cluster 1</code> to <code class="literal">cluster 2</code>, as long as you point different cluster’s Velero instances to the same external object storage location.</p><div id="id-1.15.16.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Velero does not support the migration of persistent volumes across public cloud providers.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>(At cluster 1) Backup the entire Kubernetes cluster manually:</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup create &lt;BACKUP_NAME&gt;</pre></div></li><li class="listitem "><p>(At cluster 2) Prepare a {cluster} cluster deployed by skuba:</p></li><li class="listitem "><p>(At cluster 2) Helm install Velero and make sure the backup-location and snapshot-location point to the same location as cluster 1:</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup-location get
velero snapshot-location get</pre></div><div id="id-1.15.16.3.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The default sync interval is 1 minute. You could change the interval with the flag <code class="literal">--backup-sync-period</code> when creating a backup location.</p></div></li><li class="listitem "><p>(At cluster 2) Make sure the cluster 1 backup resources are sync to the external object storage server:</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero backup get &lt;BACKUP_NAME&gt;
velero backup describe &lt;BACKUP_NAME&gt;</pre></div></li><li class="listitem "><p>(At cluster 2) Restore the cluster from the backup file:</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore create --from-backup &lt;BACKUP_NAME&gt;</pre></div></li><li class="listitem "><p>(At cluster 2) Verify the cluster is behaving correctly:</p><div class="verbatim-wrap highlight bash"><pre class="screen">velero restore get
velero restore describe &lt;RESTORE_NAME&gt;
velero restore logs &lt;RESTORE_NAME&gt;</pre></div></li><li class="listitem "><p>(At cluster 2) Since Velero doesn’t overwrite objects in-cluster if they already exist, a manual check of all addon configurations is desired after the cluster is restored:</p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>Check dex configuration:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Download dex.yaml
kubectl -n kube-system get configmap oidc-dex-config -o yaml &gt; oidc-dex-config.yaml

# Edit oidc-dex-config.yaml to desired
vim oidc-dex-config.yaml

# Apply new oidc-dex-config.yaml
kubectl apply -f oidc-dex-config.yaml --force

# Restart oidc-dex deployment
kubectl rollout restart deployment/oidc-dex -n kube-system</pre></div></li><li class="listitem "><p>Check gangway configuration:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Download gangway.yaml
kubectl -n kube-system get configmap oidc-gangway-config -o yaml &gt; oidc-gangway-config.yaml

# Edit oidc-gangway-config.yaml to desired
vim oidc-gangway-config.yaml

# Apply new oidc-gangway-config.yaml
kubectl apply -f oidc-gangway-config.yaml --force

# Restart oidc-gangway deployment
kubectl rollout restart deployment/oidc-gangway -n kube-system</pre></div></li><li class="listitem "><p>Check kured is disabled automatically reboots</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get daemonset kured -o yaml</pre></div></li><li class="listitem "><p>Check that psp is what you wish it to be:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get psp suse.caasp.psp.privileged -o yaml
kubectl get clusterrole suse:caasp:psp:privileged -o yaml
kubectl get rolebinding suse:caasp:psp:privileged -o yaml

kubectl get psp suse.caasp.psp.unprivileged -o yaml
kubectl get clusterrole suse:caasp:psp:unprivileged -o yaml
kubectl get clusterrolebinding suse:caasp:psp:default -o yaml</pre></div></li></ol></div></li></ol></div></div></div><div class="sect1" id="_uninstall"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Uninstall</span> <a title="Permalink" class="permalink" href="#_uninstall">#</a></h2></div></div></div><p>Remove the Velero server deployment and <code class="literal">restic</code> DaemonSet if it exist.
Then, delete Velero custom resource definitions (CRDs).</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm del --purge velero
kubectl delete crds -l app.kubernetes.io/name=velero</pre></div><p>Or if you have selected the Helm 3 alternative also see <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm uninstall velero -n &lt;NAMESPACE&gt;
kubectl delete crds -l app.kubernetes.io/name=velero</pre></div></div></div><div class="chapter " id="_miscellaneous"><div class="titlepage"><div><div><h1 class="title"><span class="number">14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Miscellaneous</span> <a title="Permalink" class="permalink" href="#_miscellaneous">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_configuring_httphttps_proxy_for_cri_o"><span class="number">14.1 </span><span class="name">Configuring HTTP/HTTPS Proxy for CRI-O</span></a></span></dt><dt><span class="section"><a href="#config-crio-container-registry"><span class="number">14.2 </span><span class="name">Configuring Container Registries for CRI-O</span></a></span></dt><dt><span class="section"><a href="#_flexvolume_configuration"><span class="number">14.3 </span><span class="name">FlexVolume Configuration</span></a></span></dt><dt><span class="section"><a href="#_configuring_kubelet"><span class="number">14.4 </span><span class="name">Configuring kubelet</span></a></span></dt><dt><span class="section"><a href="#k8s-changes-117-118"><span class="number">14.5 </span><span class="name">Changes from Kubernetes 1.17 to 1.18</span></a></span></dt></dl></div></div><div class="sect1" id="_configuring_httphttps_proxy_for_cri_o"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring HTTP/HTTPS Proxy for CRI-O</span> <a title="Permalink" class="permalink" href="#_configuring_httphttps_proxy_for_cri_o">#</a></h2></div></div></div><p>In some cases you must configure the container runtime to use a proxy to pull
container images.</p><p>The CRI-O runtime uses the system-wide proxy configuration, defined at <code class="literal">/etc/sysconfig/proxy</code>.</p><p>This file can be edited a number of ways.
It can be pre-configured at build time via AutoYaST, as described in the
<a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-autoyast/#Configuration-Network-Proxy" target="_blank">AutoYaST documentation</a>.
On an existing system, the file can be edited via YaST by running <code class="literal">yast2 proxy</code>.</p><p>If preferred, it can alternatively be edited manually as described in the SUSE <a class="link" href="https://www.suse.com/support/kb/doc/?id=7006845" target="_blank">Knowledge Base</a>
article</p><div id="id-1.16.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>CRI-O and skuba both support four types of comma-separated entries in the <code class="literal">NO_PROXY</code> variable:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>An exact IP address (<code class="literal">1.2.3.4</code>)</p></li><li class="listitem "><p>CIDR IP range (<code class="literal">1.2.3.4/16</code>)</p></li><li class="listitem "><p>DNS domain name (<code class="literal">eg.com</code> matches <code class="literal">www.eg.com</code> and <code class="literal">eg.com</code>)</p></li><li class="listitem "><p>Restricted DNS subdomain (<code class="literal">.eg.com</code> matches <code class="literal">www.eg.com</code> but not <code class="literal">eg.com</code>)</p></li></ul></div><p>All standard programs should ignore unsupported values in that variable and continue to work (albeit without the configured proxy)
when encountering an unsupported value.</p></div><div id="id-1.16.2.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Not all programs on all systems will respect CIDR ranges or restricted subdomains.</p></div><p>After you have configured the system proxy for your environment, restart the container runtime with:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl restart crio</pre></div></div><div class="sect1" id="config-crio-container-registry"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Container Registries for CRI-O</span> <a title="Permalink" class="permalink" href="#config-crio-container-registry">#</a></h2></div></div></div><div id="id-1.16.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The configuration example in this text uses <code class="literal">VERSION 2</code> of the CRI-O registries
configuration syntax. It is not compatible with the <code class="literal">VERSION 1</code> syntax present
in some upstream examples.</p><p>Please refer to: <a class="link" href="https://raw.githubusercontent.com/containers/image/master/docs/containers-registries.conf.5.md" target="_blank">https://raw.githubusercontent.com/containers/image/master/docs/containers-registries.conf.5.md</a></p></div><div id="id-1.16.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>You can create and deploy a custom <code class="literal">registries.conf</code> during the initial bootstrap of the cluster with skuba.
Create the file <code class="literal">&lt;CLUSTER_NAME&gt;/addons/containers/registries.conf</code> and apply your changes there.
This file will be rolled out during node bootstrapping and upgrading.</p></div><p>Every registry-related configuration needs to be done in the <a class="link" href="https://github.com/toml-lang/toml" target="_blank">TOML</a> file
<code class="literal">/etc/containers/registries.conf</code>. After any change of this file, CRI-O
needs to be restarted.</p><p>The configuration is a sequence of <code class="literal">[[registry]]</code> entries. For example, a
single registry entry within that configuration could be added like this:</p><p><code class="literal">/etc/containers/registries.conf</code></p><div class="verbatim-wrap"><pre class="screen">[[registry]]
blocked = false
insecure = false
location = "example.net/bar"
prefix = "example.com/foo/images"
mirror = [
    { location = "example-mirror-0.local", insecure = false },
    { location = "example-mirror-1.local", insecure = true, mirror-by-digest-only = true }
]

[[registry]]
blocked = false
insecure = false
location = "example.net/mymirror"
prefix = "example.com/mirror/images"
mirror = [
    { location = "example-mirror-2.local", insecure = false, mirror-by-digest-only = true },
    { location = "example-mirror-3.local", insecure = true }
]
unqualified-search = false</pre></div><p>Given an image name, a single <code class="literal">[[registry]]</code> TOML table is chosen based on its
<code class="literal">prefix</code> field.</p><p>A prefix is mainly a user-specified image name and can have one of the
following formats:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">host[:port]</code></p></li><li class="listitem "><p><code class="literal">host[:port]/namespace[/namespace…]</code></p></li><li class="listitem "><p><code class="literal">host[:port]/namespace[/namespace…]/repo</code></p></li><li class="listitem "><p><code class="literal">host[:port]/namespace[/namespace…]/repo[:tag|@digest]</code></p></li></ul></div><p>The user-specified image name must start with the specified <code class="literal">prefix</code> (and
continue with the appropriate separator) for a particular <code class="literal">[[registry]]</code> TOML
table to be considered. Only the TOML entry with the longest match is used.</p><p>As a special case, the <code class="literal">prefix</code> field can be missing. If so, it defaults to the
value of the <code class="literal">location</code> field.</p><div class="sect2" id="_per_namespace_settings"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Per-namespace Settings</span> <a title="Permalink" class="permalink" href="#_per_namespace_settings">#</a></h3></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">insecure</code> (<code class="literal">true</code> or <code class="literal">false</code>): By default, container runtimes require TLS
when retrieving images from a registry. If <code class="literal">insecure</code> is set to <code class="literal">true</code>,
unencrypted HTTP as well as TLS connections with untrusted certificates are
allowed.</p></li><li class="listitem "><p><code class="literal">blocked</code> (<code class="literal">true</code> or <code class="literal">false</code>): If <code class="literal">true</code>, pulling images with matching names
is forbidden.</p></li></ul></div></div><div class="sect2" id="_remapping_and_mirroring_registries"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remapping and Mirroring Registries</span> <a title="Permalink" class="permalink" href="#_remapping_and_mirroring_registries">#</a></h3></div></div></div><p>The user-specified image reference is, primarily, a "logical" image name,
always used for naming the image. By default, the image reference also directly
specifies the registry and repository to use, but the following options can be
used to redirect the underlying accesses to different registry servers or
locations. This can be used to support configurations with no access to the
Internet without having to change Dockerfiles, or to add redundancy.</p><div class="sect3" id="_location"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><code class="literal">location</code></span> <a title="Permalink" class="permalink" href="#_location">#</a></h4></div></div></div><p>Accepts the same format as the <code class="literal">prefix</code> field, and specifies the physical
location of the <code class="literal">prefix</code>-rooted namespace. By default, this is equal to <code class="literal">prefix</code>
(in which case <code class="literal">prefix</code> can be omitted and the <code class="literal">[[registry]]</code> TOML table can
just specify <code class="literal">location</code>).</p><div class="sect4" id="_example"><div class="titlepage"><div><div><h5 class="title"><span class="number">14.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example</span> <a title="Permalink" class="permalink" href="#_example">#</a></h5></div></div></div><div class="verbatim-wrap"><pre class="screen">prefix = "example.com/foo"
location = "internal-registry-for-example.net/bar"</pre></div><p>Requests for the image <code class="literal">example.com/foo/myimage:latest</code> will actually work with
the <code class="literal">internal-registry-for-example.net/bar/myimage:latest</code> image.</p></div></div><div class="sect3" id="_mirror"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><code class="literal">mirror</code></span> <a title="Permalink" class="permalink" href="#_mirror">#</a></h4></div></div></div><p>An array of TOML tables specifying (possibly partial) mirrors for the
<code class="literal">prefix</code>-rooted namespace.</p><p>The mirrors are attempted in the specified order. The first one that can be
contacted and contains the image will be used (and if none of the mirrors
contains the image, the primary location specified by the <code class="literal">registry.location</code>
field, or using the unmodified user-specified reference, is tried last).</p><p>Each TOML table in the <code class="literal">mirror</code> array can contain the following fields, with
the same semantics as if specified in the <code class="literal">[[registry]]</code> TOML table directly:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">location</code></p></li><li class="listitem "><p><code class="literal">insecure</code></p></li></ul></div></div><div class="sect3" id="_mirror_by_digest_only"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><code class="literal">mirror-by-digest-only</code></span> <a title="Permalink" class="permalink" href="#_mirror_by_digest_only">#</a></h4></div></div></div><p>Can be <code class="literal">true</code> or <code class="literal">false</code>. If <code class="literal">true</code>, mirrors will only be used during pulling
if the image reference includes a digest. Referencing an image by digest
ensures that the same one is always used (whereas referencing an image by a tag may
cause different registries to return different images if the tag mapping is out
of sync).</p><p>Note that if this is <code class="literal">true</code>, images referenced by a tag will only use the primary
registry, failing if that registry is not accessible.</p></div></div></div><div class="sect1" id="_flexvolume_configuration"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">FlexVolume Configuration</span> <a title="Permalink" class="permalink" href="#_flexvolume_configuration">#</a></h2></div></div></div><p>FlexVolume drivers are external (out-of-tree) drivers usually provided by a specific vendor.
They are executable files that are placed in a predefined directory in the cluster on both worker and master nodes.
Pods interact with FlexVolume drivers through the <code class="literal">flexvolume</code> in-tree plugin.</p><p>The vendor driver first has to be installed on each worker and master node in a Kubernetes cluster.
On SUSE CaaS Platform 4, the path to install the drivers is <code class="literal">/usr/lib/kubernetes/kubelet-plugins/volume/exec/</code>.</p><p>If the drivers are deployed with <code class="literal">DaemonSet</code>, this will require changing
the FlexVolume directory path, which is usually stored as an environment
variable, for example for rook:</p><div class="verbatim-wrap highlight bash"><pre class="screen">FLEXVOLUME_DIR_PATH=/usr/lib/kubernetes/kubelet-plugins/volume/exec/</pre></div><p>For a general guide to the FlexVolume configuration, see <a class="link" href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md" target="_blank">https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md</a></p></div><div class="sect1" id="_configuring_kubelet"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring kubelet</span> <a title="Permalink" class="permalink" href="#_configuring_kubelet">#</a></h2></div></div></div><div id="id-1.16.5.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Modifying the file <code class="literal">/etc/sysconfig/kubelet</code> directly is not supported.</p><p>The changes made to this file will not persist through an update/upgrade of the software.
Please follow the instructions below to change the configuration for <code class="literal">kubelet</code> persistently.</p></div><div id="id-1.16.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>This procedure does not override the default configuration but amends the changes
from the "drop-in" configuration.</p><p>Please refer to: <a class="link" href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html" target="_blank">https://www.freedesktop.org/software/systemd/man/systemd.unit.html</a></p></div><p>If you wish to modify the configuration for <code class="literal">kubelet</code> you must use the "drop-in"
configuration feature of systemd. The steps need to be performed on each cluster
node whose <code class="literal">kubelet</code> you wish to reconfigure.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create an appropriate <code class="literal">.conf</code> file (e.g. <code class="literal">resource-handling.conf</code>) in <code class="literal">/usr/lib/systemd/system/kubelet.service.d/</code> with your desired changes.
.</p></li><li class="listitem "><p>Reload the service definitions</p><div class="verbatim-wrap"><pre class="screen">sudo systemctl daemon-reload</pre></div></li><li class="listitem "><p>Restart kubelet</p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart kubelet</pre></div></li></ol></div></div><div class="sect1" id="k8s-changes-117-118"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changes from Kubernetes 1.17 to 1.18</span> <a title="Permalink" class="permalink" href="#k8s-changes-117-118">#</a></h2></div></div></div><p>This documentation page lists all the behavioral changes and API deprecations that will happen from Kubernetes 1.17 to 1.18.
You will require this information when migrating to SUSE CaaS Platform 5.x shipping with Kubernetes 1.18.</p><div class="sect2" id="_api_deprecations"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">API deprecations</span> <a title="Permalink" class="permalink" href="#_api_deprecations">#</a></h3></div></div></div><p>The following APIs have been deprecated:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>All resources under <span class="strong"><strong>apps/v1beta1</strong></span> and <span class="strong"><strong>apps/v1beta2</strong></span>: please use <span class="strong"><strong>apps/v1</strong></span> instead.</p></li><li class="listitem "><p><span class="strong"><strong>daemonsets</strong></span>, <span class="strong"><strong>deployments</strong></span>, <span class="strong"><strong>replicasets</strong></span> under <span class="strong"><strong>extensions/v1beta1</strong></span>: please use <span class="strong"><strong>apps/v1</strong></span> instead.</p></li><li class="listitem "><p><span class="strong"><strong>networkpolicies</strong></span> under <span class="strong"><strong>extensions/v1beta1</strong></span>: please use <span class="strong"><strong>networking.k8s.io/v1</strong></span> instead.</p></li><li class="listitem "><p><span class="strong"><strong>podsecuritypolicies</strong></span> resources under <span class="strong"><strong>extensions/v1beta1</strong></span>: please use <span class="strong"><strong>policy/v1beta1</strong></span> instead.</p></li></ul></div></div><div class="sect2" id="_behavioral_changes"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Behavioral changes</span> <a title="Permalink" class="permalink" href="#_behavioral_changes">#</a></h3></div></div></div><p>In this section we will highlight some relevant changes that might interest you for the new Kubernetes version.</p><div class="sect3" id="_core"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Core</span> <a title="Permalink" class="permalink" href="#_core">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/853" target="_blank">#853</a> Configurable scale velocity for HPA.</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1393" target="_blank">#1393</a> Provide OIDC discovery for service account token issuer.</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1513" target="_blank">#1513</a> CertificateSigningRequest API.</p></li></ul></div></div><div class="sect3" id="_scheduling"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scheduling</span> <a title="Permalink" class="permalink" href="#_scheduling">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1451" target="_blank">#1451</a> Run multiple Scheduling Profiles [Alpha]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/895" target="_blank">#895</a> Even pod spreading across failure domains [Beta]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1258" target="_blank">#1258</a> Add a configurable default Even Pod Spreading rule [Alpha]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/166" target="_blank">#166</a> Taint Based Eviction [Stable]</p></li></ul></div></div><div class="sect3" id="_nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nodes</span> <a title="Permalink" class="permalink" href="#_nodes">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1539" target="_blank">#1539</a> Extending Hugepage Feature [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/688" target="_blank">#688</a> Pod Overhead: account resources tied to the pod sandbox, but not specific containers [Beta]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/693" target="_blank">#693</a> Node Topology Manager [Beta]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/950" target="_blank">#950</a> Add pod-startup liveness-probe holdoff for slow-starting pods [Beta]</p></li></ul></div></div><div class="sect3" id="_networking"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking</span> <a title="Permalink" class="permalink" href="#_networking">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/752" target="_blank">#752</a> EndpointSlice API [Beta]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/508" target="_blank">#508</a> IPv6 support added [Beta]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1024" target="_blank">#1024</a> Graduate NodeLocal DNSCache to GA [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1453" target="_blank">#1453</a> Graduate Ingress to V1 [Beta]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1507" target="_blank">#1507</a> Adding AppProtocol to Services and Endpoints [Stable]</p></li></ul></div></div><div class="sect3" id="_api"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.5.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">API</span> <a title="Permalink" class="permalink" href="#_api">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1040" target="_blank">#1040</a> Priority and Fairness for API Server Requests [Alpha]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1601" target="_blank">#1601</a> client-go signature refactor to standardize options and context handling [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/576" target="_blank">#576</a> APIServer DryRun [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1281" target="_blank">#1281</a> API Server Network Proxy KEP to Beta [Beta]</p></li></ul></div></div><div class="sect3" id="_storage_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.5.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage</span> <a title="Permalink" class="permalink" href="#_storage_2">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/695" target="_blank">#695</a> Skip Volume Ownership Change [Alpha]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1412" target="_blank">#1412</a> Immutable Secrets and ConfigMaps [Alpha]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1495" target="_blank">#1495</a> Generic data populators [Alpha]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/770" target="_blank">#770</a> Skip attach for non-attachable CSI volumes [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/351" target="_blank">#351</a> Raw block device using persistent volume source [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/565" target="_blank">#565</a> CSI Block storage support [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/603" target="_blank">#603</a> Pass Pod information in CSI calls [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/989" target="_blank">#989</a> Extend allowed PVC DataSources [Stable]</p></li></ul></div></div><div class="sect3" id="_features"><div class="titlepage"><div><div><h4 class="title"><span class="number">14.5.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Features</span> <a title="Permalink" class="permalink" href="#_features">#</a></h4></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/1441" target="_blank">#1441</a> kubectl debug [Alpha]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/491" target="_blank">#491</a> kubectl diff [Stable]</p></li><li class="listitem "><p><a class="link" href="https://github.com/kubernetes/enhancements/issues/670" target="_blank">#670</a> Support Out-of-Tree vSphere Cloud Provider [Stable]</p></li></ul></div></div></div></div></div><div class="chapter " id="_troubleshooting_3"><div class="titlepage"><div><div><h1 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#_troubleshooting_3">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_the_supportconfig_tool"><span class="number">15.1 </span><span class="name">The <code class="literal">supportconfig</code> Tool</span></a></span></dt><dt><span class="section"><a href="#_cluster_definition_directory"><span class="number">15.2 </span><span class="name">Cluster definition directory</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-logs"><span class="number">15.3 </span><span class="name">Log collection</span></a></span></dt><dt><span class="section"><a href="#_debugging_sles_nodes_provision"><span class="number">15.4 </span><span class="name">Debugging SLES Nodes provision</span></a></span></dt><dt><span class="section"><a href="#_debugging_cluster_deployment"><span class="number">15.5 </span><span class="name">Debugging Cluster Deployment</span></a></span></dt><dt><span class="section"><a href="#_error_x509_certificate_signed_by_unknown_authority"><span class="number">15.6 </span><span class="name">Error <code class="literal">x509: certificate signed by unknown authority</code></span></a></span></dt><dt><span class="section"><a href="#_error_invalid_client_credentials"><span class="number">15.7 </span><span class="name">Error <code class="literal">Invalid client credentials</code></span></a></span></dt><dt><span class="section"><a href="#_replacing_a_lost_node"><span class="number">15.8 </span><span class="name">Replacing a Lost Node</span></a></span></dt><dt><span class="section"><a href="#_rebooting_an_undrained_node_with_rbd_volumes_mapped"><span class="number">15.9 </span><span class="name">Rebooting an Undrained Node with RBD Volumes Mapped</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-etcd"><span class="number">15.10 </span><span class="name">ETCD Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#_kubernetes_debugging_tips"><span class="number">15.11 </span><span class="name">Kubernetes debugging tips</span></a></span></dt><dt><span class="section"><a href="#_helm_error_context_deadline_exceeded"><span class="number">15.12 </span><span class="name">Helm <code class="literal">Error: context deadline exceeded</code></span></a></span></dt><dt><span class="section"><a href="#_aws_deployment_fails_with_cannot_attach_profile_error"><span class="number">15.13 </span><span class="name">AWS Deployment fails with <code class="literal">cannot attach profile</code> error</span></a></span></dt></dl></div></div><p>This chapter summarizes frequent problems that can occur while using SUSE CaaS Platform
and their solutions.</p><p>Additionally, SUSE support collects problems and their solutions online at <a class="link" href="https://www.suse.com/support/kb/?id=SUSE_CaaS_Platform" target="_blank">https://www.suse.com/support/kb/?id=SUSE_CaaS_Platform</a> .</p><div class="sect1" id="_the_supportconfig_tool"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The <code class="literal">supportconfig</code> Tool</span> <a title="Permalink" class="permalink" href="#_the_supportconfig_tool">#</a></h2></div></div></div><p>As a first step for any troubleshooting/debugging effort, you need to find out
the location of the cause of the problem. For this purpose we ship the <code class="literal">supportconfig</code> tool
and plugin with SUSE CaaS Platform. With a simple command you can collect and compile
a variety of details about your cluster to enable SUSE support to pinpoint
the potential cause of an issue.</p><p>In case of problems, a detailed system report can be created with the
<code class="literal">supportconfig</code> command line tool. It will collect information about the system, such as:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Current Kernel version</p></li><li class="listitem "><p>Hardware information</p></li><li class="listitem "><p>Installed packages</p></li><li class="listitem "><p>Partition setup</p></li><li class="listitem "><p>Cluster and node status</p></li></ul></div><div id="id-1.17.4.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>A full list of of the data collected by <code class="literal">supportconfig</code> can be found under
<a class="link" href="https://github.com/SUSE/supportutils-plugin-suse-caasp/blob/master/README.md" target="_blank">https://github.com/SUSE/supportutils-plugin-suse-caasp/blob/master/README.md</a>.</p></div><div id="id-1.17.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>To collect all the relevant logs, run the <code class="literal">supportconfig</code> command on all the master
and worker nodes individually.</p></div><div class="verbatim-wrap highlight bash"><pre class="screen">sudo supportconfig
sudo tar -xvJf /var/log/nts_*.txz
cd /var/log/nts*
sudo cat kubernetes.txt crio.txt</pre></div><p>The result is a <code class="literal">TAR</code> archive of files. Each of the <code class="literal">*.txz</code> files should be given a name that can be used to identify which cluster node it was created on.</p><p>After opening a Service Request (SR), you can upload the <code class="literal">TAR</code> archives to SUSE Global Technical Support.</p><p>The data will help to debug the issue you reported and assist you in solving the problem. For details, see <a class="link" href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-adm-support</a>.</p></div><div class="sect1" id="_cluster_definition_directory"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster definition directory</span> <a title="Permalink" class="permalink" href="#_cluster_definition_directory">#</a></h2></div></div></div><p>Apart from the logs provided by running the <code class="literal">supportconfig</code> tool, an additional set of data might be required for debugging purposes. This information is located at the Management node, under your cluster definition directory. This folder contains important and sensitive information about your SUSE CaaS Platform cluster and it’s the one from where you issue <code class="literal">skuba</code> commands.</p><div id="id-1.17.5.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>If the problem you are facing is related to your production environment, do <span class="strong"><strong>not</strong></span> upload the <code class="literal">admin.conf</code> as this would expose access to your cluster to anyone in possession of the collected information! The same precautions apply for the <code class="literal">pki</code> directory, since this also contains sensitive information (CA cert and key).</p><p>In this case add <code class="literal">--exclude='./&lt;CLUSTER_NAME&gt;/admin.conf' --exclude='./&lt;CLUSTER_NAME&gt;/pki/'</code> to the command in the following example. Make sure to replace <code class="literal">./&lt;CLUSTER_NAME&gt;</code> with the actual path of your cluster definition folder.</p><p>If you need to debug issues with your private certificates, a separate call with SUSE support must be scheduled to help you.</p></div><p>Create a <code class="literal">TAR</code> archive by compressing the cluster definition directory.</p><div class="verbatim-wrap highlight bash"><pre class="screen"># Read the TIP above
# Move the admin.conf and pki directory to another safe location or exclude from packaging
tar -czvf cluster.tar.gz /home/user/&lt;CLUSTER_NAME&gt;/
# If the error is related to Terraform, please copy the terraform configuration files as well
tar -czvf cluster.tar.gz /home/user/my-terraform-configuration/</pre></div><p>After opening a Service Request (SR), you can upload the <code class="literal">TAR</code> archive to SUSE Global Technical Support.</p></div><div class="sect1" id="troubleshooting-logs"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Log collection</span> <a title="Permalink" class="permalink" href="#troubleshooting-logs">#</a></h2></div></div></div><p>Some of these information are required for debugging certain cases. The data collected
via <code class="literal">supportconfig</code> in such cases are following:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>etcd.txt (<span class="emphasis"><em>master nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/health
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/members
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/leader
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/self
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/store
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/metrics

etcdcontainer=$(crictl ps --label io.kubernetes.container.name=etcd --quiet)

crictl exec $etcdcontainer sh -c \"ETCDCTL_ENDPOINTS='https://127.0.0.1:2379' ETCDCTL_CACERT='/etc/kubernetes/pki/etcd/ca.crt' ETCDCTL_CERT='/etc/kubernetes/pki/etcd/server.crt' ETCDCTL_KEY='/etc/kubernetes/pki/etcd/server.key' ETCDCTL_API=3 etcdctl check perf\"

crictl logs -t $etcdcontainer

crictl stats --id $etcdcontainer

etcdpod=$(crictl ps | grep etcd | awk -F ' ' '{ print $9 }')

crictl inspectp $etcdpod</pre></div></li></ul></div><div id="id-1.17.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>For more information about <code class="literal">etcd</code>, refer to <a class="xref" href="#troubleshooting-etcd" title="15.10. ETCD Troubleshooting">Section 15.10, “ETCD Troubleshooting”</a>.</p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>kubernetes.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">export KUBECONFIG=/etc/kubernetes/admin.conf

kubectl version

kubectl api-versions

kubectl config view

kubectl -n kube-system get pods

kubectl get events --sort-by=.metadata.creationTimestamp

kubectl get nodes

kubectl get all -A

kubectl get nodes -o yaml</pre></div></li><li class="listitem "><p>kubernetes-cluster-info.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">export KUBECONFIG=/etc/kubernetes/admin.conf

# a copy of kubernetes logs /var/log/kubernetes
kubectl cluster-info dump --output-directory="/var/log/kubernetes"</pre></div></li><li class="listitem "><p>kubelet.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">systemctl status --full kubelet

journalctl -u kubelet

# a copy of kubernetes manifests /etc/kubernetes/manifests"
cat /var/lib/kubelet/config.yaml</pre></div></li><li class="listitem "><p>oidc-gangway.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="oidc-gangway" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "oidc-gangway" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>oidc-dex.txt (<span class="emphasis"><em>worker nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="oidc-dex" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "oidc-dex" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>cilium-agent.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="cilium-agent" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "cilium-agent" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>cilium-operator.txt (<span class="emphasis"><em>only from the worker node is runs</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="cilium-operator" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "cilium-operator" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>kured.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="kured" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kured" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>coredns.txt (_worker nodes)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="coredns" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "coredns" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>kube-apiserver.txt (<span class="emphasis"><em>master nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="kube-apiserver" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kube-apiserver" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>kube-proxy.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="kube-proxy" --quiet)

crictl logs -t $container

crictl inspect $container
After skuba 4.2.2
pod=$(crictl ps | grep "kube-proxy" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>kube-scheduler.txt (<span class="emphasis"><em>master nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="kube-scheduler" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kube-scheduler" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>kube-controller-manager.txt (<span class="emphasis"><em>master nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">container=$(crictl ps --label io.kubernetes.container.name="kube-controller-manager" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kube-controller-manager" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod</pre></div></li><li class="listitem "><p>kube-system.txt (<span class="emphasis"><em>all nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">export KUBECONFIG=/etc/kubernetes/admin.conf

kubectl get all -n kube-system -o yaml</pre></div></li><li class="listitem "><p>crio.txt (<span class="emphasis"><em>all_nodes</em></span>)</p><div class="verbatim-wrap"><pre class="screen">crictl version

systemctl status --full crio.service

crictl info

crictl images

crictl ps --all

crictl stats --all

journalctl -u crio

# a copy of /etc/crictl.yaml

# a copy of /etc/sysconfig/crio

# a copy of every file under /etc/crio/

# Run the following three commands for every container using this loop:
for i in $(crictl  ps -a 2&gt;/dev/null | grep -v "CONTAINER" | awk '{print $1}');
do
    crictl stats --id $i
    crictl logs $i
    crictl inspect $i
done</pre></div></li></ul></div></div><div class="sect1" id="_debugging_sles_nodes_provision"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Debugging SLES Nodes provision</span> <a title="Permalink" class="permalink" href="#_debugging_sles_nodes_provision">#</a></h2></div></div></div><p>If Terraform fails to setup the required SLES infrastructure for your cluster, please provide the configuration
you applied in a form of a TAR archive.</p><p>Create a <code class="literal">TAR</code> archive by compressing the Terraform.</p><div class="verbatim-wrap highlight bash"><pre class="screen">tar -czvf terraform.tar.gz /path/to/terraform/configuration</pre></div><p>After opening a Service Request (SR), you can upload the TAR archive to Global Technical Support.</p></div><div class="sect1" id="_debugging_cluster_deployment"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Debugging Cluster Deployment</span> <a title="Permalink" class="permalink" href="#_debugging_cluster_deployment">#</a></h2></div></div></div><p>If the cluster deployment fails, please re-run the command again with setting verbosity level to 5 <code class="literal">-v=5</code>.</p><p>For example, if bootstraps the first master node of the cluster fails, re-run the command like</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node bootstrap --user sles --sudo --target &lt;IP/FQDN&gt; &lt;NODE_NAME&gt; -v=5</pre></div><p>However, if the <code class="literal">join</code> procedure fails at the last final steps, re-running it might <span class="emphasis"><em>not</em></span> help. To verify
this, please list the current member nodes of your cluster and look for the one who failed.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get nodes</pre></div><p>If the node that failed to <code class="literal">join</code> is nevertheless listed in the output as part of your cluster,
then this is a bad indicator. This node cannot be reset back to a clean state anymore and it’s not safe to keep
it online in this <span class="emphasis"><em>unknown</em></span> state. As a result, instead of trying to fix its existing configuration either by hand or re-running
the join/bootstrap command, we would highly recommend you to remove this node completely from your cluster and
then replace it with a new one.</p><div class="verbatim-wrap highlight bash"><pre class="screen">skuba node remove &lt;NODE_NAME&gt; --drain-timeout 5s</pre></div></div><div class="sect1" id="_error_x509_certificate_signed_by_unknown_authority"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error <code class="literal">x509: certificate signed by unknown authority</code></span> <a title="Permalink" class="permalink" href="#_error_x509_certificate_signed_by_unknown_authority">#</a></h2></div></div></div><p>When interacting with Kubernetes, you might run into the situation where your existing configuration for the authentication has changed (cluster has been rebuild, certificates have been switched.)
In such a case you might see an error message in the output of your CLI or Web browser.</p><div class="verbatim-wrap"><pre class="screen">x509: certificate signed by unknown authority</pre></div><p>This message indicates that your current system does not know the Certificate Authority (CA) that signed the SSL certificates used for encrypting the communication to the cluster.
You then need to add or update the Root CA certificate in your local trust store.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Obtain the root CA certificate from on of the Kubernetes cluster node, at the location <code class="literal">/etc/kubernetes/pki/ca.crt</code></p></li><li class="listitem "><p>Copy the root CA certificate into your local machine directory <code class="literal">/etc/pki/trust/anchors/</code></p></li><li class="listitem "><p>Update the cache for know CA certificates</p><div class="verbatim-wrap highlight bash"><pre class="screen">sudo update-ca-certificates</pre></div></li></ol></div></div><div class="sect1" id="_error_invalid_client_credentials"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error <code class="literal">Invalid client credentials</code></span> <a title="Permalink" class="permalink" href="#_error_invalid_client_credentials">#</a></h2></div></div></div><p>When using Dex &amp; Gangway for authentication, you might see the following error message in the Web browser output:</p><div class="verbatim-wrap"><pre class="screen">oauth2: cannot fetch token: 401 Unauthorized
Response: {"error":"invalid_client","error_description":"Invalid client credentials."}</pre></div><p>This message indicates that your Kubernetes cluster Dex &amp; Gangway client secret is out of sync.</p><div class="sect2" id="_versions_before_suse_caas_platform_4_2_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Versions before SUSE CaaS Platform 4.2.2</span> <a title="Permalink" class="permalink" href="#_versions_before_suse_caas_platform_4_2_2">#</a></h3></div></div></div><div id="id-1.17.10.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>These steps apply to <code class="literal">skuba</code> ≤ 1.3.5</p></div><p>Please update the Dex &amp; Gangway ConfigMap to use the same client secret.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl -n kube-system get configmap oidc-dex-config -o yaml &gt; oidc-dex-config.yaml
kubectl -n kube-system get configmap oidc-gangway-config -o yaml &gt; oidc-gangway-config.yaml</pre></div><p>Make sure the oidc’s <code class="literal">secret</code> in <code class="literal">oidc-dex-config.yaml</code> is the same as the <code class="literal">clientSecret</code> in <code class="literal">oidc-gangway-config.yaml</code>.
Then, apply the updated ConfigMap.</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl replace -f oidc-dex-config.yaml
kubectl replace -f oidc-gangway-config.yaml</pre></div></div><div class="sect2" id="_versions_after_suse_caas_platform_4_2_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Versions after SUSE CaaS Platform 4.2.2</span> <a title="Permalink" class="permalink" href="#_versions_after_suse_caas_platform_4_2_2">#</a></h3></div></div></div><div id="id-1.17.10.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>These steps apply to <code class="literal">skuba</code> ≥ 1.4.1</p></div><p>If you have configured Dex via a kustomize patch, please update your patch to use <code class="literal">secretEnv: OIDC_GANGWAY_CLIENT_SECRET</code>.
Change your patch as follows, from:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">- id: oidc
  ...
  name: 'OIDC'
  secret: &lt;client-secret&gt;
  trustedPeers:
  - oidc-cli</pre></div><p>to</p><div class="verbatim-wrap highlight yaml"><pre class="screen">- id: oidc
  ...
  name: 'OIDC'
  secretEnv: OIDC_GANGWAY_CLIENT_SECRET
  trustedPeers:
  - oidc-cli</pre></div><p>Dex &amp; Gangway will then use the same client secret.</p></div></div><div class="sect1" id="_replacing_a_lost_node"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Lost Node</span> <a title="Permalink" class="permalink" href="#_replacing_a_lost_node">#</a></h2></div></div></div><p>If your cluster loses a node, for example due to failed hardware, remove the node as explained in <a class="xref" href="#removing-nodes" title="2.4. Removing Nodes">Section 2.4, “Removing Nodes”</a>.
Then add a new node as described in <a class="xref" href="#adding-nodes" title="2.3. Adding Nodes">Section 2.3, “Adding Nodes”</a>.</p></div><div class="sect1" id="_rebooting_an_undrained_node_with_rbd_volumes_mapped"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting an Undrained Node with RBD Volumes Mapped</span> <a title="Permalink" class="permalink" href="#_rebooting_an_undrained_node_with_rbd_volumes_mapped">#</a></h2></div></div></div><p>Rebooting a cluster node always requires a preceding <code class="literal">drain</code>.
In some cases, draining the nodes first might not be possible and some problem can occur during reboot if some RBD volumes are mapped to the nodes.</p><p>In this situation, apply the following steps.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Make sure kubelet and CRI-O are stopped:</p><div class="verbatim-wrap highlight bash"><pre class="screen">systemctl stop kubelet crio</pre></div></li><li class="listitem "><p>Unmount every RBD device <code class="literal">/dev/rbd*</code> before rebooting. For example:</p><div class="verbatim-wrap highlight bash"><pre class="screen">umount -vAf /dev/rbd0</pre></div></li></ol></div><p>If there are several device mounted, this little script can be used to avoid manual unmounting:</p><div class="verbatim-wrap highlight bash"><pre class="screen">#!/usr/bin/env bash

while grep "rbd" /proc/mounts &gt; /dev/null 2&gt;&amp;1; do
  for dev in $(lsblk -p -o NAME | grep "rbd"); do
    if $(mountpoint -x $dev &gt; /dev/null 2&gt;&amp;1); then
      echo "&gt;&gt;&gt; umounting $dev"
      umount -vAf "$dev"
    fi
  done
done</pre></div></div><div class="sect1" id="troubleshooting-etcd"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ETCD Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-etcd">#</a></h2></div></div></div><div class="sect2" id="_introduction_5"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction_5">#</a></h3></div></div></div><p>This document aims to describe debugging an etcd cluster.</p><p>The required etcd logs are part of the <code class="literal">supportconfig</code>, a utility that collects all the required information for debugging a problem. The rest of the document provides information on how you can obtain these information manually.</p></div><div class="sect2" id="_etcd_container"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ETCD container</span> <a title="Permalink" class="permalink" href="#_etcd_container">#</a></h3></div></div></div><p>ETCD is a distributed reliable key-value store for the most critical data of a distributed system. It is running <span class="strong"><strong>only on the master</strong></span> nodes in a form a container application. For instance, in a cluster with 3 master nodes, it is expected
to have 3 etcd instances as well:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get pods -n kube-system -l component=etcd
NAME                            READY   STATUS    RESTARTS   AGE
etcd-vm072044.qa.prv.suse.net   1/1     Running   1          7d
etcd-vm072050.qa.prv.suse.net   1/1     Running   1          7d
etcd-vm073033.qa.prv.suse.net   1/1     Running   1          7d</pre></div><p>The specific configuration which <code class="literal">etcd</code> is using to start, is the following:</p><div class="verbatim-wrap highlight bash"><pre class="screen">etcd \
      --advertise-client-urls=https://&lt;YOUR_MASTER_NODE_IP_ADDRESS&gt;:2379 \
      --cert-file=/etc/kubernetes/pki/etcd/server.crt  \
      --client-cert-auth=true --data-dir=/var/lib/etcd \
      --initial-advertise-peer-urls=https://&lt;YOUR_MASTER_NODE_IP_ADDRESS&gt;:2380 \
      --initial-cluster=vm072050.qa.prv.suse.net=https://&lt;YOUR_MASTER_NODE_IP_ADDRESS&gt;:2380 \
      --key-file=/etc/kubernetes/pki/etcd/server.key \
      --listen-client-urls=https://127.0.0.1:2379,https://&lt;YOUR_MASTER_NODE_IP_ADDRESS&gt;:2379 \
      --listen-peer-urls=https://&lt;YOUR_MASTER_NODE_IP_ADDRESS&gt;:2380 \
      --name=vm072050.qa.prv.suse.net \
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt \
      --peer-client-cert-auth=true \
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key \
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \
      --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</pre></div><div id="id-1.17.13.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>For more information related to ETCD, we <span class="strong"><strong>highly</strong></span> recommend you to read <a class="link" href="https://etcd.io/docs/v3.4.0/faq/" target="_blank">ETCD FAQ</a> page.</p></div></div><div class="sect2" id="_logging_2"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">logging</span> <a title="Permalink" class="permalink" href="#_logging_2">#</a></h3></div></div></div><p>Since <code class="literal">etcd</code> is running in a container, that means it is not controlled by <code class="literal">systemd</code>, thus any commands related to that (e.g. <code class="literal">journalctl</code>) will fail, therefore you need to use container debugging approach instead.</p><div id="id-1.17.13.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>To use the following commands, you need to connect (e.g. via SSH) to the master node where the etcd pod is running.</p></div><p>To see the <code class="literal">etcd</code> logs, connect to a Kubernetes master node and then run as root:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ssh sles@&lt;MASTER_NODE&gt;
sudo bash # connect as root
etcdcontainer=$(crictl ps --label io.kubernetes.container.name=etcd --quiet)
crictl logs -f $etcdcontainer</pre></div></div><div class="sect2" id="_etcdctl"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">etcdctl</span> <a title="Permalink" class="permalink" href="#_etcdctl">#</a></h3></div></div></div><p><code class="literal">etcdctl</code> is a command line client for <code class="literal">etcd</code>. The new version of SUSE CaaS Platform is using the <code class="literal">v3</code> API. For that, you need to make sure to set environment variable <code class="literal">ETCDCTL_API=3</code> before using it. Apart from that, you need to provide the required keys and certificates for authentication and authorization, via <code class="literal">ETCDCTL_CACERT</code>, <code class="literal">ETCDCTL_CERT</code> and <code class="literal">ETCDCTL_KEY</code> environment variables. Last but not least, you need to also specify the endpoint via <code class="literal">ETCDCTL_ENDPOINTS</code> environment variable.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>Example</strong></span></p><p>To find out if your network and disk latency are fast enough, you can benchmark your node using the <code class="literal">etcdctl check perf</code> command. To do this, frist connect to a Kubernetes master node:</p><div class="verbatim-wrap highlight bash"><pre class="screen">ssh sles@&lt;MASTER_NODE&gt;
sudo bash # login as root</pre></div><p>and then run as root:</p><div class="verbatim-wrap highlight bash"><pre class="screen">etcdcontainer=$(crictl ps --label io.kubernetes.container.name=etcd --quiet)
crictl exec $etcdcontainer sh -c \
"ETCDCTL_ENDPOINTS='https://127.0.0.1:2379' \
ETCDCTL_CACERT='/etc/kubernetes/pki/etcd/ca.crt' \
ETCDCTL_CERT='/etc/kubernetes/pki/etcd/server.crt' \
ETCDCTL_KEY='/etc/kubernetes/pki/etcd/server.key' \
ETCDCTL_API=3 \
etcdctl check perf"</pre></div></li></ul></div></div><div class="sect2" id="_curl_as_an_alternative"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">curl as an alternative</span> <a title="Permalink" class="permalink" href="#_curl_as_an_alternative">#</a></h3></div></div></div><p>For most of the <code class="literal">etcdctl</code> commands, there is an alternative way to fetch the same information via <code class="literal">curl</code>. First you need to connect to the master node and then issue a <code class="literal">curl</code> command against the ETCD endpoint. Here’s an example of the information which <code class="literal">supportconfig</code> is collecting:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Health check:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">sudo curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/server.key \
--cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/health</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Member list</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">sudo curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/server.key \
--cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/members</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Leader information</p></li></ul></div><div class="verbatim-wrap"><pre class="screen"># available only from the master node where ETCD **leader** runs
sudo curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/server.key \
--cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/leader</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Current member information</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">sudo curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/server.key \
--cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/self</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Statistics</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">sudo curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/server.key \
--cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/store</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Metrics</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">sudo curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/server.key \
--cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/metrics</pre></div></div></div><div class="sect1" id="_kubernetes_debugging_tips"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Kubernetes debugging tips</span> <a title="Permalink" class="permalink" href="#_kubernetes_debugging_tips">#</a></h2></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>General guidelines and instructions:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/</a></p></li><li class="listitem "><p>Troubleshooting applications:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-application" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-application</a></p></li><li class="listitem "><p>Troubleshooting clusters:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster</a></p></li><li class="listitem "><p>Debugging pods:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller</a></p></li><li class="listitem "><p>Debugging services:
<a class="link" href="https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-service" target="_blank">https://v1-18.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-service</a></p></li></ul></div></div><div class="sect1" id="_helm_error_context_deadline_exceeded"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Helm <code class="literal">Error: context deadline exceeded</code></span> <a title="Permalink" class="permalink" href="#_helm_error_context_deadline_exceeded">#</a></h2></div></div></div><p>This means the tiller installation was secured via SSL/TLS as described in <a class="xref" href="#helm-tiller-install" title="3.1.2.1. Installing Helm">Section 3.1.2.1, “Installing Helm”</a>.
You must pass the <code class="literal">--tls</code> flag to helm to enable authentication.</p></div><div class="sect1" id="_aws_deployment_fails_with_cannot_attach_profile_error"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">AWS Deployment fails with <code class="literal">cannot attach profile</code> error</span> <a title="Permalink" class="permalink" href="#_aws_deployment_fails_with_cannot_attach_profile_error">#</a></h2></div></div></div><p>For SUSE CaaS Platform to be properly deployed, you need to have proper IAM role, role policy and instance profile set up in AWS.
Under normal circumstances Terraform will be invoked by a user with suitable permissions during deployment and automatically create these profiles.
If your access permissions on the AWS account forbid Terraform from creating the profiles automatically, they must be created before attempting deployment.</p><div class="sect2" id="_create_iam_role_role_policy_and_instance_profile_through_aws_cli"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create IAM Role, Role Policy, and Instance Profile through AWS CLI</span> <a title="Permalink" class="permalink" href="#_create_iam_role_role_policy_and_instance_profile_through_aws_cli">#</a></h3></div></div></div><p>Users who do not have permission to create IAM role, role policy, and instance profile using Terraform, devops should create them for you, using the instructions below:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">STACK_NAME</code>: Cluster Stack Name</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install AWS CLI:</p><div class="verbatim-wrap"><pre class="screen">sudo zypper --gpg-auto-import-keys install -y aws-cli</pre></div></li><li class="listitem "><p>Setup AWS credentials:</p><div class="verbatim-wrap"><pre class="screen">aws configure</pre></div></li><li class="listitem "><p>Prepare role policy:</p><div class="verbatim-wrap"><pre class="screen">cat &lt;&lt;*EOF* &gt;"./&lt;STACK_NAME&gt;-trust-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "sts:AssumeRole",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Effect": "Allow",
      "Sid": ""
    }
  ]
}
*EOF*</pre></div></li><li class="listitem "><p>Prepare master instance policy:</p><div class="verbatim-wrap"><pre class="screen">cat &lt;&lt;*EOF* &gt;"./&lt;STACK_NAME&gt;-master-role-trust-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeTags",
        "ec2:DescribeInstances",
        "ec2:DescribeRegions",
        "ec2:DescribeRouteTables",
        "ec2:DescribeSecurityGroups",
        "ec2:DescribeSubnets",
        "ec2:DescribeVolumes",
        "ec2:CreateSecurityGroup",
        "ec2:CreateTags",
        "ec2:CreateVolume",
        "ec2:ModifyInstanceAttribute",
        "ec2:ModifyVolume",
        "ec2:AttachVolume",
        "ec2:AuthorizeSecurityGroupIngress",
        "ec2:CreateRoute",
        "ec2:DeleteRoute",
        "ec2:DeleteSecurityGroup",
        "ec2:DeleteVolume",
        "ec2:DetachVolume",
        "ec2:RevokeSecurityGroupIngress",
        "ec2:DescribeVpcs",
        "elasticloadbalancing:AddTags",
        "elasticloadbalancing:AttachLoadBalancerToSubnets",
        "elasticloadbalancing:ApplySecurityGroupsToLoadBalancer",
        "elasticloadbalancing:CreateLoadBalancer",
        "elasticloadbalancing:CreateLoadBalancerPolicy",
        "elasticloadbalancing:CreateLoadBalancerListeners",
        "elasticloadbalancing:ConfigureHealthCheck",
        "elasticloadbalancing:DeleteLoadBalancer",
        "elasticloadbalancing:DeleteLoadBalancerListeners",
        "elasticloadbalancing:DescribeLoadBalancers",
        "elasticloadbalancing:DescribeLoadBalancerAttributes",
        "elasticloadbalancing:DetachLoadBalancerFromSubnets",
        "elasticloadbalancing:DeregisterInstancesFromLoadBalancer",
        "elasticloadbalancing:ModifyLoadBalancerAttributes",
        "elasticloadbalancing:RegisterInstancesWithLoadBalancer",
        "elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer",
        "elasticloadbalancing:AddTags",
        "elasticloadbalancing:CreateListener",
        "elasticloadbalancing:CreateTargetGroup",
        "elasticloadbalancing:DeleteListener",
        "elasticloadbalancing:DeleteTargetGroup",
        "elasticloadbalancing:DescribeListeners",
        "elasticloadbalancing:DescribeLoadBalancerPolicies",
        "elasticloadbalancing:DescribeTargetGroups",
        "elasticloadbalancing:DescribeTargetHealth",
        "elasticloadbalancing:ModifyListener",
        "elasticloadbalancing:ModifyTargetGroup",
        "elasticloadbalancing:RegisterTargets",
        "elasticloadbalancing:SetLoadBalancerPoliciesOfListener",
        "iam:CreateServiceLinkedRole",
        "kms:DescribeKey"
      ],
      "Resource": [
        "*"
      ]
    }
  ]
}
*EOF*</pre></div></li><li class="listitem "><p>Prepare worker instance policy:</p><div class="verbatim-wrap"><pre class="screen">cat &lt;&lt;*EOF* &gt;"./&lt;STACK_NAME&gt;-worker-role-trust-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:DescribeInstances",
        "ec2:DescribeRegions",
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:GetRepositoryPolicy",
        "ecr:DescribeRepositories",
        "ecr:ListImages",
        "ecr:BatchGetImage"
      ],
      "Resource": "*"
    }
  ]
}
*EOF*</pre></div></li><li class="listitem "><p>Create roles:</p><div class="verbatim-wrap"><pre class="screen">aws iam create-role --role-name &lt;STACK_NAME&gt;_cpi_master --assume-role-policy-document file://&lt;FILE_DIRECTORY&gt;/&lt;STACK_NAME&gt;-trust-policy.json
aws iam create-role --role-name &lt;STACK_NAME&gt;_cpi_worker --assume-role-policy-document file://&lt;FILE_DIRECTORY&gt;/&lt;STACK_NAME&gt;-trust-policy.json</pre></div></li><li class="listitem "><p>Create instance role policies:</p><div class="verbatim-wrap"><pre class="screen">aws iam put-role-policy --role-name &lt;STACK_NAME&gt;_cpi_master --policy-name &lt;STACK_NAME&gt;_cpi_master --policy-document file://&lt;FILE_DIRECTORY&gt;/&lt;STACK_NAME&gt;-master-role-trust-policy.json
aws iam put-role-policy --role-name &lt;STACK_NAME&gt;_cpi_worker --policy-name &lt;STACK_NAME&gt;_cpi_worker --policy-document file://&lt;FILE_DIRECTORY&gt;/&lt;STACK_NAME&gt;-worker-role-trust-policy.json</pre></div></li><li class="listitem "><p>Create instance profiles:</p><div class="verbatim-wrap"><pre class="screen">aws iam create-instance-profile --instance-profile-name &lt;STACK_NAME&gt;_cpi_master
aws iam create-instance-profile --instance-profile-name &lt;STACK_NAME&gt;_cpi_worker</pre></div></li><li class="listitem "><p>Add role to instance profiles:</p><div class="verbatim-wrap"><pre class="screen">aws iam add-role-to-instance-profile --role-name &lt;STACK_NAME&gt;_cpi_master --instance-profile-name &lt;STACK_NAME&gt;_cpi_master
aws iam add-role-to-instance-profile --role-name &lt;STACK_NAME&gt;_cpi_worker --instance-profile-name &lt;STACK_NAME&gt;_cpi_worker</pre></div></li></ol></div></li></ul></div></div></div></div><div class="chapter " id="_glossary"><div class="titlepage"><div><div><h1 class="title"><span class="number">16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Glossary</span> <a title="Permalink" class="permalink" href="#_glossary">#</a></h1></div></div></div><div class="line"></div><div class="horizontal"><table class="horizontal" border="0"><colgroup><col /><col /></colgroup><tbody valign="top"><tr><td valign="top">
<p>AWS</p>
</td><td valign="top">
<p>Amazon Web Services. A broadly adopted cloud platform run by Amazon.</p>
</td></tr><tr><td valign="top">
<p>BPF</p>
</td><td valign="top">
<p>Berkeley Packet Filter. Technology used by Cilium to filter network traffic at the level of packet processing in the kernel.</p>
</td></tr><tr><td valign="top">
<p>CA</p>
</td><td valign="top">
<p>Certificate or Certification Authority. An entity that issues digital certificates.</p>
</td></tr><tr><td valign="top">
<p>CIDR</p>
</td><td valign="top">
<p>Classless Inter-Domain Routing. Method for allocating IP addresses and IP routing.</p>
</td></tr><tr><td valign="top">
<p>CNI</p>
</td><td valign="top">
<p>Container Networking Interface. Creates a generic plugin-based networking solution for containers based on spec files in JSON format.</p>
</td></tr><tr><td valign="top">
<p>CRD</p>
</td><td valign="top">
<p>Custom Resource Definition. Functionality to define non-default resources for Kubernetes pods.</p>
</td></tr><tr><td valign="top">
<p>FQDN</p>
</td><td valign="top">
<p>Fully Qualified Domain Name. The complete domain name for a specific computer, or host, on the internet, consisting of two parts: the hostname and the domain name.</p>
</td></tr><tr><td valign="top">
<p>GKE</p>
</td><td valign="top">
<p>Google Kubernetes Engine. Manager for container orchestration built on Kubernetes by Google. Similar for example to Amazon Elastic Kubernetes Service (Amazon EKS) and Azure Kubernetes Service (AKS).</p>
</td></tr><tr><td valign="top">
<p>HPA</p>
</td><td valign="top">
<p>Horizontal Pod Autoscaler. Based on CPU usage, HPA controls the number of pods in a deployment/replica or stateful set or a replication controller.</p>
</td></tr><tr><td valign="top">
<p>KVM</p>
</td><td valign="top">
<p>Kernel-based Virtual Machine. Linux native virtualization tool that allows the kernel to function as a hypervisor.</p>
</td></tr><tr><td valign="top">
<p>LDAP</p>
</td><td valign="top">
<p>Lightweight Directory Access Protocol. A client/server protocol used to access and manage directory information. It reads and edits directories over IP networks and runs directly over TCP/IP using simple string formats for data transfer.</p>
</td></tr><tr><td valign="top">
<p>OCI</p>
</td><td valign="top">
<p>Open Containers Initiative. A project under the Linux Foundation with the goal of creating open industry standards around container formats and runtime.</p>
</td></tr><tr><td valign="top">
<p>OIDC</p>
</td><td valign="top">
<p>OpenID Connect. Identity layer on top of the OAuth 2.0 protocol.</p>
</td></tr><tr><td valign="top">
<p>OLM</p>
</td><td valign="top">
<p>Operator Lifecycle Manager. Open Source tool for managing operators in a Kubernetes cluster.</p>
</td></tr><tr><td valign="top">
<p>POC</p>
</td><td valign="top">
<p>Proof of Concept. Pioneering project directed at proving the feasibility of a design concept.</p>
</td></tr><tr><td valign="top">
<p>PSP</p>
</td><td valign="top">
<p>Pod Security Policy. PSPs are cluster-level resources that control security-sensitive aspects of pod specification.</p>
</td></tr><tr><td valign="top">
<p>PVC</p>
</td><td valign="top">
<p>Persistent Volume Claim. A request for storage by a user.</p>
</td></tr><tr><td valign="top">
<p>RBAC</p>
</td><td valign="top">
<p>Role-based Access Control. An approach to restrict authorized user access based on defined roles.</p>
</td></tr><tr><td valign="top">
<p>RMT</p>
</td><td valign="top">
<p>Repository Mirroring Tool. Successor of the SMT. Helps optimize the management of SUSE Linux Enterprise software updates and subscription entitlements.</p>
</td></tr><tr><td valign="top">
<p>RPO</p>
</td><td valign="top">
<p>Recovery Point Objective. Defines the interval of time that can occur between to backup points before normal business can no longer be resumed.</p>
</td></tr><tr><td valign="top">
<p>RTO</p>
</td><td valign="top">
<p>Recovery Time Objective. This defines the time (and typically service level from SLA) with which backup relevant incidents must be handled within.</p>
</td></tr><tr><td valign="top">
<p>RSA</p>
</td><td valign="top">
<p>Rivest-Shamir-Adleman. Asymmetric encryption technique that uses two different keys as public and private keys to perform the encryption and decryption.</p>
</td></tr><tr><td valign="top">
<p>SLA</p>
</td><td valign="top">
<p>Service Level Agreement. A contractual clause or set of clauses that determines the guaranteed handling of support or incidents by a software vendor or supplier.</p>
</td></tr><tr><td valign="top">
<p>SMT</p>
</td><td valign="top">
<p>SUSE Subscription Management Tool. Helps to manage software updates, maintain corporate firewall policy and meet regulatory compliance requirements in SUSE Linux Enterprise 11 and 12. Has been replaced by the RMT and SUSE Manager in newer SUSE Linux Enterprise versions.</p>
</td></tr><tr><td valign="top">
<p>STS</p>
</td><td valign="top">
<p>StatefulSet. Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods for a "stateful" application.</p>
</td></tr><tr><td valign="top">
<p>SMTP</p>
</td><td valign="top">
<p>Simple Mail Transfer Protocol. A communication protocol for electronic mail transmission.</p>
</td></tr><tr><td valign="top">
<p>TOML</p>
</td><td valign="top">
<p>Tom’s Obvious, Minimal Language. Configuration file format used for configuring container registries for CRI-O.</p>
</td></tr><tr><td valign="top">
<p>VPA</p>
</td><td valign="top">
<p>Vertical Pod Autoscaler. VPA automatically sets the values for resource requests and container limits based on usage.</p>
</td></tr><tr><td valign="top">
<p>VPC</p>
</td><td valign="top">
<p>Virtual Private Cloud. Division of a public cloud, which supports private cloud computing and thus offers more control over virtual networks and an isolated environment for sensitive workloads.</p>
</td></tr></tbody></table></div></div><div class="appendix " id="_contributors"><div class="titlepage"><div><div><h1 class="title"><span class="number">A </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Contributors</span> <a title="Permalink" class="permalink" href="#_contributors">#</a></h1></div></div></div><div class="line"></div><p>The contents of these documents are edited by the technical writers for
SUSE CaaS Platform and original works created by its <a class="link" href="https://github.com/SUSE/doc-caasp/graphs/contributors" target="_blank">contributors</a>.</p></div><div class="appendix " id="_gnu_licenses"><div class="titlepage"><div><div><h1 class="title"><span class="number">B </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">GNU Licenses</span> <a title="Permalink" class="permalink" href="#_gnu_licenses">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_gnu_free_documentation_license"><span class="number">B.1 </span><span class="name">GNU Free Documentation License</span></a></span></dt></dl></div></div><p>This appendix contains the GNU Free Documentation License version 1.2.</p><div class="sect1" id="_gnu_free_documentation_license"><div class="titlepage"><div><div><h2 class="title"><span class="number">B.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">GNU Free Documentation License</span> <a title="Permalink" class="permalink" href="#_gnu_free_documentation_license">#</a></h2></div></div></div><p>Copyright © 2000, 2001, 2002 Free Software Foundation, Inc.
51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p><div class="sect2" id="gfdl-preamble"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">0. PREAMBLE</span> <a title="Permalink" class="permalink" href="#gfdl-preamble">#</a></h3></div></div></div><p>The purpose of this License is to make a manual, textbook, or other functional and useful document "free" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or non-commercially.
Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.</p><p>This License is a kind of "copyleft", which means that derivative works of the document must themselves be free in the same sense.
It complements the GNU General Public License, which is a copyleft license designed for free software.</p><p>We have designed this License to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does.
But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book.
We recommend this License principally for works whose purpose is instruction or reference.</p></div><div class="sect2" id="gfdl-applicabilty"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">1. APPLICABILITY AND DEFINITIONS</span> <a title="Permalink" class="permalink" href="#gfdl-applicabilty">#</a></h3></div></div></div><p>This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License.
Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein.
The "Document", below, refers to any such manual or work.
Any member of the public is a licensee, and is addressed as "you". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.</p><p>A "Modified Version" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.</p><p>A "Secondary Section" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document’s overall subject (or to related matters) and contains nothing that could fall directly within that overall subject.
(Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.</p><p>The "Invariant Sections" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License.
If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant.
The Document may contain zero Invariant Sections.
If the Document does not identify any Invariant Sections then there are none.</p><p>The "Cover Texts" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License.
A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.</p><p>A "Transparent" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters.
A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent.
An image format is not Transparent if used for any substantial amount of text.
A copy that is not "Transparent" is called "Opaque".</p><p>Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification.
Examples of transparent image formats include PNG, XCF and JPG.
Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.</p><p>The "Title Page" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page.
For works in formats which do not have any title page as such, "Title Page" means the text near the most prominent appearance of the work’s title, preceding the beginning of the body of the text.</p><p>A section "Entitled XYZ" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language.
(Here XYZ stands for a specific section name mentioned below, such as "Acknowledgements", "Dedications", "Endorsements", or "History".) To "Preserve the Title" of such a section when you modify the Document means that it remains a section "Entitled XYZ" according to this definition.</p><p>The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document.
These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.</p></div><div class="sect2" id="gfdl-verbatim-copying"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">2. VERBATIM COPYING</span> <a title="Permalink" class="permalink" href="#gfdl-verbatim-copying">#</a></h3></div></div></div><p>You may copy and distribute the Document in any medium, either commercially or non-commercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License.
You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute.
However, you may accept compensation in exchange for copies.
If you distribute a large enough number of copies you must also follow the conditions in section 3.</p><p>You may also lend copies, under the same conditions stated above, and you may publicly display copies.</p></div><div class="sect2" id="gfdl-quantity-copying"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">3. COPYING IN QUANTITY</span> <a title="Permalink" class="permalink" href="#gfdl-quantity-copying">#</a></h3></div></div></div><p>If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document’s license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover.
Both covers must also clearly and legibly identify you as the publisher of these copies.
The front cover must present the full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition.
Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.</p><p>If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.</p><p>If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material.
If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.</p><p>It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.</p></div><div class="sect2" id="gfdl-modifications"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">4. MODIFICATIONS</span> <a title="Permalink" class="permalink" href="#gfdl-modifications">#</a></h3></div></div></div><p>You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it.
In addition, you must do these things in the Modified Version:</p><div class="orderedlist "><ol class="orderedlist" type="A"><li class="listitem "><p>Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.</p></li><li class="listitem "><p>List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.</p></li><li class="listitem "><p>State on the Title page the name of the publisher of the Modified Version, as the publisher.</p></li><li class="listitem "><p>Preserve all the copyright notices of the Document.</p></li><li class="listitem "><p>Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.</p></li><li class="listitem "><p>Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.</p></li><li class="listitem "><p>Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document’s license notice.</p></li><li class="listitem "><p>Include an unaltered copy of this License.</p></li><li class="listitem "><p>Preserve the section Entitled "History", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled "History" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.</p></li><li class="listitem "><p>Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the "History" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.</p></li><li class="listitem "><p>For any section Entitled "Acknowledgements" or "Dedications", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.</p></li><li class="listitem "><p>Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.</p></li><li class="listitem "><p>Delete any section Entitled "Endorsements". Such a section may not be included in the Modified Version.</p></li><li class="listitem "><p>Do not retitle any existing section to be Entitled "Endorsements" or to conflict in title with any Invariant Section.</p></li><li class="listitem "><p>Preserve any Warranty Disclaimers.</p></li></ol></div><p>If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant.
To do this, add their titles to the list of Invariant Sections in the Modified Version’s license notice.
These titles must be distinct from any other section titles.</p><p>You may add a section Entitled "Endorsements", provided it contains nothing but endorsements of your Modified Version by various parties—​for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.</p><p>You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version.
Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity.
If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.</p><p>The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.</p></div><div class="sect2" id="gfdl-combining"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">5. COMBINING DOCUMENTS</span> <a title="Permalink" class="permalink" href="#gfdl-combining">#</a></h3></div></div></div><p>You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.</p><p>The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy.
If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.</p><p>In the combination, you must combine any sections Entitled "History" in the various original documents, forming one section Entitled "History"; likewise combine any sections Entitled "Acknowledgements", and any sections Entitled "Dedications". You must delete all sections Entitled "Endorsements".</p></div><div class="sect2" id="gfdl-collections"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">6. COLLECTIONS OF DOCUMENTS</span> <a title="Permalink" class="permalink" href="#gfdl-collections">#</a></h3></div></div></div><p>You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.</p><p>You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.</p></div><div class="sect2" id="gfdl-aggregation"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">7. AGGREGATION WITH INDEPENDENT WORKS</span> <a title="Permalink" class="permalink" href="#gfdl-aggregation">#</a></h3></div></div></div><p>A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an "aggregate" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation’s users beyond what the individual works permit.
When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.</p><p>If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document’s Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form.
Otherwise they must appear on printed covers that bracket the whole aggregate.</p></div><div class="sect2" id="gfdl-translation"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">8. TRANSLATION</span> <a title="Permalink" class="permalink" href="#gfdl-translation">#</a></h3></div></div></div><p>Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4.
Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections.
You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers.
In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.</p><p>If a section in the Document is Entitled "Acknowledgements", "Dedications", or "History", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.</p></div><div class="sect2" id="gfdl-termination"><div class="titlepage"><div><div><h3 class="title"><span class="number">B.1.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">9. TERMINATION</span> <a title="Permalink" class="permalink" href="#gfdl-termination">#</a></h3></div></div></div><p>You may not copy, modify, sublicense, or distribute the Document except as expressly provided for under this License.
Any other attempt to copy, modify, sublicense or distribute the Document is void, and will automatically terminate your rights under this License.
However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.</p><div class="sect3" id="gfdl-future"><div class="titlepage"><div><div><h4 class="title"><span class="number">B.1.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">10. FUTURE REVISIONS OF THIS LICENSE</span> <a title="Permalink" class="permalink" href="#gfdl-future">#</a></h4></div></div></div><p>The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time.
Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.
See <a class="link" href="http://www.gnu.org/copyleft/" target="_blank">http://www.gnu.org/copyleft/</a>.</p><p>Each version of the License is given a distinguishing version number.
If the Document specifies that a particular numbered version of this License "or any later version" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation.
If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation.</p></div><div class="sect3" id="gfdl-addendum"><div class="titlepage"><div><div><h4 class="title"><span class="number">B.1.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ADDENDUM: How to use this License for your documents</span> <a title="Permalink" class="permalink" href="#gfdl-addendum">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled "GNU
Free Documentation License".</pre></div><p>If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the “
with…​Texts.”
line with this:</p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>If you have Invariant Sections without Cover Texts, or some other combination of the three, merge those two alternatives to suit the situation.</p><p>If your document contains nontrivial examples of program code, we recommend releasing these examples in parallel under your choice of free software license, such as the GNU General Public License, to permit their use in free software.</p></div></div></div></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>